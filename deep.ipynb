{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa45f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e5a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGPT2Hybrid(nn.Module):\n",
    "    def __init__(self, bert_model=\"bert-base-uncased\", gpt2_model=\"gpt2\"):\n",
    "        super(BERTGPT2Hybrid, self).__init__()\n",
    "\n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "        # Load pre-trained GPT2 model\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model)\n",
    "\n",
    "        # Set GPT2 tokenizer padding token to be same as BERT's pad token\n",
    "        gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, output_ids=None):\n",
    "        # Get BERT outputs (used for feature extraction)\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_features = bert_outputs.last_hidden_state[:, 0, :]  # Get [CLS] token\n",
    "\n",
    "        # Use GPT2 for language modeling\n",
    "        gpt2_outputs = self.gpt2(input_ids=output_ids, labels=output_ids)\n",
    "        return bert_features, gpt2_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780bd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiHowDataset(Dataset):\n",
    "    def __init__(self, csv_path, max_length=256):\n",
    "        self.data = pd.read_csv(csv_path, encoding='ISO-8859-1').dropna(subset=['input_text', 'output_text'])\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = str(self.data.iloc[idx]['input_text'])\n",
    "        output_text = str(self.data.iloc[idx]['output_text'])\n",
    "\n",
    "        # Tokenize input and output text\n",
    "        input_ids = bert_tokenizer.encode(input_text, truncation=True, max_length=self.max_length, return_tensors=\"pt\").squeeze()\n",
    "        output_ids = gpt2_tokenizer.encode(output_text, truncation=True, max_length=self.max_length, return_tensors=\"pt\").squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"output_ids\": output_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbec5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    output_ids = [item['output_ids'] for item in batch]\n",
    "\n",
    "    # Pad sequences to maximum length\n",
    "    input_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=bert_tokenizer.pad_token_id)\n",
    "    output_padded = torch.nn.utils.rnn.pad_sequence(output_ids, batch_first=True, padding_value=gpt2_tokenizer.pad_token_id)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_padded,\n",
    "        \"output_ids\": output_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        output_ids = batch['output_ids']\n",
    "\n",
    "        # Get features from BERT and outputs from GPT2\n",
    "        bert_features, gpt2_outputs = model(input_ids, attention_mask=(input_ids != bert_tokenizer.pad_token_id), output_ids=output_ids)\n",
    "\n",
    "        # Compute loss (use GPT2 loss for simplicity)\n",
    "        loss = gpt2_outputs.loss\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066fae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\AppData\\Local\\Temp\\ipykernel_16396\\753510273.py:3: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_path, encoding='ISO-8859-1').dropna(subset=['input_text', 'output_text'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/53641 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "  0%|          | 20/53641 [00:25<19:20:28,  1.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 24\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 20\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    csv_path = \"C:/Users/aravi/Downloads/archive (2)/wikihowAll.csv\" \n",
    "    batch_size = 4\n",
    "    max_length = 256\n",
    "    epochs = 3\n",
    "    lr = 5e-5\n",
    "\n",
    "    # Prepare dataset and dataloaders\n",
    "    dataset = WikiHowDataset(csv_path, max_length=max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = BERTGPT2Hybrid(bert_model=\"bert-base-uncased\", gpt2_model=\"gpt2\")\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = train(model, dataloader, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    torch.save(model.state_dict(), \"bert_gpt2_hybrid_finetuned.pth\")\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e36763d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583dc693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudagpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
