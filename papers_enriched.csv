id,title,doi,publication_year,authors,abstract,open_access,host_venue,clean_abstract,keywords,entities
https://openalex.org/W2896457183,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://doi.org/10.48550/arxiv.1810.04805,2018,"Jacob Devlin,we introduce a new language representation model called bert which stands for bidirectional encoder representations from transformers unlike recent language representation models bert is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers as a result the pretrained bert model can be finetuned with just one additional output layer to create stateoftheart models for a wide range of tasks such as question answering and language inference without substantial taskspecific architecture modifications bert is conceptually simple and empirically powerful it obtains new stateoftheart results on eleven natural language processing tasks including pushing the glue score to 805 77 point absolute improvement multinli accuracy to 867 46 absolute improvement squad v11 question answering test f1 to 932 15 point absolute improvement and squad v20 test f1 to 831 51 point absolute improvement, Kenton Lee, Kristina Toutanova",we introduce a new language representation model called bert which stands for bidirectional encoder representations from transformers unlike recent language representation models bert is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers as a result the pretrained bert model can be finetuned with just one additional output layer to create stateoftheart models for a wide range of tasks such as question answering and language inference without substantial taskspecific architecture modifications bert is conceptually simple and empirically powerful it obtains new stateoftheart results on eleven natural language processing tasks including pushing the glue score to 805 77 point absolute improvement multinli accuracy to 867 46 absolute improvement squad v11 question answering test f1 to 932 15 point absolute improvement and squad v20 test f1 to 831 51 point absolute improvement, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",True,,we introduce a new language representation model called bert which stands for bidirectional encoder representations from transformers unlike recent language representation models bert is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers as a result the pretrained bert model can be finetuned with just one additional output layer to create stateoftheart models for a wide range of tasks such as question answering and language inference without substantial taskspecific architecture modifications bert is conceptually simple and empirically powerful it obtains new stateoftheart results on eleven natural language processing tasks including pushing the glue score to 805 77 point absolute improvement multinli accuracy to 867 46 absolute improvement squad v11 question answering test f1 to 932 15 point absolute improvement and squad v20 test f1 to 831 51 point absolute improvement,[],[]
https://openalex.org/W3138516171,Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,https://doi.org/10.1109/iccv48922.2021.00986,2021,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo","This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",True,,this paper presents a new vision transformer called swin transformer that capably serves as a generalpurpose backbone for computer vision challenges in adapting transformer from language to vision arise from differences between the two domains such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text to address these differences we propose a hierarchical transformer whose representation is computed with shifted windows the shifted windowing scheme brings greater efficiency by limiting selfattention computation to nonoverlapping local windows while also allowing for crosswindow connection this hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size these qualities of swin transformer make it compatible with a broad range of vision tasks including image classification 873 top1 accuracy on imagenet1k and dense prediction tasks such as object detection 587 box ap and 511 mask ap on coco testdev and semantic segmentation 535 miou on ade20k val its performance surpasses the previous stateoftheart by a large margin of 27 box ap and 26 mask ap on coco and 32 miou on ade20k demonstrating the potential of transformerbased models as vision backbones the hierarchical design and the shifted window approach also prove beneficial for allmlp architectures the code and models are publicly available at httpsgithubcommicrosoftswintransformer,[],[]
https://openalex.org/W3094502228,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,https://doi.org/10.48550/arxiv.2010.11929,2020,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby","While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",True,,while the transformer architecture has become the defacto standard for natural language processing tasks its applications to computer vision remain limited in vision attention is either applied in conjunction with convolutional networks or used to replace certain components of convolutional networks while keeping their overall structure in place we show that this reliance on cnns is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks when pretrained on large amounts of data and transferred to multiple midsized or small image recognition benchmarks imagenet cifar100 vtab etc vision transformer vit attains excellent results compared to stateoftheart convolutional networks while requiring substantially fewer computational resources to train,[],[]
https://openalex.org/W3096609285,End-to-End Object Detection with Transformers,https://doi.org/10.1007/978-3-030-58452-8_13,2020,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko",,False,,,[],[]
https://openalex.org/W603908379,Spatial transformer networks,,2015,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu","Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",False,,convolutional neural networks define an exceptionally powerful class of models but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner in this work we introduce a new learnable module the spatial transformer which explicitly allows the spatial manipulation of data within the network this differentiable module can be inserted into existing convolutional architectures giving neural networks the ability to actively spatially transform feature maps conditional on the feature map itself without any extra training supervision or modification to the optimisation process we show that the use of spatial transformers results in models which learn invariance to translation scale rotation and more generic warping resulting in stateoftheart performance on several benchmarks and for a number of classes of transformations,[],[]
https://openalex.org/W4288089799,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://doi.org/10.48550/arxiv.1910.10683,2019,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu","Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",True,,transfer learning where a model is first pretrained on a datarich task before being finetuned on a downstream task has emerged as a powerful technique in natural language processing nlp the effectiveness of transfer learning has given rise to a diversity of approaches methodology and practice in this paper we explore the landscape of transfer learning techniques for nlp by introducing a unified framework that converts all textbased language problems into a texttotext format our systematic study compares pretraining objectives architectures unlabeled data sets transfer approaches and other factors on dozens of language understanding tasks by combining the insights from our exploration with scale and our new colossal clean crawled corpus we achieve stateoftheart results on many benchmarks covering summarization question answering text classification and more to facilitate future work on transfer learning for nlp we release our data set pretrained models and code,[],[]
https://openalex.org/W2979826702,Transformers: State-of-the-Art Natural Language Processing,https://doi.org/10.18653/v1/2020.emnlp-demos.6,2020,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clément Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush","Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.",True,,thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi pierric cistac tim rault remi louf morgan funtowicz joe davison sam shleifer patrick von platen clara ma yacine jernite julien plu canwen xu teven le scao sylvain gugger mariama drame quentin lhoest alexander rush proceedings of the 2020 conference on empirical methods in natural language processing system demonstrations 2020,[],[]
https://openalex.org/W2980282514,HuggingFace's Transformers: State-of-the-art Natural Language Processing,https://doi.org/10.48550/arxiv.1910.03771,2019,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clément Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew","Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",True,,recent progress in natural language processing has been driven by advances in both model architecture and model pretraining transformer architectures have facilitated building highercapacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks textittransformers is an opensource library with the goal of opening up these advances to the wider machine learning community the library consists of carefully engineered stateofthe art transformer architectures under a unified api backing this library is a curated collection of pretrained models made by and available for the community textittransformers is designed to be extensible by researchers simple for practitioners and fast and robust in industrial deployments the library is available at urlhttpsgithubcomhuggingfacetransformers,[],[]
https://openalex.org/W3015468748,Longformer: The Long-Document Transformer,https://doi.org/10.48550/arxiv.2004.05150,2020,"Iz Beltagy, Matthew E. Peters, Arman Cohan","Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",True,,transformerbased models are unable to process long sequences due to their selfattention operation which scales quadratically with the sequence length to address this limitation we introduce the longformer with an attention mechanism that scales linearly with sequence length making it easy to process documents of thousands of tokens or longer longformers attention mechanism is a dropin replacement for the standard selfattention and combines a local windowed attention with a task motivated global attention following prior work on longsequence transformers we evaluate longformer on characterlevel language modeling and achieve stateoftheart results on text8 and enwik8 in contrast to most prior work we also pretrain longformer and finetune it on a variety of downstream tasks our pretrained longformer consistently outperforms roberta on long document tasks and sets new stateoftheart results on wikihop and triviaqa we finally introduce the longformerencoderdecoder led a longformer variant for supporting long document generative sequencetosequence tasks and demonstrate its effectiveness on the arxiv summarization dataset,[],[]
https://openalex.org/W3159481202,Emerging Properties in Self-Supervised Vision Transformers,https://doi.org/10.1109/iccv48922.2021.00951,2021,"Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jeǵou, Julien Mairal, Piotr Bojanowski, Armand Joulin","In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",True,,in this paper we question if selfsupervised learning provides new properties to vision transformer vit 16 that stand out compared to convolutional networks convnets beyond the fact that adapting selfsupervised methods to this architecture works particularly well we make the following observations first selfsupervised vit features contain explicit information about the semantic segmentation of an image which does not emerge as clearly with supervised vits nor with convnets second these features are also excellent knn classifiers reaching 783 top1 on imagenet with a small vit our study also underlines the importance of momentum encoder 26 multicrop training 9 and the use of small patches with vits we implement our findings into a simple selfsupervised method called dino which we interpret as a form of selfdistillation with no labels we show the synergy between dino and vits by achieving 801 top1 on imagenet in linear evaluation with vitbase,[],[]
https://openalex.org/W2964110616,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,https://doi.org/10.18653/v1/p19-1285,2019,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov","Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",True,,transformers have a potential of learning longerterm dependency but are limited by a fixedlength context in the setting of language modeling we propose a novel neural architecture transformerxl that enables learning dependency beyond a fixed length without disrupting temporal coherence it consists of a segmentlevel recurrence mechanism and a novel positional encoding scheme our method not only enables capturing longerterm dependency but also resolves the context fragmentation problem as a result transformerxl learns dependency that is 80 longer than rnns and 450 longer than vanilla transformers achieves better performance on both short and long sequences and is up to 1800 times faster than vanilla transformers during evaluation notably we improve the stateoftheart results of bpcperplexity to 099 on enwiki8 108 on text8 183 on wikitext103 218 on one billion word and 545 on penn treebank without finetuning when trained only on wikitext103 transformerxl manages to generate reasonably coherent novel text articles with thousands of tokens our code pretrained models and hyperparameters are available in both tensorflow and pytorch,[],[]
https://openalex.org/W4213019189,A Survey on Vision Transformer,https://doi.org/10.1109/tpami.2022.3152247,2022,"Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao","Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",True,,transformer first applied to the field of natural language processing is a type of deep neural network mainly based on the selfattention mechanism thanks to its strong representation capabilities researchers are looking at ways to apply transformer to computer vision tasks in a variety of visual benchmarks transformerbased models perform similar to or better than other types of networks such as convolutional and recurrent neural networks given its high performance and less need for visionspecific inductive bias transformer is receiving more and more attention from the computer vision community in this paper we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages the main categories we explore include the backbone network highmidlevel vision lowlevel vision and video processing we also include efficient transformer methods for pushing transformer into real devicebased applications furthermore we also take a brief look at the selfattention mechanism in computer vision as it is the base component in transformer toward the end of this paper we discuss the challenges and provide several further research directions for vision transformers,[],[]
https://openalex.org/W3207918547,SwinIR: Image Restoration Using Swin Transformer,https://doi.org/10.1109/iccvw54120.2021.00210,2021,"Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte","Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14∼0.45dB, while the total number of parameters can be reduced by up to 67%.",True,,image restoration is a longstanding lowlevel vision problem that aims to restore highquality images from lowquality images eg downscaled noisy and compressed images while stateoftheart image restoration methods are based on convolutional neural networks few attempts have been made with transformers which show impressive performance on highlevel vision tasks in this paper we propose a strong baseline model swinir for image restoration based on the swin transformer swinir consists of three parts shallow feature extraction deep feature extraction and highquality image reconstruction in particular the deep feature extraction module is composed of several residual swin transformer blocks rstb each of which has several swin transformer layers together with a residual connection we conduct experiments on three representative tasks image superresolution including classical lightweight and realworld image superresolution image denoising including grayscale and color image denoising and jpeg compression artifact reduction experimental results demonstrate that swinir outperforms stateoftheart methods on different tasks by up to 014045db while the total number of parameters can be reduced by up to 67,[],[]
https://openalex.org/W4206706211,Transformers in Vision: A Survey,https://doi.org/10.1145/3505244,2022,"Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, Mubarak Shah","Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.",True,,astounding results from transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems among their salient benefits transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks eg long shortterm memory lstm different from convolutional networks transformers require minimal inductive biases for their design and are naturally suited as setfunctions furthermore the straightforward design of transformers allows processing multiple modalities eg images videos text and speech using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets these strengths have led to exciting progress on a number of vision tasks using transformer networks this survey aims to provide a comprehensive overview of the transformer models in the computer vision discipline we start with an introduction to fundamental concepts behind the success of transformers ie selfattention largescale pretraining and bidirectional encoding we then cover extensive applications of transformers in vision including popular recognition tasks eg image classification object detection action recognition and segmentation generative modeling multimodal tasks eg visualquestion answering visual reasoning and visual grounding video processing eg activity recognition video forecasting lowlevel vision eg image superresolution image enhancement and colorization and 3d analysis eg point cloud classification and segmentation we compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value finally we provide an analysis on open research directions and possible future works,[],[]
https://openalex.org/W3097777922,Conformer: Convolution-augmented Transformer for Speech Recognition,https://doi.org/10.21437/interspeech.2020-3015,2020,"Anmol Gulati, James Qin, Chung‐Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang","Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs).Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way.To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%without using a language model and 1.9%/3.9%with an external language model on test/testother.We also observe competitive performance of 2.7%/6.3%with a small model of only 10M parameters.",True,,recently transformer and convolution neural network cnn based models have shown promising results in automatic speech recognition asr outperforming recurrent neural networks rnnstransformer models are good at capturing contentbased global interactions while cnns exploit local features effectivelyin this work we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameterefficient wayto this regard we propose the convolutionaugmented transformer for speech recognition named conformerconformer significantly outperforms the previous transformer and cnn based models achieving stateoftheart accuracieson the widely used librispeech benchmark our model achieves wer of 2143without using a language model and 1939with an external language model on testtestotherwe also observe competitive performance of 2763with a small model of only 10m parameters,[],[]
https://openalex.org/W3131500599,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,https://doi.org/10.1109/iccv48922.2021.00061,2021,"Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lü, Ping Luo, Ling Shao","Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research.",True,,although convolutional neural networks cnns have achieved great success in computer vision this work investigates a simpler convolutionfree backbone network usefid for many dense prediction tasks unlike the recentlyproposed vision transformer vit that was designed for image classification specifically we introduce the pyramid vision transformer pvt which overcomes the difficulties of porting transformer to various dense prediction tasks pvt has several merits compared to current state of the arts 1 different from vit that typically yields lowresolution outputs and incurs high computational and memory costs pvt not only can be trained on dense partitions of an image to achieve high output resolution which is important for dense prediction but also uses a progressive shrinking pyramid to reduce the computations of large feature maps 2 pvt inherits the advantages of both cnn and transformer making it a unified backbone for various vision tasks without convolutions where it can be used as a direct replacement for cnn backbones 3 we validate pvt through extensive experiments showing that it boosts the performance of many downstream tasks including object detection instance and semantic segmentation for example with a comparable number of parameters pvtretinanet achieves 404 ap on the coco dataset surpassing resnet50retinnet 363 ap by 41 absolute ap see figure 2 we hope that pvt could serre as an alternative and useful backbone for pixellevel predictions and facilitate future research,[],[]
https://openalex.org/W3170841864,Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://doi.org/10.1109/cvpr46437.2021.00681,2021,"Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, Li Zhang","Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.",True,,most recent semantic segmentation methods adopt a fullyconvolutional network fcn with an encoderdecoder architecture the encoder progressively reduces the spatial resolution and learns more abstractsemantic visual concepts with larger receptive fields since context modeling is critical for segmentation the latest efforts have been focused on increasing the receptive field through either dilatedatrous convolutions or inserting attention modules however the encoderdecoder based fcn architecture remains unchanged in this paper we aim to provide an alternative perspective by treating semantic segmentation as a sequencetosequence prediction task specifically we deploy a pure transformer ie without convolution and resolution reduction to encode an image as a sequence of patches with the global context modeled in every layer of the transformer this encoder can be combined with a simple decoder to provide a powerful segmentation model termed segmentation transformer setr extensive experiments show that setr achieves new state of the art on ade20k 5028 miou pascal context 5583 miou and competitive results on cityscapes particularly we achieve the first position in the highly competitive ade20k test server leaderboard on the day of submission,[],[]
https://openalex.org/W3127751679,TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation,https://doi.org/10.48550/arxiv.2102.04306,2021,"Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lü, Alan Yuille, Yuyin Zhou","Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.",True,,medical image segmentation is an essential prerequisite for developing healthcare systems especially for disease diagnosis and treatment planning on various medical image segmentation tasks the ushaped architecture also known as unet has become the defacto standard and achieved tremendous success however due to the intrinsic locality of convolution operations unet generally demonstrates limitations in explicitly modeling longrange dependency transformers designed for sequencetosequence prediction have emerged as alternative architectures with innate global selfattention mechanisms but can result in limited localization abilities due to insufficient lowlevel details in this paper we propose transunet which merits both transformers and unet as a strong alternative for medical image segmentation on one hand the transformer encodes tokenized image patches from a convolution neural network cnn feature map as the input sequence for extracting global contexts on the other hand the decoder upsamples the encoded features which are then combined with the highresolution cnn feature maps to enable precise localization we argue that transformers can serve as strong encoders for medical image segmentation tasks with the combination of unet to enhance finer details by recovering localized spatial information transunet achieves superior performances to various competing methods on different medical applications including multiorgan segmentation and cardiac segmentation code and models are available at httpsgithubcombeckschentransunet,[],[]
https://openalex.org/W3177318507,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,https://doi.org/10.1609/aaai.v35i12.17325,2021,"Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang","Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.",True,,many realworld applications require the prediction of long sequence timeseries such as electricity consumption planning long sequence timeseries forecasting lstf demands a high prediction capacity of the model which is the ability to capture precise longrange dependency coupling between output and input efficiently recent studies have shown the potential of transformer to increase the prediction capacity however there are several severe issues with transformer that prevent it from being directly applicable to lstf including quadratic time complexity high memory usage and inherent limitation of the encoderdecoder architecture to address these issues we design an efficient transformerbased model for lstf named informer with three distinctive characteristics i a probsparse selfattention mechanism which achieves ol log l in time complexity and memory usage and has comparable performance on sequences dependency alignment ii the selfattention distilling highlights dominating attention by halving cascading layer input and efficiently handles extreme long input sequences iii the generative style decoder while conceptually simple predicts the long timeseries sequences at one forward operation rather than a stepbystep way which drastically improves the inference speed of longsequence predictions extensive experiments on four largescale datasets demonstrate that informer significantly outperforms existing methods and provides a new solution to the lstf problem,[],[]
https://openalex.org/W3211490618,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,https://doi.org/10.48550/arxiv.2105.15203,2021,"Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Álvarez, Ping Luo","We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.",True,,we present segformer a simple efficient yet powerful semantic segmentation framework which unifies transformers with lightweight multilayer perception mlp decoders segformer has two appealing features 1 segformer comprises a novel hierarchically structured transformer encoder which outputs multiscale features it does not need positional encoding thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training 2 segformer avoids complex decoders the proposed mlp decoder aggregates information from different layers and thus combining both local attention and global attention to render powerful representations we show that this simple and lightweight design is the key to efficient segmentation on transformers we scale our approach up to obtain a series of models from segformerb0 to segformerb5 reaching significantly better performance and efficiency than previous counterparts for example segformerb4 achieves 503 miou on ade20k with 64m parameters being 5x smaller and 22 better than the previous best method our best model segformerb5 achieves 840 miou on cityscapes validation set and shows excellent zeroshot robustness on cityscapesc code will be released at githubcomnvlabssegformer,[],[]
https://openalex.org/W1536164530,Transformer and Inductor Design Handbook,https://doi.org/10.1201/9780203913598,2004,Colonel Wm. T. McLyman,"Fundamentals of Magnetics. Magnetic Materials and Their Characteristics. Magnetic Cores, Iron Alloy and Ferrites. Window Utilization and Magnet Wire. Transformer-Inductor Design. Transformer-Inductor Efficiency, Regulation, and Temperature Rise. Power Transformer Design. DC Inductor Design Gap Core. DC Inductor Design Powder Core. AC Inductor Design. Constant Voltage Transformer Design (CVT). Three Phase Transformer Design. Flyback Converter Design. Forward Converter Transformer and Inductor Design. Input Filter Design. Current Transformer Design. Winding Capacitance and Leakage Inductance. Quiet Converter Design. Rotary Transformer Design. Planar Transformers. Derivation for the Design Equations. Index.",False,,fundamentals of magnetics magnetic materials and their characteristics magnetic cores iron alloy and ferrites window utilization and magnet wire transformerinductor design transformerinductor efficiency regulation and temperature rise power transformer design dc inductor design gap core dc inductor design powder core ac inductor design constant voltage transformer design cvt three phase transformer design flyback converter design forward converter transformer and inductor design input filter design current transformer design winding capacitance and leakage inductance quiet converter design rotary transformer design planar transformers derivation for the design equations index,[],[]
https://openalex.org/W3133696297,Transformer in Transformer,https://doi.org/10.48550/arxiv.2103.00112,2021,"Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang","Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16$\times$16) as ""visual sentences"" and present to further divide them into smaller patches (e.g., 4$\times$4) as ""visual words"". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/TNT.",True,,transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism basically the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship since natural images are of high complexity with abundant detail and color information the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations in this paper we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture namely transformer in transformer tnt specifically we regard the local patches eg 16times16 as visual sentences and present to further divide them into smaller patches eg 4times4 as visual words the attention of each word will be calculated with other words in the given visual sentence with negligible computational costs features of both words and sentences will be aggregated to enhance the representation ability experiments on several benchmarks demonstrate the effectiveness of the proposed tnt architecture eg we achieve an 815 top1 accuracy on the imagenet which is about 17 higher than that of the stateoftheart visual transformer with similar computational cost the pytorch code is available at httpsgithubcomhuaweinoahcvbackbones and the mindspore code is available at httpsgiteecommindsporemodelstreemasterresearchcvtnt,[],[]
https://openalex.org/W2970231061,LXMERT: Learning Cross-Modality Encoder Representations from Transformers,https://doi.org/10.18653/v1/d19-1514,2019,"Hao Tan, Mohit Bansal","Hao Tan, Mohit Bansal. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",True,,hao tan mohit bansal proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlpijcnlp 2019,[],[]
https://openalex.org/W4214755140,Point Transformer,https://doi.org/10.1109/iccv48922.2021.01595,2021,"Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip H. S. Torr, Vladlen Koltun","Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time.",False,,selfattention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection inspired by this success we investigate the application of selfattention networks to 3d point cloud processing we design selfattention layers for point clouds and use these to construct selfattention networks for tasks such as semantic scene segmentation object part segmentation and object classification our point transformer design improves upon prior work across domains and tasks for example on the challenging s3dis dataset for largescale semantic scene segmentation the point transformer attains an miou of 704 on area 5 outperforming the strongest prior model by 33 absolute percentage points and crossing the 70 miou threshold for the first time,[],[]
https://openalex.org/W3082274269,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,,2020,"Colin Raffel, Noam Shazeer, Adam P. Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu","Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",False,,transfer learning where a model is first pretrained on a datarich task before being finetuned on a downstream task has emerged as a powerful technique in natural language processing nlp the effectiveness of transfer learning has given rise to a diversity of approaches methodology and practice in this paper we explore the landscape of transfer learning techniques for nlp by introducing a unified framework that converts all textbased language problems into a texttotext format our systematic study compares pretraining objectives architectures unlabeled data sets transfer approaches and other factors on dozens of language understanding tasks by combining the insights from our exploration with scale and our new colossal clean crawled corpus we achieve stateoftheart results on many benchmarks covering summarization question answering text classification and more to facilitate future work on transfer learning for nlp we release our data set pretrained models and code,[],[]
https://openalex.org/W4214612132,ViViT: A Video Vision Transformer,https://doi.org/10.1109/iccv48922.2021.00676,2021,"Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid","We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks.",True,,we present puretransformer based models for video classification drawing upon the recent success of such models in image classification our model extracts spatiotemporal tokens from the input video which are then encoded by a series of transformer layers in order to handle the long sequences of tokens encountered in video we propose several efficient variants of our model which factorise the spatial and temporaldimensions of the input although transformerbased models are known to only be effective when large training datasets are available we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets we conduct thorough ablation studies and achieve stateoftheart results on multiple video classification benchmarks including kinetics 400 and 600 epic kitchens somethingsomething v2 and moments in time outperforming prior methods based on deep 3d convolutional networks,[],[]
https://openalex.org/W2149521553,Effects of eddy currents in transformer windings,https://doi.org/10.1049/piee.1966.0236,1966,P.L. Dowell,"The effects of eddy currents in transformer windings are considered, and a method is derived for calculating the variation of winding resistance and leakage inductance with frequency for transformers with single-layer, multilayer and sectionalised windings. The method consists in dividing the winding into portions, calculating the d.c. resistances and d.c. leakage inductances of each of these portions, and then multiplying the d.c. values by appropriate factors to obtain the corresponding a.c. values. These a.c. values are then referred to, say, the primary winding and summed to give the total winding resistance and leakage inductance of the transformer. Formulas are derived and quoted for calculating the d.c. resistances and leakage inductances of the winding portions. Theoretical expressions are derived for the variation with frequency etc. of the factors by which the d.c. values must be multiplied to obtain the corresponding a.c. values. These expressions are presented in the form of graphs, permitting the factors to be read as required.",False,,the effects of eddy currents in transformer windings are considered and a method is derived for calculating the variation of winding resistance and leakage inductance with frequency for transformers with singlelayer multilayer and sectionalised windings the method consists in dividing the winding into portions calculating the dc resistances and dc leakage inductances of each of these portions and then multiplying the dc values by appropriate factors to obtain the corresponding ac values these ac values are then referred to say the primary winding and summed to give the total winding resistance and leakage inductance of the transformer formulas are derived and quoted for calculating the dc resistances and leakage inductances of the winding portions theoretical expressions are derived for the variation with frequency etc of the factors by which the dc values must be multiplied to obtain the corresponding ac values these expressions are presented in the form of graphs permitting the factors to be read as required,[],[]
https://openalex.org/W4214493665,CvT: Introducing Convolutions to Vision Transformers,https://doi.org/10.1109/iccv48922.2021.00009,2021,"Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang","We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT.",True,,we present in this paper a new architecture named convolutional vision transformer cvt that improves vision transformer vit in performance and efficiency by introducing convolutions into vit to yield the best of both designs this is accomplished through two primary modifications a hierarchy of transformers containing a new convolutional token embedding and a convolutional transformer block leveraging a convolutional projection these changes introduce desirable properties of convolutional neural networks cnns to the vit architecture ie shift scale and distortion invariance while maintaining the merits of transformers ie dynamic attention global context and better generalization we validate cvt by conducting extensive experiments showing that this approach achieves stateoftheart performance over other vision transformers and resnets on imagenet1k with fewer parameters and lower flops in addition performance gains are maintained when pretrained on larger datasets eg imagenet22k and finetuned to downstream tasks pretrained on imagenet22k our cvtw24 obtains a top1 accuracy of 877 on the imagenet1k val set finally our results show that the positional encoding a crucial component in existing vision transformers can be safely removed in our model simplifying the design for higher resolution vision tasks code will be released at httpsgithubcommicrosoftcvt,[],[]
https://openalex.org/W3171125843,Pre-Trained Image Processing Transformer,https://doi.org/10.1109/cvpr46437.2021.01212,2021,"Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao","As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT",True,,as the computing power of modern hardware is increasing strongly pretrained deep learning models eg bert gpt3 learned on largescale datasets have shown their effectiveness over conventional methods the big progress is mainly contributed to the representation ability of transformer and its variant architectures in this paper we study the lowlevel computer vision task eg denoising superresolution and deraining and develop a new pretrained model namely image processing transformer ipt to maximally excavate the capability of transformer we present to utilize the wellknown imagenet benchmark for generating a large amount of corrupted image pairs the ipt model is trained on these images with multiheads and multitails in addition the contrastive learning is introduced for well adapting to different image processing tasks the pretrained model can therefore efficiently employed on desired task after finetuning with only one pretrained model ipt outperforms the current stateoftheart methods on various lowlevel benchmarks code is available at httpsgithubcomhuaweinoahpretrainedipt and httpsgiteecommindsporemindsporetreemastermodelzooresearchcvipt,[],[]
https://openalex.org/W3153465022,PCT: Point cloud transformer,https://doi.org/10.1007/s41095-021-0229-5,2021,"Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai‐Jiang Mu, Ralph R. Martin, Shi‐Min Hu","The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks.",True,,the irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing this paper presents a novel framework named point cloud transformerpct for point cloud learning pct is based on transformer which achieves huge success in natural language processing and displays great potential in image processing it is inherently permutation invariant for processing a sequence of points making it wellsuited for point cloud learning to better capture local context within the point cloud we enhance input embedding with the support of farthest point sampling and nearest neighbor search extensive experiments demonstrate that the pct achieves the stateoftheart performance on shape classification part segmentation and normal estimation tasks,[],[]
https://openalex.org/W4214893857,Segmenter: Transformer for Semantic Segmentation,https://doi.org/10.1109/iccv48922.2021.00717,2021,"Robin Strudel, Ricardo García, Ivan Laptev, Cordelia Schmid","Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embed-dings with a point-wise linear decoder or a mask trans-former decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.",True,,image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus in this paper we introduce segmenter a transformer model for semantic segmentation in contrast to convolutionbased methods our approach allows to model global context already at the first layer and throughout the network we build on the recent vision transformer vit and extend it to semantic segmentation to do so we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a pointwise linear decoder or a mask transformer decoder we leverage models pretrained for image classification and show that we can finetune them on moderate sized datasets available for semantic segmentation the linear decoder allows to obtain excellent results already but the performance can be further improved by a mask transformer generating class masks we conduct an extensive ablation study to show the impact of the different parameters in particular the performance is better for large models and small patch sizes segmenter attains excellent results for semantic segmentation it outperforms the state of the art on both ade20k and pascal context datasets and is competitive on cityscapes,[],[]
https://openalex.org/W3098824823,Transformers: State-of-the-Art Natural Language Processing,https://doi.org/10.5281/zenodo.5347031,2020,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clément Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush","Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",True,,recent progress in natural language processing has been driven by advances in both model architecture and model pretraining transformer architectures have facilitated building highercapacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks transformers is an opensource library with the goal of opening up these advances to the wider machine learning community the library consists of carefully engineered stateofthe art transformer architectures under a unified api backing this library is a curated collection of pretrained models made by and available for the community transformers is designed to be extensible by researchers simple for practitioners and fast and robust in industrial deployments the library is available at httpsgithubcomhuggingfacetransformers,[],[]
https://openalex.org/W3092462694,Deformable DETR: Deformable Transformers for End-to-End Object Detection,,2020,"Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai","DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at this https URL.",False,,detr has been recently proposed to eliminate the need for many handdesigned components in object detection while demonstrating good performance however it suffers from slow convergence and limited feature spatial resolution due to the limitation of transformer attention modules in processing image feature maps to mitigate these issues we proposed deformable detr whose attention modules only attend to a small set of key sampling points around a reference deformable detr can achieve better performance than detr especially on small objects with 10 times less training epochs extensive experiments on the coco benchmark demonstrate the effectiveness of our approach code is released at this https url,[],[]
https://openalex.org/W3012871709,Heterogeneous Graph Transformer,https://doi.org/10.1145/3366423.3380027,2020,"Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun","Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks. The dataset and source code of HGT are publicly available at https://github.com/acbull/pyHGT.",True,,recent years have witnessed the emerging success of graph neural networks gnns for modeling structured data however most gnns are designed for homogeneous graphs in which all nodes and edges belong to the same types making it infeasible to represent heterogeneous structures in this paper we present the heterogeneous graph transformer hgt architecture for modeling webscale heterogeneous graphs to model heterogeneity we design node and edgetype dependent parameters to characterize the heterogeneous attention over each edge empowering hgt to maintain dedicated representations for different types of nodes and edges to handle webscale graph data we design the heterogeneous minibatch graph sampling algorithmhgsamplingfor efficient and scalable training extensive experiments on the open academic graph of 179 million nodes and 2 billion edges show that the proposed hgt model consistently outperforms all the stateoftheart gnn baselines by 921 on various downstream tasks the dataset and source code of hgt are publicly available at httpsgithubcomacbullpyhgt,[],[]
https://openalex.org/W4295838474,Reformer: The Efficient Transformer,https://doi.org/10.48550/arxiv.2001.04451,2020,"Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya","Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",True,,large transformer models routinely achieve stateoftheart results on a number of tasks but training these models can be prohibitively costly especially on long sequences we introduce two techniques to improve the efficiency of transformers for one we replace dotproduct attention by one that uses localitysensitive hashing changing its complexity from ol2 to ollog l where l is the length of the sequence furthermore we use reversible residual layers instead of the standard residuals which allows storing activations only once in the training process instead of n times where n is the number of layers the resulting model the reformer performs on par with transformer models while being much more memoryefficient and much faster on long sequences,[],[]
https://openalex.org/W4214520160,Vision Transformers for Dense Prediction,https://doi.org/10.1109/iccv48922.2021.01196,2021,"René Ranftl, Alexey Bochkovskiy, Vladlen Koltun","We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",True,,we introduce dense prediction transformers an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks we assemble tokens from various stages of the vision transformer into imagelike representations at various resolutions and progressively combine them into fullresolution predictions using a convolutional decoder the transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage these properties allow the dense prediction transformer to provide finergrained and more globally coherent predictions when compared to fullyconvolutional networks our experiments show that this architecture yields substantial improvements on dense prediction tasks especially when a large amount of training data is available for monocular depth estimation we observe an improvement of up to 28 in relative performance when compared to a stateoftheart fullyconvolutional network when applied to semantic segmentation dense prediction transformers set a new state of the art on ade20k with 4902 miou we further show that the architecture can be finetuned on smaller datasets such as nyuv2 kitti and pascal context where it also sets the new state of the art our models are available at httpsgithubcomintelisldpt,[],[]
https://openalex.org/W3214586131,Transformer Tracking,https://doi.org/10.1109/cvpr46437.2021.00803,2021,"Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, Huchuan Lu","Correlation acts as a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion manner to consider the similarity between the template and the search region. However, the correlation operation itself is a local linear matching process, leading to lose semantic information and fall into local optimum easily, which may be the bottleneck of designing high-accuracy tracking algorithms. Is there any better feature fusion method than correlation? To address this issue, inspired by Transformer, this work presents a novel attention-based feature fusion network, which effectively combines the template and search region features solely using attention. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. Finally, we present a Transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression head. Experiments show that our TransT achieves very promising results on six challenging datasets, especially on large-scale LaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively 50 fps on GPU. Code and models are available at https://github.com/chenxin-dlut/TransT.",False,,correlation acts as a critical role in the tracking field especially in recent popular siamesebased trackers the correlation operation is a simple fusion manner to consider the similarity between the template and the search region however the correlation operation itself is a local linear matching process leading to lose semantic information and fall into local optimum easily which may be the bottleneck of designing highaccuracy tracking algorithms is there any better feature fusion method than correlation to address this issue inspired by transformer this work presents a novel attentionbased feature fusion network which effectively combines the template and search region features solely using attention specifically the proposed method includes an egocontext augment module based on selfattention and a crossfeature augment module based on crossattention finally we present a transformer tracking named transt method based on the siameselike feature extraction backbone the designed attentionbased fusion mechanism and the classification and regression head experiments show that our transt achieves very promising results on six challenging datasets especially on largescale lasot trackingnet and got10k benchmarks our tracker runs at approximatively 50 fps on gpu code and models are available at httpsgithubcomchenxindluttranst,[],[]
https://openalex.org/W4312560592,Video Swin Transformer,https://doi.org/10.1109/cvpr52688.2022.00320,2022,"Ze Liu, Ning Jia, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu","The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-l accuracy on Kinetics-400 and 85.9 top-l accuracy on Kinetics-600 with ~20× less pre-training data and ~3× smaller model size) and temporal modeling (69.6 top-l accuracy on Something-Something v2).",False,,the vision community is witnessing a modeling shift from cnns to transformers where pure transformer architectures have attained top accuracy on the major video recognition benchmarks these video models are all built on transformer layers that globally connect patches across the spatial and temporal dimensions in this paper we instead advocate an inductive bias of locality in video transformers which leads to a better speedaccuracy tradeoff compared to previous approaches which compute selfattention globally even with spatialtemporal factorization the locality of the proposed video architecture is realized by adapting the swin transformer designed for the image domain while continuing to leverage the power of pretrained image models our approach achieves stateoftheart accuracy on a broad range of video recognition benchmarks including on action recognition 849 topl accuracy on kinetics400 and 859 topl accuracy on kinetics600 with 20 less pretraining data and 3 smaller model size and temporal modeling 696 topl accuracy on somethingsomething v2,[],[]
https://openalex.org/W3180355996,Taming Transformers for High-Resolution Image Synthesis,https://doi.org/10.1109/cvpr46437.2021.01268,2021,"Patrick Esser, Robin Rombach, Björn Ommer","Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.",True,,designed to learn longrange interactions on sequential data transformers continue to show stateoftheart results on a wide variety of tasks in contrast to cnns they contain no inductive bias that prioritizes local interactions this makes them expressive but also computationally infeasible for long sequences such as highresolution images we demonstrate how combining the effectiveness of the inductive bias of cnns with the expressivity of transformers enables them to model and thereby synthesize highresolution images we show how to i use cnns to learn a contextrich vocabulary of image constituents and in turn ii utilize transformers to efficiently model their composition within highresolution images our approach is readily applied to conditional synthesis tasks where both nonspatial information such as object classes and spatial information such as segmentations can control the generated image in particular we present the first results on semanticallyguided synthesis of megapixel images with transformers project page at httpsgitiojllvy,[],[]
https://openalex.org/W4225672218,Restormer: Efficient Transformer for High-Resolution Image Restoration,https://doi.org/10.1109/cvpr52688.2022.00564,2022,"Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming–Hsuan Yang","Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.",True,,since convolutional neural networks cnns perform well at learning generalizable image priors from largescale data these models have been extensively applied to image restoration and related tasks recently another class of neural architectures transformers have shown significant performance gains on natural language and highlevel vision tasks while the transformer model mitigates the shortcomings of cnns ie limited receptive field and inadaptability to input content its computational complexity grows quadratically with the spatial resolution therefore making it infeasible to apply to most image restoration tasks involving highresolution images in this work we propose an efficient transformer model by making several key designs in the building blocks multihead attention and feedforward network such that it can capture longrange pixel interactions while still remaining applicable to large images our model named restoration transformer restormer achieves stateoftheart results on several image restoration tasks including image deraining singleimage motion deblurring defocus deblurring singleimage and dualpixel data and image denoising gaussian grayscalecolor denoising and real image denoising the source code and pretrained models are available at httpsgithubcomswz30restormer,[],[]
https://openalex.org/W4212875960,UNETR: Transformers for 3D Medical Image Segmentation,https://doi.org/10.1109/wacv51458.2022.00181,2022,"Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett A. Landman, Holger R. Roth, Daguang Xu","Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful ""U-shaped"" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",True,,fully convolutional neural networks fcnns with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade in fcnns the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder despite their success the locality of convolutional layers in fcnns limits the capability of learning longrange spatial dependencies inspired by the recent success of transformers for natural language processing nlp in longrange sequence learning we reformulate the task of volumetric 3d medical image segmentation as a sequencetosequence prediction problem we introduce a novel architecture dubbed as unet transformers unetr that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multiscale information while also following the successful ushaped network design for the encoder and decoder the transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output we have validated the performance of our method on the multi atlas labeling beyond the cranial vault btcv dataset for multiorgan segmentation and the medical segmentation decathlon msd dataset for brain tumor and spleen segmentation tasks our benchmarks demonstrate new stateoftheart performance on the btcv leaderboard,[],[]
https://openalex.org/W3172509117,Bottleneck Transformers for Visual Recognition,https://doi.org/10.1109/cvpr46437.2021.01625,2021,"Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, Ashish Vaswani","We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt [67] evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in ""compute"" <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>",True,,we present botnet a conceptually simple yet powerful backbone architecture that incorporates selfattention for multiple computer vision tasks including image classification object detection and instance segmentation by just replacing the spatial convolutions with global selfattention in the final three bottleneck blocks of a resnet and no other changes our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters with minimal overhead in latency through the design of botnet we also point out how resnet bottleneck blocks with selfattention can be viewed as transformer blocks without any bells and whistles botnet achieves 444 mask ap and 497 box ap on the coco instance segmentation benchmark using the mask rcnn framework surpassing the previous best published single model and single scale results of resnest 67 evaluated on the coco validation set finally we present a simple adaptation of the botnet design for image classification resulting in models that achieve a strong performance of 847 top1 accuracy on the imagenet benchmark while being up to 164x faster in compute sup xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlink1sup time than the popular efficientnet models on tpuv3 hardware we hope our simple and effective approach will serve as a strong baseline for future research in selfattention models for vision sup xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlink2sup,[],[]
https://openalex.org/W2964051877,Multimodal Transformer for Unaligned Multimodal Language Sequences,https://doi.org/10.18653/v1/p19-1656,2019,"Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis‐Philippe Morency, Ruslan Salakhutdinov","Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.",True,,yaohung hubert tsai shaojie bai paul pu liang j zico kolter louisphilippe morency ruslan salakhutdinov proceedings of the 57th annual meeting of the association for computational linguistics 2019,[],[]
https://openalex.org/W3170874841,Training data-efficient image transformers & distillation through attention,,2021,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jeǵou",,True,,,[],[]
https://openalex.org/W3121523901,Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet,https://doi.org/10.1109/iccv48922.2021.00060,2021,"Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis E. H. Tay, Jiashi Feng, Shuicheng Yan","Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>",True,,transformers which are popular for language modeling have been explored for solving vision tasks recently eg the vision transformer vit for image classification the vit model splits each image into a sequence of tokens with fixed length and then applies multiple transformer layers to model their global relation for classification however vit achieves inferior performance to cnns when trained from scratch on a midsize dataset like imagenet we find it is because 1 the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels leading to low training sample efficiency 2 the redundant attention backbone design of vit leads to limited feature richness for fixed computation budgets and limited training samples to overcome such limitations we propose a new tokenstotoken vision transformer t2tvtt which incorporates 1 a layerwise tokenstotoken t2t transformation to progressively structurize the image to tokens by recursively aggregating neighboring tokens into one token tokenstotoken such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced 2 an efficient backbone with a deepnarrow structure for vision transformer motivated by cnn architecture design after empirical study notably t2tvit reduces the parameter count and macs of vanilla vit by half while achieving more than 30 improvement when trained from scratch on imagenet it also outperforms resnets and achieves comparable performance with mobilenets by directly training on imagenet for example t2tvit with comparable size to resnet50 215m parameters can achieve 833 top1 accuracy in image resolution 384x384 on imagenet sup xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlink1sup,[],[]
https://openalex.org/W3034655362,Meshed-Memory Transformer for Image Captioning,https://doi.org/10.1109/cvpr42600.2020.01059,2020,"Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara","Transformer-based architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts like image captioning, however, is still largely under-explored. With the aim of filling this gap, we present M <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> - a Meshed Transformer with Memory for Image Captioning. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features. Experimentally, we investigate the performance of the M <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> Transformer and different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves a new state of the art in single-model and ensemble configurations on the ""Karpathy"" test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer.",True,,transformerbased architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding their applicability to multimodal contexts like image captioning however is still largely underexplored with the aim of filling this gap we present m sup xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlink2sup  a meshed transformer with memory for image captioning the architecture improves both the image encoding and the language generation steps it learns a multilevel representation of the relationships between image regions integrating learned a priori knowledge and uses a meshlike connectivity at decoding stage to exploit low and highlevel features experimentally we investigate the performance of the m sup xmlnsmmlhttpwwww3org1998mathmathml xmlnsxlinkhttpwwww3org1999xlink2sup transformer and different fullyattentive models in comparison with recurrent ones when tested on coco our proposal achieves a new state of the art in singlemodel and ensemble configurations on the karpathy test split and on the online test server we also assess its performances when describing objects unseen in the training set trained models and code for reproducing the experiments are publicly available at httpsgithubcomaimagelabmeshedmemorytransformer,[],[]
https://openalex.org/W4214614183,Multiscale Vision Transformers,https://doi.org/10.1109/iccv48922.2021.00675,2021,"Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer","We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10× more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast.",True,,we present multiscale vision transformers mvit for video and image recognition by connecting the seminal idea of multiscale feature hierarchies with transformer models multiscale transformers have several channelresolution scale stages starting from the input resolution and a small channel dimension the stages hierarchically expand the channel capacity while reducing the spatial resolution this creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple lowlevel visual information and deeper layers at spatially coarse but complex highdimensional features we evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pretraining and are 510 more costly in computation and parameters we further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers code is available at httpsgithubcomfacebookresearchslowfast,[],[]
https://openalex.org/W2964979676,Learning RoI Transformer for Oriented Object Detection in Aerial Images,https://doi.org/10.1109/cvpr.2019.00296,2019,"Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, Qikai Lu","Object detection in aerial images is an active yet challenging task in computer vision because of the bird’s-eye view perspective, the highly complex backgrounds, and the variant appearances of objects. Especially when detecting densely packed objects in aerial images, methods relying on horizontal proposals for common object detection often introduce mismatches between the Region of Interests (RoIs) and objects. This leads to the common misalignment between the final object classification confidence and localization accuracy. In this paper, we propose a RoI Transformer to address these problems. The core idea of RoI Transformer is to apply spatial transformations on RoIs and learn the transformation parameters under the supervision of oriented bounding box (OBB) annotations. RoI Transformer is with lightweight and can be easily embedded into detectors for oriented object detection. Simply apply the RoI Transformer to light head RCNN has achieved state-of-the-art performances on two common and challenging aerial datasets, i.e., DOTA and HRSC2016, with a neglectable reduction to detection speed. Our RoI Transformer exceeds the deformable Position Sensitive RoI pooling when oriented bounding-box annotations are available. Extensive experiments have also validated the flexibility and effectiveness of our RoI Transformer.",False,,object detection in aerial images is an active yet challenging task in computer vision because of the birdseye view perspective the highly complex backgrounds and the variant appearances of objects especially when detecting densely packed objects in aerial images methods relying on horizontal proposals for common object detection often introduce mismatches between the region of interests rois and objects this leads to the common misalignment between the final object classification confidence and localization accuracy in this paper we propose a roi transformer to address these problems the core idea of roi transformer is to apply spatial transformations on rois and learn the transformation parameters under the supervision of oriented bounding box obb annotations roi transformer is with lightweight and can be easily embedded into detectors for oriented object detection simply apply the roi transformer to light head rcnn has achieved stateoftheart performances on two common and challenging aerial datasets ie dota and hrsc2016 with a neglectable reduction to detection speed our roi transformer exceeds the deformable position sensitive roi pooling when oriented boundingbox annotations are available extensive experiments have also validated the flexibility and effectiveness of our roi transformer,[],[]
https://openalex.org/W2963563276,Video Action Transformer Network,https://doi.org/10.1109/cvpr.2019.00033,2019,"Rohit Girdhar, João Carreira, Carl Doersch, Andrew Zisserman","We introduce the Action Transformer model for recognizing and localizing human actions in video clips. We repurpose a Transformer-style architecture to aggregate features from the spatiotemporal context around the person whose actions we are trying to classify. We show that by using high-resolution, person-specific, class-agnostic queries, the model spontaneously learns to track individual people and to pick up on semantic context from the actions of others. Additionally its attention mechanism learns to emphasize hands and faces, which are often crucial to discriminate an action - all without explicit supervision other than boxes and class labels. We train and test our Action Transformer network on the Atomic Visual Actions (AVA) dataset, outperforming the state-of-the-art by a significant margin using only raw RGB frames as input.",True,,we introduce the action transformer model for recognizing and localizing human actions in video clips we repurpose a transformerstyle architecture to aggregate features from the spatiotemporal context around the person whose actions we are trying to classify we show that by using highresolution personspecific classagnostic queries the model spontaneously learns to track individual people and to pick up on semantic context from the actions of others additionally its attention mechanism learns to emphasize hands and faces which are often crucial to discriminate an action  all without explicit supervision other than boxes and class labels we train and test our action transformer network on the atomic visual actions ava dataset outperforming the stateoftheart by a significant margin using only raw rgb frames as input,[],[]
https://openalex.org/W2891483507,Monolithic transformers for silicon RF IC design,https://doi.org/10.1109/4.868049,2000,J.R. Long,"A comprehensive review of the electrical performance of passive transformers fabricated in silicon IC technology is presented. Two types of transformer construction are considered in detail, and the characteristics of two-port (1:1 and 1:n turns ratio) and multiport transformers (i.e., baluns) are presented from both computer simulation and experimental measurements. The effects of parasitics and imperfect coupling between transformer windings are outlined from the circuit point of view. Resonant tuning is shown to reduce the losses between input and output at the expense of operating bandwidth. A procedure for estimating the size of a monolithic transformer to meet a given specification is outlined, and circuit examples are used to illustrate the applications of the monolithic transformer in RF ICs.",False,,a comprehensive review of the electrical performance of passive transformers fabricated in silicon ic technology is presented two types of transformer construction are considered in detail and the characteristics of twoport 11 and 1n turns ratio and multiport transformers ie baluns are presented from both computer simulation and experimental measurements the effects of parasitics and imperfect coupling between transformer windings are outlined from the circuit point of view resonant tuning is shown to reduce the losses between input and output at the expense of operating bandwidth a procedure for estimating the size of a monolithic transformer to meet a given specification is outlined and circuit examples are used to illustrate the applications of the monolithic transformer in rf ics,[],[]
