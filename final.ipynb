{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b30dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and select a small batch for fine-tuning\n",
    "df = pd.read_csv(\"clustered_papers.csv\").dropna(subset=[\"title\", \"clean_abstract\"])\n",
    "df = df.head(20)  # fine-tune on just 20 samples\n",
    "\n",
    "# Format input/output pairs\n",
    "def build_input(row):\n",
    "    return f\"Title: {row['title']}\\nAuthors: {row.get('authors', 'Unknown')}\\nKeywords: {row.get('keywords', '')}\\nAbstract: {row['clean_abstract']}\"\n",
    "\n",
    "def build_target(row):\n",
    "    return f\"This paper discusses {row.get('keywords', 'key topics')} and contributes to cluster {row.get('gmm_cluster', 'X')} research.\"\n",
    "\n",
    "dataset = [{\"input\": build_input(row), \"target\": build_target(row)} for _, row in df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a09d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Response text: {\"totalHits\":1832062,\"limit\":10,\"offset\":0,\"results\":[{\"acceptedDate\":\"\",\"arxivId\":null,\"authors\":[{\"name\":\"Benito Picazo, Jes\\u00fas\"},{\"name\":\"Dom\\u00ednguez-Merino, Enrique\"},{\"name\":\"L\\u00f3pez-Rubio, Ezequiel\"},{\"name\":\"Ortiz-de-lazcano-Lobato, Juan Miguel\"},{\"name\":\"Palomo, Esteban J.\"}],\"citationCount\":0,\"contributors\":[\"Jesus\"],\"outputs\":[\"https:\\/\\/api.core.ac.uk\\/v3\\/outputs\\/468593434\",\"https:\\/\\/api.core.ac.uk\\/v3\\/outputs\\/214840920\",\"https:\\/\\/api.core.ac.uk\\/v3\\/outputs\\/323333340\"],\"createdDate\":\"2019-07-09T14:23:56\",\"dataProviders\":[{\"id\":4786,\"name\":\"\",\"url\":\"https:\\/\\/api.core.ac.uk\\/v3\\/data-providers\\/4786\",\"logo\":\"https:\\/\\/api.core.ac.uk\\/data-providers\\/4786\\/logo\"},{\"id\":2072,\"name\":\"\",\"url\":\"https:\\/\\/api.core.ac.uk\\/v3\\/data-providers\\/2072\",\"logo\":\"https:\\/\\/api.core.ac.uk\\/data-providers\\/2072\\/logo\"},{\"id\":11082,\"name\":\"\",\"url\":\"https:\\/\\/api.core.ac.uk\\/v3\\/data-providers\\/11082\",\"logo\":\"https:\\/\\/api.core.ac.uk\\/data-providers\\/11082\\/logo\"}],\"depositedDate\":\"\",\"abstract\":\"Automatic video surveillance systems are usually designed to detect anomalous objects being present in a scene or behaving dangerously. In order to perform adequately, they must incorporate models able to achieve accurate pattern recognition\\r\\nin an image, and deep learning neural networks excel at this task. However, exhaustive scan of the full image results in multiple image blocks or windows to analyze, which could make the time performance of the system very poor when implemented on low cost devices. This paper presents a system which attempts to\\r\\ndetect abnormal moving objects within an area covered by a PTZ camera while it is panning. The decision about the block of the image to analyze is based on a mixture distribution composed of two components: a uniform probability distribution, which\\r\\nrepresents a blind random selection, and a mixture of Gaussian probability distributions. Gaussian distributions represent windows in the image where anomalous objects were detected previously and contribute to generate the next window to analyze close to those windows of interest. The system is implemented on\\r\\na Raspberry Pi microcontroller-based board, which enables the design and implementation of a low-cost monitoring system that is able to perform image processing.Universidad de M\\u00e1laga. Campus de Excelencia Internacional Andaluc\\u00eda Tech\",\"documentType\":\"research\",\"doi\":\"10.1109\\/ijcnn.2018.8489437\",\"downloadUrl\":\"https:\\/\\/core.ac.uk\\/download\\/214840920.pdf\",\"fieldOfStudy\":null,\"fullText\":\"Deep learning-based anomalous object detectionsystem powered by microcontroller for PTZ camerasJesu\\u00b4s Benito-Picazo, Enrique Dom\\u0131\\u00b4nguez, Esteban J. Palomo,Ezequiel Lo\\u00b4pez-Rubio, Juan Miguel Ortiz-de-Lazcano-LobatoDepartment of Computer Languages and Computer ScienceUniversity of Ma\\u00b4lagaBulevar Louis Pasteur, 35. 29010 Ma\\u00b4laga. Spain.{jpicazo,enriqued,ejpalomo,ezeqlr,jmortiz}@lcc.uma.esAbstract\\u2014Automatic video surveillance systems are usuallydesigned to detect anomalous objects being present in a scene orbehaving dangerously. In order to perform adequately, they mustincorporate models able to achieve accurate pattern recognitionin an image, and deep learning neural networks excel at this task.However, exhaustive scan of the full image results in multipleimage blocks or windows to analyze, which could make the timeperformance of the system very poor when implemented on lowcost devices. This paper presents a system which attempts todetect abnormal moving objects within an area covered by a PTZcamera while it is panning. The decision about the block of theimage to analyze is based on a mixture distribution composedof two components: a uniform probability distribution, whichrepresents a blind random selection, and a mixture of Gaus-sian probability distributions. Gaussian distributions representwindows in the image where anomalous objects were detectedpreviously and contribute to generate the next window to analyzeclose to those windows of interest. The system is implemented ona Raspberry Pi microcontroller-based board, which enables thedesign and implementation of a low-cost monitoring system thatis able to perform image processing.Index Terms\\u2014Foreground detection, feed forward neural net-work, PTZ camera, convolutional neural networkI. INTRODUCTIONVideo surveillance systems have become an extremely ac-tive research area due to increasing levels of social conflictand public awareness about security issues. This has led tomotivation for the development of robust and precise automaticvideo surveillance systems, which need a source of data to beprocessed and analysed. Fixed and PTZ cameras have becomethe essential information source in those greatly demandedsystems [1]\\u2013[3]. Real time operation is essential for thesuccessful deployment of these systems [4], [5].Research on computer vision systems based on pan-tilt-zoom (PTZ) cameras has been intense for many years [6]\\u2013[10]. Nevertheless, there is a lack of a comprehensive theorywhich sets the foundations for the development of practicalsystems. Fragmentary approaches that are limited to someparts of the problem are available, but it is still not clear howto combine them to yield complete and reliable systems thatcan be deployed in many situations. In the present work wefocus on the detection of anomalous objects in the scene whilea PTZ camera is panning, tilting or zooming in or out, in orderto cover the entire environment which the video surveillancesystem has to be aware of.Nowadays, deep learning have become increasingly popularand widely applied to computer vision systems [11], [12], suchas object recognition (e.g. handwriting, face, behavior...) orimage classification. Hundreds of papers have been publishedin the last years providing different types of deep neuralnetworks, but all of them need the help of GPU-acceleratedcomputing techniques in order to achieve their objectives.Theses applications are normally based on high performanceand expensive hardware, which requires a high power con-sumption.Microcontroller boards are economic, small and flexiblehardware devices. They are frequently employed in motiondetection systems due to their low energy consumption andreduced cost [13]\\u2013[15]. A flexible Printed Board Circuit (PCB)prototype which integrates a microcontroller has been pro-posed to estimate motion and proximity [16]. In this prototype,eight photodiodes are used as light sensors. The efficiency ofsolar energy plants can be improved by low power systemswhich estimate cloud motion [17]. The approximation ofthe cloud motion vectors is carried out by an embeddedmicrocontroller, so that the arrangement of the solar panelscan be optimized for maximum electricity output. Energy-saving street lighting for smart cities can be accomplishedby low power motion detection systems equipped with lowconsumption microcontrollers and wireless communicationdevices [18]. This way, the street lamps are switched onwhen people are present in their surroundings. Recently, amotion detection algorithm based on Self-Organizing Maps(SOMs) was developed in an Arduino DUE board [19]. Theimplementation of the SOM algorithm was employed as amotion detector for static cameras in a video surveillancesystem.In this paper, a low energy and low cost system for movinganomalous object detection is proposed. The proposal is basedon deep learning algorithms and powered for microcontrollersusing PTZ cameras. Initial experiments carried out show avery promising proposal which delivers competitive results.The rest of the paper is organized as follows: Section II isreserved for the formulation of the mathematical model thissystem relies on. In Section III both the software architectureAn accepted article is a version that has been revised by the author to incorporate review suggestions and that has been accepted by IEEE for publication. The final, published version is the reviewed and accepted article, with copy-editing, proofreading, and formatting added by IEEE. \\u00a9 2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting\\/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.and the hardware architecture this system is supposed to bedeployed in, are described. Section IV is intended to illustratethe results of the experiments performed with the objectiveof determining the system performance when deployed in araspberry Pi 3 model B microcontroller-based board. Lastsection will be reserved for some conclusions based on theresults obtained from the experiments performed.II. METHODOLOGYWe consider a certain object as anomalous in a certainenvironment when its presence should generate an alert in anysurveillance system installed in the cited environment.In this section, our proposed anomalous object detectionmodel is detailed. A set of active detections is to be maintainedduring the operation of the system. Each detection is a tuple(pii, x1, x2, x3) where:\\u2022 pii is the a priori probability or mixing weight\\u2022 x1 is the pixel row of the center of the detection in thecurrent video frame coordinate system.\\u2022 x2 is the pixel column of the center of the detection inthe current video frame coordinate system.\\u2022 x3 is the size of the detection, i.e. the length (in pixel)of the side of the square which defines the detection.In order to translate a detection from the previous frame tothe next one, the following equations must be used:pii (t) = \\u03b1pii (t\\u2212 1) (1)x1 (t) = x1 (t\\u2212 1) (2)x2 (t) = x2 (t\\u2212 1) + \\u03b4 (3)x3 (t) = x3 (t\\u2212 1) (4)where \\u03b1 is a damping factor, 0 \\u003C \\u03b1 \\u003C 1, and \\u03b4 is thenumber of pixels per frame of the apparent panning motion(as estimated in [20]). A detection goes inactive whenever isgoes out of the current frame.Let us define x = (x1, x2, x3). The set of valid values forx is:V = [1, Nrows]\\u00d7 [1, Ncols]\\u00d7 [Smin, Smax] \\u2282 R3 (5)where the video frame size is Nrows \\u00d7Ncols pixels, and theminimum and maximum side lengths of the detections to beconsidered are Smin and Smax, respectively.Let us consider the following probability density functionin order to sample potential locations for detections:p (x) =qUV (x) + (1\\u2212 q) 1NdetectionsNdetections\\u2211i=1piiGauss (xi, \\u03c3)(6)where UV (x) is the uniform distribution on V , Gauss (\\u00b5, \\u03c3)stands for the spherical multivariate gaussian distribution withmean \\u00b5 and standard deviation \\u03c3, Ndetections is the currentnumber of active detections, q \\u2208 (0, 1) is a tunable parameterand \\u03c3 is a standard deviation tunable parameter. The aim is tofocus on the parts of the video frame where active detectionshave been found before, while we also pay some attention toall other regions of the video frame to search for previouslyundetected objects.The object detection algorithm which is associated with theabove defined model is as follows:1) The set of active detections A is initialized to the emptyset.2) A new video frame is acquired from the camera.3) All active detections are updated by Equations (1)-(4). All updated detections which fall outside V arediscarded, as they have become inactive.4) A set of M random samples are drawn from (6). Foreach random sample, the associated frame window isfound and resized to the shape required by a suitableconvolutional neural network (CNN). Then the sampleis fed to the CNN. If the resulting output reveals thatan object has likely been found, then the sample isinserted into A, with a weight which is proportional tothe likelihood that an object is really there.5) Go to step 2.In the following section, a possible way to implement themodel and its associated algorithm on low power hardware isproposed.III. SYSTEM ARCHITECTUREBecause of the big amount of data this task comprises,moving foreground anomalous objects detection and classi-fication from images supplied by a PTZ camera could beconsidered as a high computing power consuming task. How-ever, it would be desirable to make such a system with thehighest autonomy levels and as inexpensive as possible. Forthese reasons, apart from the mathematical model proposedin Section II a system architecture has been proposed that iscapable of accomplishing this tasks with a small fraction of thecomputing power and electric consumption required for thesekinds of systems. The proposed system architecture consistson a convolutional neural network-based object classificationand tracking software, specially designed and optimized for itsdeployment in a microcontroller-based board that will result ina functional yet inexpensive and low power consuming system.A. Software architectureSystem software architecture can be checked in Figure 2and consists of a program developed in c++ which acceptsa continuous stream of images supplied by a PTZ cameraemulator named \\u2019virtual PTZ\\u2019 [21]. This software simulatesthe functionality of a real PTZ camera. More precisely, it iscapable of emulating the performance of a Sony SNC-RZ50NPTZ camera when supplied with panoramic spherical videofootage. In fact, all the video footage employed in this projectwas obtained from a Point Grey Ladybug 3 Spherical camera(Figure 1) and supplied to the Virtual PTZ software.The reason for us considering this software as a validframework for researching is mostly its capability of replacingan actual PTZ camera by providing the operator the possibilityof moving the focus all along the spherical scene, just as witha real PTZ camera, in a way that allows total control over theimages that are being supplied to the program without havingto face the dynamical and electrical issues affecting physicaldevices.Fig. 1. 360\\u25e6 spherical images supplied by the Point Grey Ladybug 3 Sphericalcamera.The second part of the software architecture is a convo-lutional neural network which will be in charge of detectingand characterizing the objects contained in each one of thewindows guessed by the random window generator in eachframe. An efficient way for working with CNNs is to selectan existing Deep Learning framework that will facilitate theuse and implementation of these networks. At the same time,it is very important to have in mind that the computingpower of the hardware we are going to use is limited so anoptimized library for deploying CNNs in microcontrollers isneeded. For all this reasons, we have employed for our projectthe Microsoft Cognitive Toolkit (CNTK) combined with theEmbedded Learning Library (ELL) by Microsoft. First oneis a Deep Learning framework intended for designing andtraining Convolutional Neural Networks and the second oneis a special library to improve the performance of the codeby using a series of dlls aiming to properly parallelize everyoperation so it can take advantage of all the performance ofthese microcontroller architectures.The third part of the software architecture described in thisdocument is a c++ program that implements the mathematicalmodel explained in Section II. As can be seen in the diagramproposed in Fig. 2, this program will accept a video framesupplied by the Virtual PTZ software and it will launch afixed number of windows whose location in the frame willbe determined by the result of the gaussian-uniform mixtureproposed in the cited mathematical model. Next, the portionof the main frame delimited by every window will be fed tothe CNN who will determine whether the guessed windowis containing any object appearing in our list of anomalousobjects. May that be the case, this new detection is added tothe detections set (A) and the window will be reflected andupgraded in the main frame as the camera keeps moving.Fig. 2. Overview of the proposed implementationB. Hardware architectureHardware selection is a very important issue when it comesto microcontroller-powered Deep Learning applications. Gen-erally, projects consisting on object detection and categoriza-tion would be supposed to have high power computing needsand a good performance, but at the same time, they should beaffordable and low-energy consuming so they may be installedin places where no general power network is accessible. For allthese reasons, raspberry Pi microcontroller-based boards havebeen considered to be used in our project. More precisely araspberry Pi 3 Model B.Fig. 3. raspberry Pi 3 model B overviewIt presents a Broadcom BCM2837 microcontroller, featuringa CortexV8 Quad Core CPU running at 1200 Mhz from ARM,1GB of RAM memory and a microSD data storage card. Whenit comes to electrical features, it can be powered by a 5.1 Vpower source and its max power consumption is up to 2.5A approximately at max operating load using USB externaldevices.IV. EXPERIMENTAL RESULTSSection II describes our detection system as an algorithmthat is capable of detecting the presence of certain number ofobjects belonging to certain categories considered anomalousfor a particular partially controlled environment, by analyzinga video stream of the cited environment captured by a PTZcamera which is performing a panoramic movement. Framesfrom this video stream are supplied to a random windowgeneration module that will extract a certain number of por-tions of the frame that will be fed to a convolutional neuralnetwork that will be in charge of characterizing the objectsframed by the cited windows. This way, the program willbe capable of determining whether anomalous objects appearin the delimited image sections and therefore, in the chosenenvironment at the time the video was recorded.As was said in Section III, because of its speed andadaptation to the raspberry Pi architecture and its exploitationof parallelism, we have selected a CNN designed and trainedusing the convolutional neural networks implementation fromMicrosoft cognitive toolkit (Microsoft CNTK)1 combined withthe Embedded Learning Library, also from Microsoft2, whichoptimizes neural networks for their use in non-GPU micro-controller architectures such as raspberry Pi.Aiming to increase our control over the experiments, testshave been performed from 16 fps panoramic videos sup-plied by the Virtual PTZ where the camera is performinga panoramic movement to the left at 16\\u25e6\\/s angular speed.This speed also determines the \\u03b4 parameter from the detectionactualization fixing it to 5 pixels\\/frame.All set up, the operating of the main program follows thediagram in Figure 4 First, a new frame is acquired fromthe virtual PTZ software. Next, the program spreads certainamount of windows that represent potential detections all overthe frame, each one localized in the place determined by thep function expressed in Eq. 6. Then, the frame\\u2019s sectionsdelimited by each one of those windows are resized andproperly transformed so they can be fed to a pre-trainedconvolutional neural network which will determine whetherthe frame section delimited by the window contains any of theobjects of a previously elaborated anomalous object categorylist. Should the neural network finds some object belongingto the anomalous objects categories in a certain window, thiswindow\\u2019s frame will appear drawn in the original input frame,and depending on the prediction confidence degree achieved itwill be drawn in different colors using green for a value above70%, yellow for a value between 40% and 70% and red for avalue under 40%.In order to achieve adequate experiments to test the accuracyof our system and to confront our mixture-based windowgenerator against other random window generator systems, itis important to use a controlled scenario that allows us to testthe methods in the same conditions. With this objective inmind, a certain video supplied by the virtual PTZ camera has1https:\\/\\/www.microsoft.com\\/en-us\\/cognitive-toolkit\\/2https:\\/\\/microsoft.github.io\\/ELL\\/been prepared by introducing, using a video edition software,11 random moving objects to be detected: cat, dog, laptop, hat,bird, banana, soccer ball, sunglasses, wall clock and backpack.Experiments consisted of counting the number of objectsdetected by the system by performing 10 360\\u25e6 recognitionpasses to the mentioned 360\\u25e6 modified video for a numberof random windows that goes from 1 to 10. These operationswere performed for each one of the two potential detectionsgeneration methods considered in this document: A gaussian-uniform mixture and a uniform distribution.When it comes to test execution, it is prudent to constrainthe amount of variable values so the amount of cases to tryare not unmanageable. With this purpose, according to themathematical model described in Section II, and leaning onour empirical results, we have fixed some values so the amountof tests that are required is less extent and the results aremore significant. Thus, the damping factor \\u03b1 will be 0.1, thestandard deviation \\u03c3, will be assigned the value 0.5 and qwill also be fixed to 0.5. For the correct performance of thissystem, the choice of the neural network that is going to be incharge for the detection and categorization of the anomalousobjects is critical, because it has to be as simple as possibleas it has to work fast enough in a low computation powermicrocontroller architecture but at the same time it has to beaccurate enough for the system performance to be meaningful.These reasons led us to select a particular CNN designedby the Microsoft Embedded Learning Library team, for thedataset of the Large Scale Visual Recognition Challenge 2012(ILSVRC2012), which architecture is illustrated in the TableI. It is important to remark that only the classification stage isdone online using the raspberry Pi. The training of the networkhas been achieved offline using a NVIDIA TITAN X GPU.The reason for this is that the training would be orders ofmagnitude slower if done online with the raspberry Pi itself.This would be very unefficient and there is no need for doingit as the network can be periodically retrained offline to detectnew objects and sent to the raspberry Pi so it can use it.Experiments were performed by making the virtual PTZto do 10 360\\u25e6 scans for each number of potential detectionswhile running on the raspberry Pi. This number of potentialdetections goes from 1 to 10. And the results provided wouldbe the mean number of anomalous objects detected for eachnumber of potential detections. Once the experiments areproperly performed, results for the mean number of objectsdetected by the two methods can be checked in Figure 5.This figure illustrates how the gaussian-uniform mixturepotential detection generator designed for this investigationachieves a higher mean number of anomalous object detectionsfor a number equal or smaller than 3 while it achieves the samenumber of detections for 4 windows or higher. This makesthe mixture method more accurate and functional, especiallyfor its deployment in a raspberry Pi microcontroller where anumber of potential windows higher than 3 reduces drasticallythe system performance.Time performance is a critical matter when facing thedesign of an deep learning-based moving object detection andFig. 4. Working diagram of the algorithm\\u2019s regular operation mode.Accuracy ILSVRC201280.71% (Top 5)57.56% (Top 1)Input 256 x 256 x {B,G,R}ArchitectureConvolution, 224x224x16, size=3x3, stride=1Pooling, 112x112x16 size=2x2, stride=2Convolution 112x112x64 size=3x3, stride=1Pooling 56x56x64 size=2x2, stride=2Convolution 56x56x64 size=3x3, stride=1Pooling 28x28x64 size=2x2, stride=2Convolution 28x28x128 size=3x3, stride=1Pooling 14x14x128 size=2x2, stride=2Convolution 14x14x256 size=3x3, stride=1Pooling 7x7x256 size=2x2, stride=2Convolution 7x7x512 size=3x3, stride=1Pooling 4x4x512 size=2x2, stride=2Convolution 4x4x1024 size=3x3, stride=1Convolution 4x4x1000 size=1x1, stride=1Pooling 1x1x1000 size=4x4, stride=1Softmax 1x1x1000Output ILSVRC2012 1000 classesTABLE IARCHITECTURE OF THE CNNcategorisation software, more specially when this software isgoing to be deployed in a low computing capability hardwaresuch as the one described in Section III. Hence, several timeperformance tests have been performed after deploying thedescribed system in the raspberry Pi microcontroller. Just asit was done with the previously illustrated accuracy tests,experiments consisted of counting the number of objectsdetected by the system by performing 10 360\\u25e6 recognitionpasses to the mentioned 360\\u25e6 modified video for a numberof random windows that goes from 1 to 10. These operationswere performed for each one of the gaussian-uniform mixtureand the uniform distribution. Results obtained can be checkedin the Table II and Fig. 6.These results illustrate the system performance in framesper second when deployed in a raspberry Pi 3 model B.According to them, it can be observed that even though itis not capable of real-time object detection, is important to0 2 4 6 8 10Number of random potential detections01020304050Number of anomalous objectsMixtureRandomFig. 5. Number of objects detected from the scene for each method versusnumber of random windows generated in each frameremark that the system described in this paper is capableof detecting foreground objects which are in motion in anon-controlled environment in half a second approximately.For this reasons we think our proposal is justified in termsof autonomy and price\\/performance relation, as it can bedeployed in a hardware that costs approximately 25$. Whenit comes to time performance comparison, the mixture modelperformance seems to be similar than the model involving theuniform distribution potential detection generator. Hence, theaccuracy improvement illustrated in the figures and tables alsojustifies the utilisation of the mixture model suggested in thisdocument in terms of time performance.V. CONCLUSIONSIn this paper, an anomalous object detection system poweredfor microcontrollers using PTZ cameras is proposed. Thissystem is based on a Convolutional Neural Network (CNN)for detecting and characterizing objects present in the scene.Moreover, a mathematical model has been proposed to launcha fixed number of windows in the frame, which are used to fed# Windows 1 2 3 4 5 6 7 8 9 10Mixture (fps) 0.607903 0.531067 0.38625 0.314169 0.267237 0.224115 0.197433 0.173732 0.161525 0.141844Uniform (fps) 0.747384 0.519751 0.304633 0.317662 0.261643 0.20008 0.196541 0.172058 0.162285 0.1462TABLE IISYSTEM PERFORMANCE EXPRESSED IN MEAN FRAMES PER SECOND VERSUS NUMBER OF POTENTIAL DETECTION GENERATIONS FOR MIXTURE MODELAND UNIFORM MODEL.0 2 4 6 8 10Number of potential detections00.20.40.60.8Performance (fps)MixtureRandomFig. 6. System performance in fps for both random window generation modelsversus number of potential detection numberthe CNN and to track anomalous objects. On the other hand,images supplied by a PTZ camera can be high computingpower consuming. However, our proposal is a low energyand low cost system since it has been implemented in araspberry Pi microcontroller. Experimental results confirm thegood performance of our low-cost proposal.ACKNOWLEDGMENTThis work is partially supported by the Ministry of Economyand Competitiveness of Spain under grant TIN2014-53465-R,project name Video surveillance by active search of anomalousevents. It is also partially supported by the AutonomousGovernment of Andalusia (Spain) under projects TIC-6213,project name Development of Self-Organizing Neural Net-works for Information Technologies; and TIC-657, projectname Self-organizing systems and robust estimators for videosurveillance. All of them include funds from the EuropeanRegional Development Fund (ERDF). The authors thankfullyacknowledge the computer resources, technical expertise andassistance provided by the SCBI (Supercomputing and Bioin-formatics) center of the University of Ma\\u00b4laga. They alsogratefully acknowledge the support of NVIDIA Corporationwith the donation of two Titan X GPUs used for this research.REFERENCES[1] C. Chen, S. Li, H. Qin, and A. Hao, \\u201cRobust salient motion detection innon-stationary videos via novel integrated strategies of spatio-temporalcoherency clues and low-rank analysis,\\u201d Pattern Recognition, vol. 52,pp. 410 \\u2013 432, 2016.[2] H. Sajid, S.-C. S. Cheung, and N. Jacobs, \\u201cAppearance based back-ground subtraction for PTZ cameras,\\u201d Signal Processing: Image Com-munication, vol. 47, pp. 417 \\u2013 425, 2016.[3] J. Huo, Y. Gao, W. Yang, and H. Yin, \\u201cMulti-instance dictionary learningfor detecting abnormal events in surveillance videos,\\u201d InternationalJournal of Neural Systems, vol. 24, no. 03, p. 1430010, 2014.[4] R. Mesquita and C. Mello, \\u201cObject recognition using saliency guidedsearching,\\u201d Integrated Computer-Aided Engineering, vol. 23, no. 4, pp.385\\u2013400, 2016.[5] B. Lacabex, A. Cuesta-Infante, A. S. Montemayor, and J. J. Pantrigo,\\u201cLightweight tracking-by-detection system for multiple pedestrian tar-gets,\\u201d Integrated Computer-Aided Engineering, vol. 23, no. 3, pp. 299\\u2013311, 2016.[6] T. Boult, X. Gao, R. Micheals, and M. Eckmann, \\u201cOmni-directionalvisual surveillance,\\u201d Image and Vision Computing, vol. 22, no. 7, pp.515\\u2013534, 2004.[7] K.-T. Song and J.-C. Tai, \\u201cDynamic calibration of pan-tilt-zoom cam-eras for traffic monitoring,\\u201d IEEE Transactions on Systems, Man, andCybernetics, Part B: Cybernetics, vol. 36, no. 5, pp. 1091\\u20131103, 2006.[8] C. Micheloni, B. Rinner, and G. Foresti, \\u201cVideo analysis in pan-tilt-zoom camera networks,\\u201d IEEE Signal Processing Magazine, vol. 27,no. 5, pp. 78\\u201390, 2010.[9] C. Ding, B. Song, A. Morye, J. Farrell, and A. Roy-Chowdhury,\\u201cCollaborative sensing in a distributed PTZ camera network,\\u201d IEEETransactions on Image Processing, vol. 21, no. 7, pp. 3282\\u20133295, 2012.[10] C. Ding, J. H. Bappy, J. A. Farrell, and A. K. Roy-Chowdhury,\\u201cOpportunistic image acquisition of individual and group activities in adistributed camera network,\\u201d IEEE Transactions on Circuits and Systemsfor Video Technology, vol. 27, no. 3, pp. 664\\u2013672, March 2017.[11] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi, \\u201cAsurvey of deep neural network architectures and their applications,\\u201dNeurocomputing, vol. 234, no. November 2016, pp. 11\\u201326, 2017.[12] M. T. McCann, K. H. Jin, and M. Unser, \\u201cConvolutional neural networksfor inverse problems in imaging: A review,\\u201d IEEE Signal ProcessingMagazine, vol. 34, no. 6, pp. 85\\u201395, Nov 2017.[13] L. Tong, F. Dai, D. Zhang, D. Wang, and Y. Zhang, \\u201cEncoder combinedvideo moving object detection,\\u201d Neurocomput., vol. 139, pp. 150\\u2013162,Sep. 2014.[14] P. Angelov, P. Sadeghi-Tehran, and C. Clarke, \\u201cAURORA: Autonomousreal-time on-board video analytics,\\u201d Neural Comput. Appl., vol. 28,no. 5, pp. 855\\u2013865, May 2017.[15] A. Dziri, M. Duranton, and R. Chapuis, \\u201cReal-time multiple objectstracking on raspberry-pi-based smart embedded camera,\\u201d Journal ofElectronic Imaging, vol. 25, p. 041005, 04 2016.[16] M. K. Dobrzynski, R. Pericet-Camara, and D. Floreano, \\u201cVision tape-a flexible compound vision sensor for motion detection and proximityestimation,\\u201d IEEE Sensors Journal, vol. 12, no. 5, pp. 1131\\u20131139, May2012.[17] V. Fung, J. L. Bosch, S. W. Roberts, and J. Kleissl, \\u201cCloud shadowspeed sensor,\\u201d Atmospheric Measurement Techniques, vol. 7, no. 6, pp.1693\\u20131700, 2014.[18] L. Adnan, Y. Yussoff, H. Johar, and S. Baki, \\u201cEnergy-saving streetlighting system based on the waspmote mote,\\u201d Jurnal Teknologi, vol. 76,no. 4, pp. 55\\u201358, 2015.[19] F. Ortega-Zamorano, M. A. Molina-Cabello, E. Lo\\u00b4pez-Rubio, and E. J.Palomo, \\u201cSmart motion detection sensor based on video processing usingself-organizing maps,\\u201d Expert Systems with Applications, vol. 64, pp.476 \\u2013 489, 2016.[20] J. Benito-Picazo, E. Lo\\u00b4pez-Rubio, J. M. Ortiz-De-lazcano lobato,E. Dom\\u0131\\u00b4nguez, and E. J. Palomo, Motion detection by microcontrollerfor panning cameras, ser. Lecture Notes in Computer Science (includingsubseries Lecture Notes in Artificial Intelligence and Lecture Notesin Bioinformatics), 2017, vol. 10338 LNCS. [Online]. Available:www.scopus.com[21] G. Chen, P. St-Charles, W. Bouachir, G. Bilodeau, and R. Bergevin,\\u201cReproducible evaluation of pan-tilt-zoom tracking,\\u201d in Proceedings -International Conference on Image Processing (ICIP), 2015, pp. 2055\\u20132059.\",\"id\":7646372,\"identifiers\":[{\"identifier\":\"214840920\",\"type\":\"CORE_ID\"},{\"identifier\":\"oai:riuma.uma.es:10630\\/16324\",\"type\":\"OAI_ID\"},{\"identifier\":\"10.1109\\/ijcnn.2018.8489437\",\"type\":\"DOI\"},{\"identifier\":\"323333340\",\"type\":\"CORE_ID\"},{\"identifier\":\"468593434\",\"type\":\"CORE_ID\"}],\"title\":\"Deep learning-based anomalous object detection system powered by microcontroller for PTZ cameras\",\"language\":{\"code\":\"en\",\"name\":\"English\"},\"magId\":null,\"oaiIds\":[\"oai:riuma.uma.es:10630\\/16324\"],\"publishedDate\":\"2018-01-01T00:00:00\",\"publisher\":\"\\u0027Institute of Electrical and Electronics Engineers (IEEE)\\u0027\",\"pubmedId\":null,\"references\":[],\"sourceFulltextUrls\":[\"https:\\/\\/riuma.uma.es\\/xmlui\\/bitstream\\/10630\\/16324\\/3\\/deep-learning-based.pdf\"],\"updatedDate\":\"2022-03-20T00:45:27\",\"yearPublished\":2018,\"journals\":[],\"links\":[{\"type\":\"download\",\"url\":\"https:\\/\\/core.ac.uk\\/download\\/214840920.pdf\"},{\"type\":\"reader\",\"url\":\"https:\\/\\/core.ac.uk\\/reader\\/214840920\"},{\"type\":\"thumbnail_m\",\"url\":\"https:\\/\\/core.ac.uk\\/image\\/214840920\\/large\"},{\"type\":\"thumbnail_l\",\"url\":\"https:\\/\\/core.ac.uk\\/image\\/214840920\\/large\"},{\"type\":\"display\",\"url\":\"https:\\/\\/core.ac.uk\\/works\\/7646372\"}]},{\"acceptedDate\":\"\",\"arxivId\":null,\"authors\":[{\"name\":\"Han, Jungong\"},{\"name\":\"Ji, Zhong\"},{\"name\":\"Li, Xi\"},{\"name\":\"Pang, Yanwei\"},{\"name\":\"Zhao, Yuxiao\"}],\"citationCount\":0,\"contributors\":[],\"outputs\":[\"https:\\/\\/api.core.ac.uk\\/v3\\/outputs\\/323058183\"],\"createdDate\":\"2020-05-22T06:02:41\",\"dataProviders\":[{\"id\":136,\"name\":\"\",\"url\":\"https:\\/\\/api.core.ac.uk\\/v3\\/data-providers\\/136\",\"logo\":\"https:\\/\\/api.core.ac.uk\\/data-providers\\/136\\/logo\"}],\"depositedDate\":\"2020-04-26T00:00:00\",\"abstract\":\"This article studies supervised video summarization by formulating it into a sequence-to-sequence learning framework, in which the input and output are sequences of original video frames and their predicted importance scores, respectively. Two critical issues are addressed in this article: short-term contextual attention insufficiency and distribution inconsistency. The former lies in the insufficiency of capturing the short-term contextual attention information within the video sequence itself since the existing approaches focus a lot on the long-term encoder-decoder attention. The latter refers to the distributions of predicted importance score sequence and the ground-truth sequence is inconsistent, which may lead to a suboptimal solution. To better mitigate the first issue, we incorporate a self-attention mechanism in the encoder to highlight the important keyframes in a short-term context. The proposed approach alongside the encoder-decoder attention constitutes our deep attentive models for video summarization. For the second one, we propose a distribution consistency learning method by employing a simple yet effective regularization loss term, which seeks a consistent distribution for the two sequences. Our final approach is dubbed as Attentive and Distribution consistent video Summarization (ADSum). Extensive experiments on benchmark data sets demonstrate the superiority of the proposed ADSum approach against state-of-the-art approaches\",\"documentType\":\"research\",\"doi\":\"10.1109\\/tnnls.2020.2991083\",\"downloadUrl\":\"https:\\/\\/core.ac.uk\\/download\\/323058183.pdf\",\"fieldOfStudy\":null,\"fullText\":\"     warwick.ac.uk\\/lib-publications      Manuscript version: Author\\u2019s Accepted Manuscript The version presented in WRAP is the author\\u2019s accepted manuscript and may differ from the published version or Version of Record.  Persistent WRAP URL: http:\\/\\/wrap.warwick.ac.uk\\/136934                                 How to cite: Please refer to published version for the most recent bibliographic citation information.  If a published version is known of, the repository item page linked to above, will contain details on accessing it.  Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions.   Copyright \\u00a9 and all moral rights to the version of the paper presented here belong to the individual author(s) and\\/or other copyright owners. To the extent reasonable and practicable the material made available in WRAP has been checked for eligibility before being made available.  Copies of full items can be used for personal research or study, educational, or not-for-profit purposes without prior permission or charge. Provided that the authors, title and full bibliographic details are credited, a hyperlink and\\/or URL is given for the original metadata page and the content is not changed in any way.  Publisher\\u2019s statement: Please refer to the repository item page, publisher\\u2019s statement section, for further information.  For more information, please contact the WRAP Team at: wrap@warwick.ac.uk.  This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1Deep Attentive Video Summarization WithDistribution Consistency LearningZhong Ji , Member, IEEE, Yuxiao Zhao, Yanwei Pang , Senior Member, IEEE, Xi Li , and Jungong HanAbstract\\u2014 This article studies supervised video summarizationby formulating it into a sequence-to-sequence learning frame-work, in which the input and output are sequences of originalvideo frames and their predicted importance scores, respectively.Two critical issues are addressed in this article: short-termcontextual attention insufficiency and distribution inconsistency.The former lies in the insufficiency of capturing the short-term contextual attention information within the video sequenceitself since the existing approaches focus a lot on the long-termencoder\\u2013decoder attention. The latter refers to the distributionsof predicted importance score sequence and the ground-truthsequence is inconsistent, which may lead to a suboptimal solution.To better mitigate the first issue, we incorporate a self-attentionmechanism in the encoder to highlight the important keyframesin a short-term context. The proposed approach alongside theencoder\\u2013decoder attention constitutes our deep attentive modelsfor video summarization. For the second one, we propose adistribution consistency learning method by employing a simpleyet effective regularization loss term, which seeks a consistentdistribution for the two sequences. Our final approach is dubbedas Attentive and Distribution consistent video Summarization(ADSum). Extensive experiments on benchmark data sets demon-strate the superiority of the proposed ADSum approach againststate-of-the-art approaches.Index Terms\\u2014 Distribution consistency, self-attention,sequence-to-sequence (Seq2Seq) learning, video summarization.I. INTRODUCTIONBY CONDENSING a video into a concise yet comprehen-sive summary, video summarization provides an efficientand effective video browsing and thus increases understandingof video contents. It can be widely used in applications ofonline video management, interactive browsing and searching,and intelligent video surveillance [1]\\u2013[5]. Due to its greatsignificance, video summarization has been a crucially urgenttask, especially in the era of big video data.Manuscript received May 11, 2019; revised October 26, 2019 andFebruary 4, 2020; accepted April 25, 2020. This work was supported by theNational Natural Science Foundation of China under Grant 61771329, Grant61472273, and Grant 61632018. (Corresponding author: Zhong Ji.)Zhong Ji, Yuxiao Zhao, and Yanwei Pang are with the School of Electricaland Information Engineering, Tianjin University, Tianjin 300072, China(e-mail: jizhong@tju.edu.cn; 2117234097@tju.edu.cn; pyw@tju.edu.cn).Xi Li is with the College of Computer Science and Technology, ZhejiangUniversity, Hangzhou 310027, China, and also with the Alibaba-ZhejiangUniversity Joint Institute of Frontier Technologies, Hangzhou 310027, China(e-mail: xilizju@zju.edu.cn).Jungong Han is with the Data Science Group, University of Warwick,Coventry CV4 7AL, U.K. (e-mail: jungonghan77@gmail.com).Color versions of one or more of the figures in this article are availableonline at http:\\/\\/ieeexplore.ieee.org.Digital Object Identifier 10.1109\\/TNNLS.2020.2991083In recent years, some important signs of progress havebeen made in the research of supervised learning-based videosummarization. To explicitly learn the summarization capa-bility from human, it seeks supervised learning methods byemploying videos and their corresponding human-created sum-mary ground truths as training data. Many supervised learningmethods, such as supervised subset selection [6], SVM [7],multiple objective optimization [1], and sequence-to-sequence(Seq2Seq) learning [2], [3], [8], have been developed.Among them, Seq2Seq learning-based approach pioneers apromising direction [2], [3], [8]\\u2013[10]. It formulates the videosummarization as a structure prediction problem on sequentialdata, where the input is the original video frame sequenceand the output is the predicted importance score sequenceof video frames. Based on these predicted importance scores,the keyframe\\/keyshot can be determined. This line of approachadvocates the use of long-short term memory (LSTM) [11] asthe encoder to map the visual sequence to a fixed dimensionalvector. LSTM stems from recurrent neural network (RNN)architecture and is particularly good at modeling the variable-range temporal dependences among video frames. It is acrucial property for generating a meaningful summary sinceone of the challenges for video summarization is to effectivelymodel the long-term temporal contextual information. Mean-while, multilayer perceptron (MLP) and LSTM are usuallyselected as decoders to decode the vector into an importancescore sequence.One of the pioneering studies can be traced back to vsLSTMapproach [2], where a bidirectional LSTM (BiLSTM) isexploited to encode the variable range dependence in a video,and an MLP is employed to combine the hidden states ofLSTM layers and the visual features to indicate the likelihoodsof whether the frames should be chosen in the summary. AVS[3], SASUMsup [8] and SUM-GANsup [9] follow this structureby replacing MLP with LSTM in the encoder. Specifically,as our baseline method, AVS [3] further exploits an encoder\\u2013decoder attention mechanism in this framework to assigndifferent importance weights to different input frames forstrengthening their differences. By doing so, the long-termcontextual attention is strengthened.While the results are encouraging, the short-term contextualattention on the input frames is neglected. Similar to thelong-term contextual attention, we emphasize that the short-term contextual attention should also be highlighted. Thisis because the frames in a video clip contribute unevenlyto the encoded vector, especially in the case of motion.2162-237X \\u00a9 2020 IEEE. Personal use is permitted, but republication\\/redistribution requires IEEE permission.See https:\\/\\/www.ieee.org\\/publications\\/rights\\/index.html for more information.Authorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMSFig. 1. Consecutive frames in a clip, from which we observe that the visualcontents change dramatically.Fig. 1 shows some consecutive frames in a clip, where wesample 2 frames every second from the video. Due to theobject motion, the visual representations vary greatly forsimilar content, which makes them act different roles in theshort-term context. Therefore, the attention mechanism thatassigns different weights to different frames is essential tomake them more discriminative. To this end, we propose anencoder self-attention mechanism to alleviate this short-termcontextual attention insufficiency issue. To the best of ourknowledge, there has been little previous work employing self-attention in video summarization. Combining the proposedself-attention for short-term contextual information and thedecoder attention for long-term contextual information [3]together, we constitute a deep attentive model in our proposedvideo summarization approach.In addition, both vsLSTM [2] and AVS [3] train theirnetworks with mean square error (mse) loss function, which isweak in reflecting the distribution relation. It is easily observedthat the importance score distributions between the predictedone and the ground truth vary greatly even they have a smallmse loss, which leads to a suboptimal solution. Fig. 2 listsfive possible predicted importance score distributions with thesame ground truth and mse value. It can be observed thatthese predicted sequences differ greatly from the ground truthexcept for Fig. 2(a) and (b). This demonstrates that mse, as avalue-based loss function, is not a good choice for Seq2Seqlearning-based video summarization since it cannot guaranteeto reflect the distribution of the ground truth. To alleviate thisdistribution inconsistency issue, we introduce a distribution-based function as a complement for mse. In this way, both theminimal distance and distribution consistency are satisfied forthe predicted importance score sequence against the ground-truth one.The contributions of this article are summarized as follows.1) Two critical issues in video summarization are discov-ered, i.e., short-term contextual attention insufficiencyand distribution inconsistency. The former refers to thatcurrent approaches are deficient in capturing the short-term contextual attention information within the video,which is an important knowledge to be modeled in videosummarization. The latter is that the distributions ofpredicted importance score sequence and the ground-truth sequence are inconsistent, which may lead to asuboptimal solution.2) It proposes an encoder self-attention mechanism forSeq2Seq learning-based video summarization. It assignsweights to the encoder outputs according to their impor-tance to the short-term context, which is complementaryto the encoder\\u2013decoder attention mechanism. By doingso, both the short- and long-term contextual attentionsare satisfied, which is a prime requirement for effectivevideo summarization.Fig. 2. Distributions of different predictions, where (a)\\u2013(e) have thesame ground-truth sequences but different predicted importance scoresequences. Although they have different distributions, they have the same mse(mse = 1.0). (a) Possible distribution 1. (b) Possible distribution 2. (c) Possibledistribution 3. (d) Possible distribution 4. (e) Possible distribution 5.3) It presents a distribution-based loss function to overcomethe limitation of employing mse individually. On thepremise of minimal distance between the sequences ofpredicted importance score and ground truth, it learns thedistribution consistency from them, which guarantees abetter usage of the human annotations.We evaluate our approach via extensive experiments onthe benchmark data sets of SumMe [12] and TVSum [13],on which the results outperform the state-of-the-art approachesby a large margin. In addition, we also conduct an ablationstudy to prove the contribution of our encoder self-attentionand distribution consistency loss function.The rest of this article is organized as follows. Section IIreviews the related video summarization methods. Section IIIintroduces the proposed Attentive and Distribution consistentvideo Summarization (ADSum) approach. Section IV presentsthe experimental results and analyses. Finally, conclusions andfuture work are provided in Section V.II. RELATED WORKVideo summarization, also called video abstraction or videoskim, has been an active research topic for more than twodecades [3], [14], [15]. Similar works also include videokeyframe selection [16], video highlight detection [17], andvideo story segmentation [18]. The goals for these lines ofstudies are similar, i.e., to provide a succinct yet informa-tive video subsets (keyframes or keyshots) to facilitate thebrowsing and understanding of individual videos. It shouldbe noted that this article is concerned with single videosummarization but not multiple video summarization [19],Authorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.JI et al.: DEEP ATTENTIVE VIDEO SUMMARIZATION WITH DISTRIBUTION CONSISTENCY LEARNING 3[20], whose purpose is to condense a set of query-relatedvideos into a compact summary. In the following, we firstbriefly review the conventional approaches and then introducethe state-of-the-art Seq2Seq learning-based approaches.A. Conventional ApproachesConventional approaches characterize themselves withhandcrafted features and shallow structures. Many of themrely on the unsupervised approaches, such as clustering andsparse coding. Specifically, the clustering-based approachesgenerally select cluster centers as summary subsets. By doingso, the redundant content can be removed and the videois shortened. Accordingly, many efforts are devoted byemploying and designing various clustering methods, rangingfrom k-means [21], Delaunay clustering [22], graph clustering[23], and prototype selection [4], to archetypal analysis[13]. The sparse coding-based approaches formulate videosummarization as a minimum sparse reconstruction problem[24], [25] since the sparsity and reconstruction error terms init naturally accord with the problem of summarization. Forexample, Cong et al. [24] exploited a sparse coding model toleverage the dictionary as keyframes as they could reconstructthe original video. Instead of using the L2,1 norm in [24],Mei et al. [25] utilized the L0 norm in their proposed method.For efficiency consideration, Zhao and Xing [26] exploiteda quasi-real-time approach to summarize videos. In addition,motion state change detection is also employed in videosummarization. For example, Zhang et al. [27] proposed toemploy the spatiotemporal slices to analyze the object motiontrajectories and select motion state changes as a metric tosummarize videos. Specifically, the motion state changes areformulated as a collinear segment on a spatiotemporal sliceproblem, by which an attention curve is formed to generatethe summary.Besides unsupervised approaches, there has been a dramaticincrease in designing supervised learning-based approachesover the past few years. Prior work [1], [7], [28] includesoptimizing one or multiple objective functions for videosummarization. For example, SVM is employed to classifyeach segment with importance score in [7], and those seg-ments with higher scores are selected to constitute a videosummary. Li et al. [1] and Gygli et al. [28], respectively,learned a combination function of them with a maximummargin formulation to ensure that the generated summaries areclose to the human-created summaries and designed severalhandcrafted criteria. By resorting to the human annotations,this line of work usually has a better performance than theunsupervised one.B. Seq2Seq Learning-Based ApproachesRecent years have seen a surge in Seq2Seq learning-basedapproaches for video summarization with the renaissance ofdeep learning, especially the LSTM technique [2], [3], [29].A typical framework for this line of work is to take the originalvideo frame sequence as input and the frame importance scoresequence as output and exploits LSTM to capture the long-term contextual information. For example, Zhang et al. [2]proposed to utilize BiLSTM as encoder and MLP as decoder intheir vsLSTM approach and further introduced determinantalpoint process (DPP) to vsLSTM to enhance the diversity.To further strengthen the long temporal dependences amongvideo frames, Zhao et al. [30] developed a hierarchicalarchitecture of LSTMs by employing an LSTM layer and aBiLSTM layer as encoder and decoder, respectively.A critical issue of the abovementioned methods is that theyconsidered the whole input frames as equally important, thatis to say, all the frames in the input video sequence are treatedwith the same importance no matter what kind of outputframes are to be predicted, which weakens the discriminationof the representative frames. To alleviate this issue, AVS [3]introduces an encoder\\u2013decoder attention mechanism in theSeq2Seq framework by conditioning the generative processin the decoder on the encoder hidden states. By doing so,different input frames are assigned different weights, whichcan provide the inherent relations between the input videosequence and the output keyframes. SASUM [8] proposes tostrengthen the semantic attention by resorting to additionaltext descriptions. It first employs a Seq2Seq model to embedthe input visual information into text representation and thenpresents a frame selector to exploit the embedded text descrip-tion to find video keyframes that are relevant to the high-levelcontext.Besides the abovementioned supervised approaches,the Seq2Seq model can also be leveraged in an unsupervisedmanner [9], [10]. For example, SUM-GAN [9] proposesto formulate video summarization in the framework ofgenerative adversarial network (GAN), where the LSTM isapplied as generator and discriminator. Motivated by thesuccess of reinforcement learning, DR-DSN [10] formulatesvideo summarization as a sequential decision-making process,where a reward function judges the qualification degree of thegenerated summaries, and a Seq2Seq model is encouraged toearn higher rewards by learning to produce more qualifiedsummaries.Our proposed ADSum formulates video summarization asa supervised Seq2Seq problem. Different from the existingapproaches, we exploit additional encoder self-attention tostrengthen the short-term contextual attention among inputvideo frames and an effective distribution learning lossfunction.III. PROPOSED APPROACHThis section introduces our proposed ADSum videosummarization approach in detail. As shown in Fig. 3,ADSum formulates video summarization as a Seq2Seqtask, in which the input and output are sequences of videoframes and their predicted importance scores. Specifically,the video is first downsampled into frame sequence, andGoogleNet is employed to extract the visual features. Next,BiLSTM is selected as an encoder, and the proposed self-attention is applied on it. Then, LSTM is used as a decoder tooutput the predicted importance scores for each frame, whereeither additive or multiplicative attention is exploited on it.Finally, the ground truth is employed as supervision against theoutput importance scores, where both regression loss and theproposed distribution loss are used as the final objective func-tion. These steps constitute the training stage. After the modelAuthorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMSFig. 3. Illustration of the proposed ADSum approach. The dotted line indicates an alternative step.training is completed, a predicted importance score sequencewill be output when a test video is an input. Then, the sum-mary for the test video is generated according to the outputimportance score sequence. It can mainly be divided intofive components, which are encoder, encoder self-attention,decoder with attention mechanism, loss functions, andsummary generation. We introduce these components in detail.A. EncoderWe first downsample the videos into frame sequences in 2frames\\/s. For fair comparison [2], [3], [10], [31], we chooseto use the output of pool5 layer of the GoogLeNet [32](1024 dimensionality), trained on ImageNet [33], as the visualfeature for each video frame. Afterward, the features of videoframe sequence X = {x1, x2, . . . , xT } are fed into a BiLSTMnetwork [34], which is selected as the encoder. BiLSTM isusually employed as the encoder in recent studies of videosummarization [3], [8], [35] due to its capability of capturingbidirectional long-term structural dependences among frames.It splits the neurons of a regular LSTM [24] into two direc-tions: one for positive time direction (forward states) and theother for negative time direction (backward states), whichare called forward LSTM and backward LSTM. The majordifference between forward LSTM and backward LSTM isthat the former encodes information from the beginning to theend and the latter encodes information from the end to thebeginning. BiLSTM can better capture bidirectional semanticdependences since its output is the concatenation of forwardhidden states and backward hidden states, which is shownin Fig. 4.In forward LSTM, xt\\u22121, xt , and xt+1 are its inputs, andthe corresponding outputs are h ft\\u22121, hft , and hft+1. Meanwhile,the inputs of backward LSTM are xt+1, xt , and xt\\u22121, and theoutputs are hbt+1, hbt , and hbt\\u22121. The final outputs of BiLSTM hare the concatenation of forward hidden states and backwardhidden states at the same time, i.e., ht\\u22121 = [h ft\\u22121, hbt\\u22121], ht =[h ft , hbt ], and ht+1 = [h ft+1, hbt+1].B. Encoder Self-AttentionAlthough BiLSTM is good at capturing the contextual infor-mation among video frames, its limitation lies in treating eachFig. 4. Flowchart of BiLSTM.input with equal importance, that is to say, it is insufficient incapturing the attention information within the video sequenceitself. To alleviate this issue, we propose to exploit both short-and long-term contextual dependences with a deep attentivemechanism. In this section, we develop to strengthen the short-term contextual dependences with a self-attention mechanism,as shown in Fig. 5.First, the encoder outputs H = {h1, h2, . . . , hT } is con-volved in one dimension with a convolution kernel of thesame size as h, in which the slide step is 1, and it slidesT times in total, which generates T weights. The output H ofencoder is fed into a convolutional layer to adjust the encodingrepresentations and obtain original weights belonging to eachframe. Next, original weights are mapped into a new space bythe sigmoid function to restrict their values between 0 and 1.The formulation is shown asS(H) = 11 + exp[conv(H)] (1)where conv() denotes a convolutional operation,exp() represents the exponential function, S(H) ={s1, s2, . . . , sm , . . . , sT } means mapping output vectorsof original weights, and the attention scale T is the lengthof S(H), which is set to 9 in this article. Then, the mappingvalues sm are fed into the softmax function to obtain theAuthorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.JI et al.: DEEP ATTENTIVE VIDEO SUMMARIZATION WITH DISTRIBUTION CONSISTENCY LEARNING 5Fig. 5. Flowchart of the proposed encoder self-attention mechanism.attention weights, which is formulated as\\u03b1m = exp(sm)\\u2211m exp(sm)(2)where \\u03b1m represents the weight of the mth frame vec-tor, and it constructs interframe weight vector \\u03b1 ={\\u03b11, \\u03b12, . . . , \\u03b1m, . . . , \\u03b1T }. In this way, the encoder self-attention generates and delivers attention weights to eachframe. Then, we multiply each weight to its correspondingframe vector to strengthen the differences among them in thesame sequence. The attentive encoding representation pm isformulated aspm = \\u03b1mhm+hm . (3)Since the attention is guided by the encoder outputs H itselfwithin an attention scale of T , it is a short-term contextualself-attention.C. Decoder With Attention MechanismThe decoder is to generate the output importance scoresequence Y = {y\\u20321, y\\u20322, . . . , y\\u2032n} conditioned on the sequencefeatures P = {p1, p2, . . . , pm, . . . , pT } got from encoder.According to the type of output, different decoders can bechosen. For example, if the output is an image or imagesegments, the CNN is usually chosen as the decoder. When theoutput is a language or importance score sequence, the LSTMis a better selection. Thus, we employ LSTM as the decoderin our approach. In addition, as discussed in Section II,attention mechanism is capable of modeling of long-termcontextual dependences across the entire video, which hasbeen an integrant component in several Seq2Seq-based videosummarization approaches [3], [8]. To this end, we employthe attention-based decoder method in [3] in our approach.Specifically, we can write an LSTM as f (un\\u22121, y\\u2032n\\u22121, pm),where f () represents LSTM and un is hidden state at timen. Since pm is a fixed length encoding vector and cannotaccurately reflect the temporal ordering across a long-termvideo sequence, we apply the attention mechanism to pm , thenwe have qn = \\u2211Tm=1 \\u03b2nmpm , where qn represents the attentivevector of pm , and \\u03b2nm = (exp(znm))\\/(\\u2211Tm=1 exp(znm)) is themth decoder input attention weight at time n whose sum is 1.In detail, znm = \\u03c6(un\\u22121, pm) is the similarity score between thehidden states at time n \\u2212 1 and the mth decoder input, where\\u03c6 represents the attention approach. In this way, the decoderis allowed to selectively focus on only a subset of inputs byincreasing their attention weights. The attentive decoder is thenformulated as[P(y\\u2032n|{y\\u20321, . . . , y\\u2032n\\u22121}, qn), un] = f (un\\u22121, y\\u2032n\\u22121, qn). (4)Then, the key is to choose the attention approach \\u03c6. Accordingto the relations between the input and the hidden states, similarto [3], we employ either the additive attention approach [36][see (5)] or multiplicative attention approach [37] [see (6)] toformulize \\u03c6. In particular, the additive attention approach iswritten asznm = (W3)T tanh(W1un\\u22121 + W2pm + b) (5)and the multiplicative attention approach is formulated asznm = (pm)T W4un\\u22121 (6)where W1, W2, W3, and W4 denote the fully connected layermatrix and b is the bias. In the abovementioned decodingprocess, the additive and multiplicative attention models can,respectively, act on the hidden layers of the decoder to obtaindifferent attention vectors of the context. We could observethat the additive attention approach concatenates the videoframes and the hidden states of the decoder, whereas themultiplicative attention approach multiplies them to modelthe relationship. Specifically, the additive attention modeladopts a feedforward neural network with a hidden layerto allocate attention. In contrast, the multiplicative attentionmodel employs the matrix operation to assign attention weight,which is more efficient. Accordingly, we can obtain attentiveframe-level importance score at the output of decoder. Sincethe attention is guided by the decoder hidden state un\\u22121 thatcontains previous information, it is a long-term contextualattention.D. Loss FunctionThe design of loss function plays a critical role in sup-porting a high-quality video summarization. For example,Jung et al. [38] developed a variance loss to ensure the modelto predict output scores for each keyframe with high discrep-ancy, which is simply defined as a reciprocal of variance ofthe predicted scores. To preserve the semantic information in avideo summarization to the original video, Zhang et al. [29]proposed a retrospective loss by embedding both the sum-marization and original video into a shared space and min-imizing their distances. Wei et al. [8] applied two variantsof sparsity losses to force the model to generate satisfyingsummarizations. Among these loss functions, the regressionloss, especially mse, is the most popular loss employed invideo summarization [2], [3], [10], [29], and it minimizes thediscriminative losses by measuring elementwise discrepancy.We also apply mse as our loss function, which is thesum of squared distances between the predicted importancescores sequences Y\\u2032t and ground-truth sequences Yt in the tthbatch. It is formulated asLmse = 1n\\u2211(Y\\u2032t \\u2212 Yt)2 (7)Authorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMSwhere n is the number of input samples in a batch. However,we could observe that there may exist a lot of possiblesequence forms of the predicted importance scores even forthe same mse value ( \\u0003= 0), that is to say, the distributionsbetween the predicted importance scores and the ground truthmay vary greatly even they have a small regression loss,as shown in Fig. 2. To better mitigate this distribution incon-sistency issue, we propose a distribution consistency learningstrategy as a complement for mse loss by employing a simpleyet effective Kullback\\u2013Leibler (KL) regularization loss term.It aims at seeking a consistent distribution for the predictedimportance scores with the ground truth by regularizing theirKL divergence. Specifically, we feed both the predicted impor-tance scores and ground truth into the softmax function toembed them in a shared space, which is formulated asSYt = softmax(Yt) (8)SY\\u2032t = softmax(Y\\u2032t) (9)where SYt and SY\\u2032t represent the normalized ground-truthsequences and normalized predicted importance scoresequences in the tth batch, respectively. In video summa-rization, high importance score means the correspondingframe or shot could be chosen as keyframe or keyshot. Sincethe keyframe or keyshot is used to represent its relevantframes\\/shots, we could approximatively view the normalizedimportance score of each frame as its probability distribution.To this end, we could describe the KL loss function asLkl = 1n\\u2211(SYt \\u00b7 log(SYt) \\u2212 SYt \\u00b7 log(SY\\u2032t)). (10)With the KL loss function, we restrict the distributionconsistency between the predicted importance scores and theground truth, which guarantees a better prediction. Accord-ingly, the final loss function is made up of two parts: mse lossand KL loss, described as follows:Lmk = Lmse + \\u03bbLkl (11)where \\u03bb is a balance parameter. The mse loss measures thedistance between the predicted importance scores and theground truth, whereas the KL loss guarantees their distributionconsistency.E. Summary GenerationWe follow [2] and [3] to generate the video summarizationin forms of keyshots based on the frame-level importancescores. In particular, due to the lack of ground-truth tempo-ral segmentation in video summarization data sets, we firstemploy the kernel temporal segmentation (KTS) [9] algo-rithm to split a video into a set of nonintersecting temporalshots. Then, the predicted importance score Y\\u2032t is dividedinto shot-level importance scores according to different shots,which are calculated by averaging the frame importancescores within each shot. For the purpose of ensuring that thetotal summarization length is set to a predefined length \\u0006,where we follow [12] to set \\u0006 to be less than 15% of thelength of the original video, we need to solve the followingoptimization problem:maxn\\u2211i=1\\u00b5i\\u03b3i , s.t.n\\u2211i=1\\u00b5i\\u03b7i \\u2264 \\u0006, \\u00b5i \\u2208 {0, 1} (12)where n is the number of shots after video segmentation, and\\u03b3i and \\u03b7i are the shot-level importance score and the lengthof the i th shot, respectively. Notice that \\u00b5i \\u2208 {0, 1}, where\\u00b5i = 1 indicates that the i th shot is selected as a keyshot.We hope to find the shots of the highest sum of shot-levelimportance score and ensure the specific video summarylength at the same time, which belongs to the 0\\/1 knapsackproblem. After following [13] to solve it with dynamicprogramming, the keyshots conforming to the summary areobtained. Finally, the summary is created by concatenatingthose keyshots in a chronological order.IV. EXPERIMENTSA. Experimental Setup1) Data Sets: We evaluate the proposed ADSum methodon three publicly available benchmark data sets of videosummarization: SumMe [12], TVSum [13], and YouTube[21]. Specifically, the SumMe data set contains 25 user videosthat record a variety of events such as holidays, sports, andhistory. The videos range from 1.5 to 6.5 min in length. TheTVSum data set is a collection of 50 videos from YouTube,which is organized into ten categories, such as grooming ananimal, parade, attempting a bike trick, and so on. YouTubehas 39 videos, including cartoons, news, and sports. Thelength of these videos ranges typically from 1 to 10 min.It is worth noticing that there are two types of ground-truthannotations for both TVSum and SumMe data sets: indicatorvector (0 or 1) and frame-level importance score vectors.Most approaches [2], [3], [6], [10] employ the frame-levelimportance scores as the ground truth, and thus, we followthis setting. Since YouTube only provides selected keyframesas ground truths, we set them as our evaluation targetdirectly.2) Evaluation Metric: We apply the popular F-measure toevaluate the performance of automated generated summarycompared with the ground-truth summary [1]\\u2013[3], [9], [12],[13], [28]. Similar to [2] and [3], our ADSum approachgenerates a summary S that is less than 15% in duration of theoriginal. Given a generated summary S and the ground-truthsummary G, the precision P and the recall R for each pair of Sand G are calculated as a measure with the temporal overlapsbetween them as follows:P = overlaped duration of S and Gduration of S\\u00d7 100% (13)R = overlaped duration of S and Gduration of G\\u00d7 100%. (14)Finally, the F-measure is computed asF = 2 \\u00d7 P \\u00d7 RP + R \\u00d7 100%. (15)Authorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.JI et al.: DEEP ATTENTIVE VIDEO SUMMARIZATION WITH DISTRIBUTION CONSISTENCY LEARNING 73) Implementation Details: We implement experiments onthe Tensorflow platform and all experiments are conductedon an NVIDIA Tesla K40c GPU. We employ BiLSTM with256 hidden units in an encoder and one-layer LSTM with256 hidden units in a decoder, and we optimize the networkwith the gradient descent algorithm and set the learning rateto 0.15. In addition, the batch size is 16 and attention scalesare 9. The balance parameter \\u03bb in (11) is set to 1. As in theprior work [2], [3], [8], we report the average of F-scores ofall testing videos. As for the training\\/testing data, we applythe same standard supervised learning setting as [2] and [3]where the training and testing are from the disjoint part of thesame data set. We employ 20% for testing and the remaining80% for training and validation, where the proportion of thetraining set and validation set is also 4:1.B. Comparison With State-of-the-Art ApproachesSince SumMe and TVSum are most popular data sets,we first perform experiments on them. Then, we provide theexperimental result on the YouTube data set.To demonstrate the superiority of the proposed ADSumapproach, 12 state-of-the-art supervised approaches areselected for comparison on SumMe and TVSum data sets, andall of them employ the image CNN as visual features and theexperiments are performed in the same settings with ADSum.We retrieve their results from published articles.Specifically, we are interested in comparing ADSumwith those approaches within the Seq2Seq learning-basedframework, i.e., vsLSTM [2], dppLSTM [2], SUM-GANsup[9], DR-DSNsup [10], SASUMsup [8], A-AVS [3], and M-AVS[3]. Concretely, as an early encoder\\u2013decoder-based approach,vsLSTM [2] formulates video summarization as a struc-ture prediction problem on sequential data by employingLSTMs for sequence modeling. As a variant for vsLSTM,dppLSTM [2] boosts the diversity by combining LSTM andDPP. By introducing the idea of GAN into the encoder\\u2013decoder framework, SUM-GANsup [9] makes itself a super-vised method by further adding a sparse regularization withthe ground-truth summarization labels. Similarly, DR-DSNsup[10] incorporates the idea of reinforcement learning in theframework, where the reward function accounts for diversityand representativeness. The remaining three approaches arecloser to our ADSum since they also exploit the attentionmechanism. SASUMsup [8] presents a semantic attention net-work by leveraging additional text descriptions as the semanticguidance. The video summarization is implemented by mini-mizing the distance between the generated text description ofthe summarized video and the ground-truth text descriptionof the original video and the importance scores betweenthe generated keyframes and the ground truth. AVS [3] isa baseline for our ADSum. It explores the encoder\\u2013decoderattention mechanism to assign importance weights to differentframes and takes the mse as its loss function. The additiveand multiplicative attention mechanisms are leveraged, whichcorresponds to the approaches of A-AVS and M-AVS.We also choose five additional supervised approaches with-out using Seq2Seq learning-based framework for comparison.TABLE IF-SCORE (%) PERFORMANCE COMPARISON WITH STATE OF THE ARTS ONTHE SUMME AND TVSUM DATA SETS. THE FIRST SECTION AND THESECOND SECTION SHOW RESULTS WITHOUT AND WITH USINGSEQ2SEQ FRAMEWORK. AVERAGE DENOTES THE AVERAGEPERFORMANCE ON BOTH DATA SETSIn particular, the approach of Gygli et al. [28] formulated videosummarization as a subset selection problem by learning sub-modular mixtures of objectives for different criteria directly.Zhang et al. [39] proposed to learn to transfer summary struc-tures from training videos to test ones. Li et al. [1] developeda general framework for both edited and raw videos withthe idea of property-weight learning. Considering the influ-ence of video structure on summarization results, HSA-RNN[40] integrates shot segmentation and video summarizationinto a hierarchical structure-adaptive RNN to jointly exploitthe video structure and content. Also, DySeqDPP [31] is adynamic sequential DPP approach, which aims at enforcingthe local diversity in a reinforcement learning manner.Table I summarizes the comparison results of F-score onthe SumMe and TVSum data sets. We can observe that theproposed ADSum achieves the best performance on both datasets. Specifically, on the SumMe data set, ADSum-A andADSum-M outperform the runner-up approach, SASUMsup,in 0.6% and 0.8%, respectively,and on the TVSum data set,they outperform the runner-up approach, M-AVS, in 3.5%and 3.3%, respectively. Considering the average performanceon both data sets, ADSum-A and ADSum-M have inter-estingly the same performance of 55.2%. It is higher thanthat of the runner-up approach of M-AVS in 2.5%, whichis quite a large margin due to the challenge of the datasets. Besides, we could observe that the top five approacheson average metric all employ attention model, which provesthe effectiveness of encoding contextual attentive information.Furthermore, the superiority of the proposed ADSum againstthe M-AVS, A-AVS, and SASUMsup mainly lies on the factof exploiting the self-attention and the distribution consistencyloss. Finally, we could observe that all approaches performbetter on TVSum than SumMe, which is mainly due to theirsupervised property, that is to say, since the correlations amongvideos in TVSum are closer than those in SumMe, it is easierto obtain more useful supervision knowledge for supervisedapproaches from the training of TVSum data set.Then, we conduct experiments on the YouTube data set,as shown in Table II. Five state-of-the-art approaches areAuthorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMSTABLE IIF-SCORE (%) PERFORMANCE COMPARISON WITH STATE OF THE ARTS ONTHE YOUTUBE DATA SETchosen for comparison, whose results are retrieved from [35].Note that all approaches employ the GoogleNet as the visualfeatures. We could observe that our proposed approachesachieve the best performance. Specifically, ADSum-A andADSum-M outperform the corresponding baseline A-AVSand M-AVS approaches in 4.4% and 4.3%, respectively.In addition, they outperform the second-best approaches,Fu et al. [35], in 0.5% and 0.8%, respectively. The experi-mental results on YouTube further prove the effectiveness ofthe proposed approaches.C. Ablation StudiesTo further reflect the impacts of encoder self-attention andthe KL loss function, we take the AVS approach as the baselineand conduct the ablation experiments by employing encoderself-attention (AVS+SA) and KL loss function (AVS+KL).Note that the proposed ADSum approach could be consideredas AVS+SA+KL. As shown in Table III, the utilization ofself-attention and KL loss function contributes to the per-formance improvements for both additive and multiplicativeattention versions. Specifically, it can be clearly observed thatthe performance gains of A-AVS+SA against A-AVS are 4.7%and 0.2%, and the gains of M-AVS+SA against M-AVS are3% and 0.4%, both on TVSum and SumMe. These resultsprove that the short-term contextual attention is quite helpfulfor video summarization. In addition, there are 4.5% and0.5% gains for A-AVS+KL against A-AVS, and 2.7% and0.1% for M-AVS+KL against M-AVS, both on TVSum andSumMe. These results prove that the distribution consistency isreally useful for video summarization. Interestingly, it is moreobvious for the TVSum data set due to the closer associationsamong the videos. The results prove the effectiveness of boththe proposed components. Moreover, we could find that bothADSum-A and ADSum-M further improve the performance,which demonstrates that both components complement eachother. These ablation studies validate our motivation that bothencoder self-attention and distribution consistency are helpfulto Seq2Seq learning-based video summarization.D. Experiments on Combined DataSet and Augmented Data SetIt is seen from the abovementioned experiments that ourproposed encoder self-attention mechanism and the KL lossfunction contribute greatly to the improvement of the perfor-mance. In the interest of verifying the generalizability of ourmodel, we first make further efforts to experiments on theTABLE IIIABLATION EXPERIMENTS IN F-SCORE (%)Fig. 6. Parameter sensitivity analysis for attention scales on the Combineddata set.TABLE IVEXPERIMENTAL RESULTS ON THE COMBINED DATA SETcombination of both SumMe and TVSum data sets, which isdubbed Combined data set in this article. A similar setting canbe found in many existing summarization approaches [2], [40].Similarly, we also employ 20% for testing and the remaining80% for training and validation.There are several observations from Table IV. First, boththe proposed ADSum-A and ADSum-M achieve better per-formance than A-AVS and M-AVS. Second, ADSum-A andADSum-M have a similar performance, which is consistentwith the results of AVS and those shown in Table I. It showsthat both multiplicative attention and additive attention arecompetent for capturing the attentive knowledge in our deepattention framework. Finally, the performances on the Com-bined data set are a little inferior to those on average in Table Ifor both ADSum-A and ADSum-M. This lies in the fact thatthere is a distinct data difference in SumMe and TVSum,which cannot provide enough supervised information to eachother after merging.Then, following [2], [3], and [9], we conduct augmentationexperiments on the SumMe and TVSum data sets. Besidesthe YouTube data set, the OVP data set [41] is also employedas the augmented data set, which has 50 videos from variousgenres (e.g., documentary and educational) and their lengthsvary from 1 to 4 min. Under this setting, given the data set ofSumMe or TVSum, we randomly select 20% of it for testingand apply the remaining 80% with the other three data setsto form the augmented training data set. For example, whentesting the performance on the SumMe data set, its 20% videosAuthorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.JI et al.: DEEP ATTENTIVE VIDEO SUMMARIZATION WITH DISTRIBUTION CONSISTENCY LEARNING 9Fig. 7. Exemplar video summaries (orange intervals) from a sample video (Uncut_Evening_Flight of SumMe) along with the ground-truth importancescores (blue background). The corresponding keyframes are ordered in numerical order. (a) A-AVS, F-score (%) = 44.8. (b) M-AVS, F-score (%) = 47.9.(c) ADSum-A, F-score (%) = 50.0. (d) ADSum-M, F-score (%) = 56.2.are taken for test data, and the remaining 80% videos togetherwith the TVSum, YouTube, and OVP are used as training dataset. In this way, the training data sets are augmented. Sincemore training data are employed, better performance shouldbe obtained by comparing the experimental results shownin Table I, which is referred to as a canonical setting.Table V shows the results on augmented setting. Thereare four state-of-the-art approaches are chosen for compar-ison, which utilizes the same GoogleNet visual features.We could observe that the performances on this setting haveconsistent improvements against the canonical setting. Forexample, on the SumMe data set, there are 1.4% and 1.5%improvements for augmented setting against canonical settingfor ADSum-A and ADSum-M, respectively. There are also1.3% and 1.4% improvements on the TVSum data set fortwo proposed approaches, respectively. This confirms ourconjecture that the augmented training data are helpful toimprove the generalization of supervised learning approaches.In addition, we could observe that the proposed ADSum-Moutperforms the second-best M-AVS approach in 1.5% on theSumMe data set and 3.9% on the TVSum data set, whichproves the superiority of our proposed approaches.E. Parameter Sensitivity AnalysisIn this section, we evaluate the impact of attention scaleT on the Combined data set. As shown in Fig. 6, whenthe attention scale value is within the range from 3 to17, the F-scores of ADSum-A and ADSum-M fluctuate withinthe range of 1.05% and 1.22%. In addition, it is obviousAuthorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMSTABLE VEXPERIMENTAL RESULTS ON THE AUGMENTED SETTINGthat the values of ADSum-M under different attention scalesare always higher than those of ADSum-A, which indicatesthe fact that the multiplicative attention model is better attaking advantage of the decoder hidden layer output and theoptimized visual features to obtain attentive information. It canbe observed that the performance is the best when the attentionscale value is 9.F. Qualitative ResultsTo get some intuition about qualitative effects on the tem-poral selection pattern, we visualize some selected keyframeson an example video with a duration of 5.22 min in Fig. 7.It shows the results from A-AVS, M-AVS, ADSum-A, andADSum-M models on the video \\u201cUncut_Evening_Flight\\u201d ofSumMe. The video shows a story that some people controla remote-controlled aircraft to shoot an aerial video with acamera attached to the left wing. The blue blocks represent theground-truth frame-level importance scores, and the markedorange regions are the selected subsets. As shown in Fig. 7,we can observe that the summaries generated by our methodshave a more consistent distribution with the ground truththan those generated by AVS models. Besides, the keyframeschosen by our ADSum-A and ADSum-M approaches havelarger importance scores than the others.V. CONCLUSION AND DISCUSSIONIn this article, we have proposed a novel deep attentivevideo summarization approach, called ADSum. It effectivelyaddressed the short-term contextual attention insufficiency anddistribution inconsistency issues, which have been neglectedbefore. It considers both the long- and short-term contextualattention with encoder\\u2013decoder attention and encoder self-attention in a supervised Seq2Seq framework. In addition,it develops a simple yet effective KL loss for learning the dis-tribution consistency between the predicted importance scoresequences and ground-truth sequences. Extensive experimentsclearly demonstrate the superiority of ADSum.One limitation of the ADSum approach is that it is defi-cient in modeling very long-term contextual attention. Thisis mainly due to that LSTM is not effective enough incoping with sequence structure longer than 80 time steps [42].Although we downsample the video into 2 frames\\/s, 80 timesteps are only 40 s. However, one topic in a video may lastseveral minutes. Therefore, it is an important research directionin modeling very long-term important research direction whilepaying attention to short-term contextual attention.Another limitation is that it requires large training data. As asupervised learning approach, only sufficient can guarantee asatisfying performance. However, current data sets are stillrelatively small. One promising way to address this issue is toexploit the idea of transfer learning to transfer the knowledgeof other related data sets to video summarization. Of course,a large, well-annotated, publicly available data set is alsonecessary for promoting the progress of video summarizationdomain.In our future work, we will explore some other lossfunctions to better mimic the way of summarizing videosof human, such as maximum mean discrepancy (MMD) andWasserstein distance.REFERENCES[1] X. Li, B. Zhao, and X. Lu, \\u201cA general framework for edited video andraw video summarization,\\u201d IEEE Trans. Image Process., vol. 26, no. 8,pp. 3652\\u20133664, Aug. 2017.[2] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman, \\u201cVideo summarizationwith long short-term memory,\\u201d in Proc. Eur. Conf. Comput. Vis.,Amsterdam, The Netherlands, Oct. 2016, pp. 766\\u2013782.[3] Z. Ji, K. Xiong, Y. Pang, and X. Li, \\u201cVideo summarization withattention-based encoder-decoder networks,\\u201d IEEE Trans. Circuits Syst.Video Technol., early access, Mar. 14, 2019, doi: 10.1109\\/TCSVT.2019.2904996.[4] X. Zhang, Z. Zhu, Y. Zhao, D. Chang, and J. Liu, \\u201cSeeing all from afew: \\u00061-norm-induced discriminative prototype selection,\\u201d IEEE Trans.Neural Netw. Learn. Syst., vol. 30, no. 7, pp. 1954\\u20131966, Jul. 2019.[5] L. Wu, Y. Wang, L. Shao, and M. Wang, \\u201c3-D PersonVLAD: Learningdeep global representations for video-based person reidentification,\\u201dIEEE Trans. Neural Netw. Learn. Syst., vol. 30, no. 11, pp. 3347\\u20133359,Nov. 2019.[6] B. Gong, W.-L. Chao, K. Grauman, and F. Sha, \\u201cDiverse sequential sub-set selection for supervised video summarization,\\u201d in Proc. Adv. NeuralInf. Process. Syst., Montreal, QC, Canada, Dec. 2014, pp. 2069\\u20132077.[7] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid, \\u201cCategory-specific video summarization,\\u201d in Proc. Eur. Conf. Comput. Vis., Zurich,Switzerland, Sep. 2014, pp. 540\\u2013555.[8] H. Wei, B. Ni, Y. Yan, H. Yu, X. Yang, and C. Yao, \\u201cVideo summariza-tion via semantic attended networks,\\u201d in Proc. AAAI Conf. Art. Intell.,New Orleans, LA, USA, Feb. 2018, pp. 216\\u2013223.[9] B. Mahasseni, M. Lam, and S. Todorovic, \\u201cUnsupervised video sum-marization with adversarial LSTM networks,\\u201d in Proc. IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Honolulu, HI, USA, Jul. 2017,pp. 2982\\u20132991.[10] K. Zhou, Y. Qiao, and T. Xiang, \\u201cDeep reinforcement learning for unsu-pervised video summarization with diversity-representativeness reward,\\u201din Proc. AAAI Conf. Artif. Intell., New Orleans, LA, USA, Feb. 2018,pp. 7582\\u20137589.[11] S. Hochreiter and J. Schmidhuber, \\u201cLong short-term memory,\\u201d NeuralComput., vol. 9, no. 8, pp. 1735\\u20131780, 1997.[12] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool, \\u201cCreatingsummaries from user videos,\\u201d in Proc. Eur. Conf. Comput. Vis., Zurich,Switzerland, Sep. 2014, pp. 505\\u2013520.[13] Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes, \\u201cTVSum: Summarizingweb videos using titles,\\u201d in Proc. IEEE Conf. Comput. Vis. PatternRecognit. (CVPR), Boston, MA, USA, Jun. 2015, pp. 5179\\u20135187.[14] C. Toklu, S.-P. Liou, and M. Das, \\u201cVideoabstract: A hybrid approach togenerate semantically meaningful video summaries,\\u201d in Proc. IEEE Int.Conf. Multimedia Expo, New York, NY, USA, Jul. 2000, pp. 1333\\u20131336.[15] Y. Li, T. Zhang, and D. Tretter, \\u201cAn overview of video abstractiontechniques,\\u201d HP Lab., Palo Alto, CA, USA, Tech. Rep. HPL-2001-191,2001.[16] W. Wolf, \\u201cKey frame selection by motion analysis,\\u201d in Proc. IEEE Int.Conf. Acoust., Speech, Signal Process., Atlanta, GA, USA, May 1996,pp. 1228\\u20131231.[17] T. Yao, T. Mei, and Y. Rui, \\u201cHighlight detection with pairwise deepranking for first-person video summarization,\\u201d in Proc. IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Las Vegas, NV, USA, Jun. 2016,pp. 982\\u2013990.Authorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.JI et al.: DEEP ATTENTIVE VIDEO SUMMARIZATION WITH DISTRIBUTION CONSISTENCY LEARNING 11[18] X. Gao and X. Tang, \\u201cUnsupervised video-shot segmentation and model-free anchorperson detection for news video story parsing,\\u201d IEEE Trans.Circuits Syst. Video Technol., vol. 12, no. 9, pp. 765\\u2013776, Sep. 2002.[19] Z. Ji, Y. Ma, Y. Pang, and X. Li, \\u201cQuery-aware sparse coding for webmulti-video summarization,\\u201d Inf. Sci., vol. 478, pp. 152\\u2013166, Apr. 2019.[20] Z. Ji, Y. Zhang, Y. Pang, and X. Li, \\u201cHypergraph dominant set basedmulti-video summarization,\\u201d Signal Process., vol. 148, pp. 114\\u2013123,Jul. 2018.[21] S. E. F. de Avila, A. P. B. Lopes, A. da Luz, Jr., and A. de AlbuquerqueAra\\u00fajo, \\u201cVSUMM: A mechanism designed to produce static videosummaries and a novel evaluation method,\\u201d Pattern Recognit. Lett.,vol. 22, no. 1, pp. 56\\u201368, 2011.[22] P. Mundur, Y. Rao, and Y. Yesha, \\u201cKeyframe-based video summarizationusing delaunay clustering,\\u201d Int. J. Digit. Libraries, vol. 6, no. 2,pp. 219\\u2013232, Apr. 2006.[23] C.-W. Ngo, Y.-F. Ma, and H.-J. Zhang, \\u201cAutomatic video summarizationby graph modeling,\\u201d in Proc. 9th IEEE Int. Conf. Comput. Vis., Nice,France, Oct. 2003, pp. 104\\u2013109.[24] Y. Cong, J. Yuan, and J. Luo, \\u201cTowards scalable summarization of con-sumer videos via sparse dictionary selection,\\u201d IEEE Trans. Multimedia,vol. 14, no. 1, pp. 66\\u201375, Feb. 2012.[25] S. Mei, G. Guan, Z. Wang, S. Wan, M. He, and D. D. Feng, \\u201cVideosummarization via minimum sparse reconstruction,\\u201d Pattern Recognit.,vol. 48, no. 2, pp. 522\\u2013533, Feb. 2015.[26] B. Zhao and E. P. Xing, \\u201cQuasi real-time summarization for consumervideos,\\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Columbus,OH, USA, Jun. 2014, pp. 2513\\u20132520.[27] Y. Zhang, R. Tao, and Y. Wang, \\u201cMotion-state-adaptive video summa-rization via spatiotemporal analysis,\\u201d IEEE Trans. Circuits Syst. VideoTechnol., vol. 27, no. 6, pp. 1340\\u20131352, Jun. 2017.[28] M. Gygli, H. Grabner, and L. Van Gool, \\u201cVideo summarization bylearning submodular mixtures of objectives,\\u201d in Proc. IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Boston, MA, USA, Jun. 2015,pp. 3090\\u20133098.[29] K. Zhang, K. Grauman, and F. Sha, \\u201cRetrospective encoders for videosummarization,\\u201d in Proc. Eur. Conf. Comput. Vis., Munich, Germany,Sep. 2018, pp. 383\\u2013399.[30] B. Zhao, X. Li, and X. Lu, \\u201cHierarchical recurrent neural networkfor video summarization,\\u201d in Proc. ACM Multimedia Conf. (MM),Mountain View, CA, USA, Oct. 2017, pp. 863\\u2013871.[31] Y. Li, L. Wang, T. Yang, and B. Gong, \\u201cHow local is the localdiversity? Reinforcing sequential determinantal point processes withdynamic ground sets for supervised video summarization,\\u201d in Proc. Eur.Conf. Comput. Vis., Munich, Germany, Sep. 2018, pp. 151\\u2013167.[32] C. Szegedy et al., \\u201cGoing deeper with convolutions,\\u201d in Proc. IEEEConf. Comput. Vis. Pattern Recognit. (CVPR), Boston, MA, USA,Jun. 2015, pp. 1\\u20139.[33] O. Russakovsky et al., \\u201cImageNet large scale visual recognition chal-lenge,\\u201d Int. J. Comput. Vis., vol. 115, no. 3, pp. 211\\u2013252, Dec. 2015.[34] M. Schuster and K. K. Paliwal, \\u201cBidirectional recurrent neural net-works,\\u201d IEEE Trans. Signal Process., vol. 45, no. 11, pp. 2673\\u20132681,Nov. 1997.[35] T.-J. Fu, S.-H. Tai, and H.-T. Chen, \\u201cAttentive and adversarial learningfor video summarization,\\u201d in Proc. IEEE Winter Conf. Appl. Comput.Vis. (WACV), Waikoloa Village, HI, USA, Jan. 2019, pp. 1579\\u20131587.[36] D. Bahdanau, K. Cho, and Y. Bengio, \\u201cNeural machine translationby jointly learning to align and translate,\\u201d in Proc. Int. Conf. Learn.Represent., San Diego, CA, USA, May 2015, pp. 1\\u201315.[37] T. Luong, H. Pham, and C. D. Manning, \\u201cEffective approaches toattention-based neural machine translation,\\u201d in Proc. Conf. Empiri-cal Methods Natural Lang. Process., Lisbon, Portugal, Sep. 2015,pp. 1412\\u20131421.[38] Y. Jung, D. Cho, D. Kim, S. Woo, and I. S. Kweon, \\u201cDiscriminativefeature learning for unsupervised video summarization,\\u201d in Proc. AAAIConf. Artif. Intell., Honolulu, HI, USA, Jan. 2019, pp. 8537\\u20138544.[39] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman, \\u201cSummary transfer:Exemplar-based subset selection for video summarization,\\u201d in Proc.IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Las Vegas, NV,USA, Jun. 2016, pp. 1059\\u20131067.[40] B. Zhao, X. Li, and X. Lu, \\u201cHSA-RNN: Hierarchical structure-adaptiveRNN for video summarization,\\u201d in Proc. IEEE\\/CVF Conf. Comput. Vis.Pattern Recognit., Salt Lake City, UT, USA, Jun. 2018, pp. 7405\\u20137414.[41] Open Video Project. Accessed: May 6, 2017. [Online]. Available:http:\\/\\/www.open-video.org\\/[42] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, andK. Saenko, \\u201cSequence to sequence\\u2014Video to text,\\u201d in Proc. IEEE Int.Conf. Comput. Vis., Santiago, Chile, Dec. 2015, pp. 4534\\u20134542.Zhong Ji (Member, IEEE) received the Ph.D. degreein signal and information processing from TianjinUniversity, Tianjin, China, in 2008.He is currently an Associate Professor with theSchool of Electrical and Information Engineering,Tianjin University. His current research interestsinclude machine learning, computer vision, multi-media understanding, and video summarization.Yuxiao Zhao received the B.S. degree in com-munication engineering from Weifang University,Weifang, Shandong, China, in 2017. She is cur-rently pursuing the master\\u2019s degree with the Schoolof Electrical and Information Engineering, TianjinUniversity, Tianjin, China.Her research interests include computer vision andvideo summarization.Yanwei Pang (Senior Member, IEEE) received thePh.D. degree in electronic engineering from theUniversity of Science and Technology of China,Hefei, China, in 2004.He is currently a Professor with the School ofElectronic Information Engineering, Tianjin Univer-sity, Tianjin, China. He has authored over 80 scien-tific articles. His current research interests includeobject detection and recognition, vision in badweather, and computer vision.Xi Li received the Ph.D. degree from the NationalLaboratory of Pattern Recognition, Chinese Acad-emy of Sciences, Beijing, China, in 2009.He was a Senior Researcher with The Univer-sity of Adelaide, Adelaide, SA, Australia. From2009 to 2010, he was a Post-Doctoral Researcherwith CNRS\\u2013T\\u00e9\\u00e9 com ParisTech, Paris, France.He is currently a Full Professor with ZhejiangUniversity, Zhejiang, China. His current researchinterests include visual tracking, motion analysis,face recognition, Web data mining, and image andvideo retrieval.Jungong Han received the Ph.D. degree in telecom-munication and information system from XidianUniversity, Xi\\u2019an, Shaanxi, China, in 2004.He was a Senior Lecturer with the School of Com-puting and Communications, Lancaster University,Lancaster, U.K., and the Department of ComputerScience, Northumbria University, Newcastle uponTyne, U.K. He is currently a tenured AssociateProfessor of data science with the University ofWarwick, Warwick, U.K.Authorized licensed use limited to: University of Warwick. Downloaded on May 21,2020 at 09:16:40 UTC from IEEE Xplore.  Restrictions apply. \",\"id\":76709927,\"identifiers\":[{\"identifier\":\"oai:wrap.warwick.ac.uk:136934\",\"type\":\"OAI_ID\"},{\"identifier\":\"323058183\",\"type\":\"CORE_ID\"},{\"identifier\":\"10.1109\\/tnnls.2020.2991083\",\"type\":\"DOI\"}],\"title\":\"Deep attentive video summarization with distribution consistency learning\",\"language\":{\"code\":\"en\",\"name\":\"English\"},\"magId\":null,\"oaiIds\":[\"oai:wrap.warwick.ac.uk:136934\"],\"publishedDate\":\"2021-04-01T01:00:00\",\"publisher\":\"\\u0027Institute of Electrical and Electronics Engineers (IEEE)\\u0027\",\"pubmedId\":null,\"references\":[],\"sourceFulltextUrls\":[\"http:\\/\\/wrap.warwick.ac.uk\\/136934\\/1\\/WRAP-deep-attentive-video-consistency-learning-Han-2020.pdf\"],\"updatedDate\":\"2022-06-23T07:16:15\",\"yearPublished\":2021,\"journals\":[],\"links\":[{\"type\":\"download\",\"url\":\"https:\\/\\/core.ac.uk\\/download\\/323058183.pdf\"},{\"type\":\"reader\",\"url\":\"https:\\/\\/core.ac.uk\\/reader\\/323058183\"},{\"type\":\"thumbnail_m\",\"url\":\"https:\\/\\/core.ac.uk\\/image\\/323058183\\/large\"},{\"type\":\"thumbnail_l\",\"url\":\"https:\\/\\/core.ac.uk\\/image\\/323058183\\/large\"},{\"type\":\"display\",\"url\":\"https:\\/\\/core.ac.uk\\/works\\/76709927\"}]},{\"acceptedDate\":\"2017-01-20T00:00:00\",\"arxivId\":\"1605.01369\",\"authors\":[{\"name\":\"Ding, Chris\"},{\"name\":\"Vishnu, Abhinav\"},{\"name\":\"Zheng, Shuai\"}],\"citationCount\":0,\"contributors\":[\"Shuai\"],\"outputs\":[\"https:\\/\\/api.core.ac.uk\\/v3\\/outputs\\/187036708\"],\"createdDate\":\"2016-08-03T02:59:47\",\"dataProviders\":[{\"id\":144,\"name\":\"\",\"url\":\"https:\\/\\/api.core.ac.uk\\/v3\\/data-providers\\/144\",\"logo\":\"https:\\/\\/api.core.ac.uk\\/data-providers\\/144\\/logo\"},{\"id\":4786,\"name\":\"\",\"url\":\"https:\\/\\/api.core.ac.uk\\/v3\\/data-providers\\/4786\",\"logo\":\"https:\\/\\/api.core.ac.uk\\/data-providers\\/4786\\/logo\"}],\"depositedDate\":\"2016-12-01T00:00:00\",\"abstract\":\"Deep Learning is a very powerful machine learning model. Deep Learning trains\\na large number of parameters for multiple layers and is very slow when data is\\nin large scale and the architecture size is large. Inspired from the shrinking\\ntechnique used in accelerating computation of Support Vector Machines (SVM)\\nalgorithm and screening technique used in LASSO, we propose a shrinking Deep\\nLearning with recall (sDLr) approach to speed up deep learning computation. We\\nexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network\\n(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data\\nsets. Results show that the speedup using shrinking Deep Learning with recall\\n(sDLr) can reach more than 2.0 while still giving competitive classification\\nperformance.Comment: The 22nd IEEE International Conference on Parallel and Distributed\\n  Systems (ICPADS 2016\",\"documentType\":\"research\",\"doi\":\"10.1109\\/icpads.2016.0129\",\"downloadUrl\":\"http:\\/\\/arxiv.org\\/abs\\/1605.01369\",\"fieldOfStudy\":null,\"fullText\":\"ar\\nX\\niv\\n:1\\n60\\n5.\\n01\\n36\\n9v\\n2 \\n [c\\ns.L\\nG]\\n  1\\n9 S\\nep\\n 20\\n16\\nAccelerating Deep Learning with Shrinkage and Recall\\nShuai Zheng\\u2217, Abhinav Vishnu\\u2020 and Chris Ding\\u2217\\n\\u2020Pacific Northwest National Laboratory, Richland, WA, USA\\nabhinav.vishnu@pnnl.gov\\n\\u2217Department of Computer Science and Engineering,\\nUniversity of Texas at Arlington, TX, USA\\nzhengs123@gmail.com, chqding@uta.edu\\nAbstract\\u2014Deep Learning is a very powerful machine learn-\\ning model. Deep Learning trains a large number of parameters\\nfor multiple layers and is very slow when data is in large scale\\nand the architecture size is large. Inspired from the shrinking\\ntechnique used in accelerating computation of Support Vector\\nMachines (SVM) algorithm and screening technique used in\\nLASSO, we propose a shrinking Deep Learning with recall\\n(sDLr) approach to speed up deep learning computation. We\\nexperiment shrinking Deep Learning with recall (sDLr) using\\nDeep Neural Network (DNN), Deep Belief Network (DBN) and\\nConvolution Neural Network (CNN) on 4 data sets. Results\\nshow that the speedup using shrinking Deep Learning with\\nrecall (sDLr) can reach more than 2.0 while still giving\\ncompetitive classification performance.\\nKeywords-Deep Learning; Deep Neural Network (DNN);\\nDeep Belief Network (DBN); Convolution Neural Network\\n(CNN)\\nI. INTRODUCTION\\nDeep Learning [1] has become a powerful machine\\nlearning model. It differs from traditional machine learning\\napproaches in the following aspects: Firstly, Deep Learning\\ncontains multiple non-linear hidden layers and can learn\\nvery complicated relationships between inputs and outputs.\\nDeep architectures using multiple layers outperform shadow\\nmodels [2]. Secondly, there is no need to extract human\\ndesign features [3], which can reduce the dependence of the\\nquality of human extracted features. We mainly study three\\nDeep Learning models in this work: Deep Neural Networks\\n(DNN), Deep Belief Network (DBN) and Convolution Neu-\\nral Network (CNN).\\nDeep Neural Network (DNN) is the very basic deep\\nlearning model. It contains multiple layers with many hidden\\nneurons with non-linear activation function in each layer.\\nFigure 1 shows one simple example of Deep Neural Network\\n(DNN) model. This Deep neural network has one input layer,\\ntwo hidden layers and one output layer. Training process of\\nDeep Neural Network (DNN) includes forward propagation\\nand back propagation. Forward propagation uses the current\\nconnection weight to give a prediction based on current state\\nThis work was conducted while the first author was doing internship at\\nPacific Northwest National Laboratory, Richland, WA, USA.\\nof model. Back propagation computes the amount of weight\\nshould be changed based on the difference of ground truth\\nlabel and forward propagation prediction. Back propagation\\nin Deep Neural Network (DNN) is a non-convex problem.\\nDifferent initialization affects classification accuracy and\\nconvergence speed of models.\\nSeveral unsupervised pretraining methods for neural net-\\nwork have been proposed to improve the performance of\\nrandom initialized DNN, such as using stacks of RBMs\\n(Restricted Boltzmann Machines) [1], autoencoders [4],\\nor DBM (Deep Boltzmann Machines) [5]. Compared to\\nrandom initialization, pretraining followed with finetuning\\nbackpropagation will improve the performance significantly.\\nDeep Belief Network (DBN) is a generative unsupervised\\npretraining network which uses stacked RBMs [6] during\\npretraining. A DNN with a corresponding configured DBN\\noften produces much better results. DBN has undirected con-\\nnections between its first two layers and directed connections\\nbetween all its lower layers[7] [5].\\nConvolution Neural Network (CNN) [8] [3] [9] has been\\nproposed to deal with images, speech and time-series. This is\\nbecause standard DNN has some limitations. Firstly, images,\\nspeeches are usually large. A simple Neural Network to\\nprocess an image size of 100 \\u00d7 100 with 1 layer of 100\\nhidden neurons will require 1,000,000 (100 \\u00d7 100 \\u00d7 100)\\nweight parameters. With so many variables, it will lead\\nto overfitting easily. Computation of standard DNN model\\nrequires expensive memory too. Secondly, standard DNN\\ndoes not consider the local structure and topology of the\\ninput. For example, images have strong 2D local structure.\\nMany areas in the image are similar. Speeches have a strong\\n1D structure, where variables temporally nearby are highly\\ncorrelated. CNN forces the extraction of local features by\\nrestricting the receptive fields of hidden neurons to be local\\n[9].\\nHowever, the training process for deep learning algo-\\nrithms, including DNN, DBN, CNN, is computationally ex-\\npensive. This is due to the large number of training data and\\na large number of parameters for multiple layers. Inspired\\nfrom the shrinking technique [10] [11] used in accelerating\\ncomputation of Support Vector Machines (SVM) algorithm\\nInput layer\\nHidden layer\\nOutput layer\\nHidden layer\\nFigure 1: Deep Neural Network (DNN).\\nand screening [12] [13] technique used in LASSO, we\\npropose an accelerating algorithm shrinking Deep Learning\\nwith Recall (sDLr). The main contribution of sDLr is that\\nit can reduce the running time significantly. Though there is\\na trade-off between classification improvement and speedup\\non training time, for some data sets, sDLr approach can\\neven improve classification accuracy. It should be noted\\nthat the approach sDLr is a general model and a new\\nway of thinking, which can be applied to both large data,\\nlarge network and small data small network, both sequential\\nand parallel implementations. We will study the impact of\\nproposed accelerating approaches on DNN, DBN and CNN\\nusing 4 data sets from computer vision and high energy\\nphysics, biology science.\\nII. MOTIVATION\\nThe amount of data in our world has been exploding.\\nAnalyzing large data sets, so-called big data, will become\\na key basis of competition, underpinning new waves of\\nproductivity growth, innovation, and consumer interest [14].\\nA lot of big data technologies, including cloud computing,\\ndimensionality reduction have been proposed [15], [16],\\n[17], [18], [19]. Analyzing big data with machine learning\\nalgorithms requires special hardware implementations and\\nlarge amount of running time.\\nSVM [20] solves the following optimization problem:\\nmin\\nw,\\u03be,b\\n1\\n2\\nwTw + C\\nn\\u2211\\ni=1\\n\\u03bei, (1)\\nsubject to yi(wT\\u03a6(xi)\\u2212 b) \\u003E 1\\u2212 \\u03bei,\\n\\u03bei \\u003E 0, i = 1, ..., n,\\nwhere xi is a training sample, yi is the corresponding label,\\n\\u03bei is positive slack variable, \\u03a6(xi) is mapping function,\\nw gives the solution and is known as weight vector, C\\ncontrols the relative importance of maximizing the margin\\nand minimizing the amount of the slack. Since SVM learn-\\ning problem has much less support vectors than training\\nexamples, shrinking [10] [11] was proposed to eliminate\\ntraining samples for large learning tasks where the fraction\\nof support vectors is small compared to the training sample\\nsize or when many support vectors are at the upper bound\\nof Lagrange multipliers.\\nLASSO [21] is an optimization problem to find sparse\\nrepresentation of some signals with respect to a predefined\\ndictionary. It solves the following problem:\\nmin\\nx\\n1\\n2\\n\\u2016Dx\\u2212 y\\u201622 + \\u03bb\\u2016x\\u20161, (2)\\nwhere y is a testing point, D \\u2208 \\u211cp\\u00d7n is a dictionary with\\ndimension p and size n, \\u03bb is a parameter controls the sparsity\\nof representation x. When both p and n are large, which is\\nusually the case in practical applications, such as denoising\\nor classification, it is difficult and time-intensive to compute.\\nScreening [12] [13] is a technique used to reduce the size\\nof dictionary using some rules in order to accelerate the\\ncomputation of LASSO.\\nEither in shrinking of SVM or in screening of LASSO,\\nthese approaches are trying to reduce the size of computation\\ndata. Inspired from these two techniques, we propose a faster\\nand reliable approach for deep learning, shrinking Deep\\nLearning.\\nIII. SHRINKING DEEP LEARNING\\nGiven testing point xi \\u2208 \\u211cp\\u00d71, i = 1, 2, ..., n, let class\\nindicator vector be y(0)i \\u2208 \\u211c1\\u00d7c, where n is number of\\ntesting samples, c is number of classes, yi has all 0s except\\none 1 to indicate the class of this test point. Let the output\\nof a neural network for testing point xi be yi \\u2208 \\u211c1\\u00d7c. yi\\ncontains continuous values and is the ith row of Y.\\nA. Standard Deep Learning\\nAlgorithm 1 gives the framework of standard deep learn-\\ning. During each epoch (iteration), standard deep learning\\nfirst runs a forward-propagation on all training data, then\\ncomputes the output Y(w), where output Y is a function of\\nweight parameters w. Deep learning tries to find an optimal\\nw to minimize error loss e = [e1, ..., en], which can be\\nsum squared error loss (DNN, DBN in our experiment) or\\nsoftmax loss (CNN in our experiment). In backpropagation\\nprocess, deep learning updates weight parameter vector\\nusing gradient descent. For an training data xi, gradient\\ndescent can be denoted as:\\nw(epoch+1) = w(epoch) \\u2212 \\u03b7\\u2207ei(w\\n(epoch)), (3)\\nwhere \\u03b7 is step size.\\nBefore we present shrinking Deep Leaning algorithm, we\\nfirst give Lemma 1.\\nLemma 1: Magnitude of gradient \\u2207ei(w(epoch)) in\\nEq.(3) is positive correlated with the error ei, i = 1, ..., n.\\nAlgorithm 1 Deep Learning (DL)\\nInput: Data matrix X \\u2208 \\u211cp\\u00d7n, class matrix Y(0) \\u2208 \\u211cn\\u00d7c\\nOutput: Classification error\\n1: Preprocessing training data\\n2: Active training data index A = {1, 2, ..., n}\\n3: for epoch = 1, 2, ... do\\n4: Run forward-propagation on A\\n5: Compute forward-propagation output Y \\u2208 \\u211cn\\u00d7c\\n6: Run back-propagation\\n7: Update weight w using Eq.(3)\\n8: end for\\n9: Compute classification error using Y and Y(0)\\nProof: In the case of sum squared error, error loss of\\nsample xi is given as:\\nei =\\n1\\n2\\nc\\u2211\\nk=1\\n(yik(w)\\u2212 y\\n(0)\\nik )\\n2, i = 1, ..., n. (4)\\nUsing Eq.(4), gradient \\u2207ei(w) is:\\n\\u2207ei(w) =\\nc\\u2211\\nk=1\\n(yik(w)\\u2212 y\\n(0)\\nik )\\u2207yik(w)\\n\\u2207ei(w) = \\u2207yi(w)(yi \\u2212 y\\n(0)\\ni )\\nT . (5)\\nAs we can see from Eq.(5), \\u2207ei(w) is linear related to\\n(yi \\u2212 y\\n(0)\\ni ). Data points with larger error will have larger\\ngradient, thus will have a stronger and larger correction\\nsignal when updating w. Data points with smaller error will\\nhave smaller gradient, thus will have a weaker and smaller\\ncorrection signal when updating w.\\nIn the case of softmax loss function, ei is denoted as:\\nei = \\u2212\\nc\\u2211\\nk=1\\ny\\n(0)\\nik logpik, i = 1, ..., n (6)\\npik =\\nexp(yik(w))\\u2211c\\nj=1 exp(yij(w))\\n. (7)\\nUsing Eq.(6), gradient \\u2207ei(w) is:\\n\\u2207ei(w) =\\nc\\u2211\\nk=1\\n\\u2202ei(yik)\\n\\u2202yik\\n\\u2207yik(w),\\n\\u2207ei(w) =\\nc\\u2211\\nk=1\\n(pik \\u2212 y\\n(0)\\nik )\\u2207yik(w). (8)\\nNow let\\u2019s see the relation between softmax loss function\\n(Eq.(6)) and its gradient with respect to weight parameter\\nw (Eq.(8)). For example, given point i is in class 1, so\\ny\\n(0)\\ni1 = 1 and y\\n(0)\\nij = 0, j 6= 1. When pi1 is large, pi1 \\u2192 1,\\nsoftmax loss function (Eq.(6)) is very small. For gradient\\nof softmax loss function (Eq.(8)), when k = 1, (pi1 \\u2212 1)\\nis close to 0; when k 6= 1, (pi1 \\u2212 0) is also close to 0. In\\nsummary, when softmax loss function (Eq.(6)) is very small,\\nits gradient (Eq.(8)) is also very small.\\nAlgorithm 2 Shrinking Deep Learning (sDL)\\nInput: Data matrix X \\u2208 \\u211cp\\u00d7n, class matrix Y(0) \\u2208 \\u211cn\\u00d7c,\\nelimination rate s (s is a percentage), stop threshold t\\nOutput: Classification error\\n1: Preprocessing training data\\n2: Active training data index A = {1, 2, ..., n}\\n3: for epoch = 1, 2, ... do\\n4: Run forward-propagation on A\\n5: Compute forward-propagation output Y \\u2208 \\u211cn\\u00d7c\\n6: Run back-propagation\\n7: Update weight w using Eq.(3)\\n8: if nepoch \\u003E= t then\\n9: Compute error using Eq.(4)\\n10: Compute set S, which contains indexes of nepochs\\nsmallest ei values (nepoch is size of A in current epoch)\\n11: Eliminate all samples in S and update A, A = A\\u2212 S\\n12: end if\\n13: end for\\n14: Compute classification error using Y and Y(0)\\nAlgorithm 3 Shrinking Deep Learning with Recall (sDLr)\\nInput: Data matrix X \\u2208 \\u211cp\\u00d7n, class matrix Y(0) \\u2208 \\u211cn\\u00d7c,\\nelimination rate s (s is a percentage), stop threshold t\\nOutput: Classification error\\n1: Preprocessing training data\\n2: Active training data index A = {1, 2, ..., n}, A0 = A\\n3: for epoch = 1, 2, ... do\\n4: Run forward-propagation on A\\n5: Compute forward-propagation output Y \\u2208 \\u211cn\\u00d7c\\n6: Run back-propagation\\n7: Update weight w using Eq.(3)\\n8: if nepoch \\u003E= t then\\n9: Compute error using Eq.(4)\\n10: Compute set S, which contains indexes of nepochs\\nsmallest ei values (nepoch is size of A in current epoch)\\n11: Eliminate all samples in S and update A, A = A\\u2212 S\\n12: else\\n13: Use all data for training, A = A0\\n14: end if\\n15: end for\\n16: Compute classification error using Y and Y(0)\\nB. Shrinking Deep Learning\\nIn order to accelerate computation and inspired from\\ntechniques of shrinking in SVM and screening of LASSO,\\nwe propose shrinking Deep Learning in Algorithm 2 by\\neliminating samples with small error (Eq.(4)) from training\\ndata and use less data for training.\\nAlgorithm 2 gives the outline of shrinking Deep Learning\\n(sDL). Compared to standard deep learning in Algorithm\\n1, sDL requires two more inputs, elimination rate s and\\nstop threshold t. s is a percentage indicating the amount\\nof training data to be eliminated during one epoch, t is a\\nnumber indication to stop eliminating training data when\\nnepoch \\u003C t, where nepoch is current number of training\\ndata. We maintain an index vector A. In Algorithm 1, both\\nforward and backward propagation apply on all training\\ndata. In Algorithm 2, the training process is applied on\\na subset of all training data. In the first epoch, we set\\nA = {1, 2, ..., n} to include all training indexes. After\\nforward and backward propagation in each epoch, we select\\nthe nepochs indexes of training data with smallest error ei,\\nwhere nepoch is size of current number of training data A.\\nThen we eliminate indexes in S from A, and update A,\\nA = A\\u2212 S. When nepoch \\u003C t, we stop eliminating training\\ndata anymore. Lemma 1 gives theoretical foundation that\\nsamples with small error will smaller impact on the gradient.\\nThus eliminating those samples will not impact the gradient\\nsignificantly. Figure 2 shows that the errors using sDL is\\nsmaller than errors using DL, which proves that sDL gives\\na stronger correction signal and reduce the errors faster.\\nWhen eliminating samples, elimination rate s denotes the\\npercentage of samples to be removed. We select the nepochs\\nindexes of training data with smallest error ei. For the same\\nepoch, in different batches, the threshold used to eliminate\\nsamples is different. Assume there are nbatch batches one\\nepoch, in every batch, we need to drop nepochs\\/nbatch\\nsamples on average. In batch i, let the threshold to drop\\nnepochs\\/nbatch smallest error be ti; in batch i + 1, let the\\nthreshold be ti+1. ti and ti+1 will differ a lot. We use\\nexponential smoothing [22] to adjust the threshold used in\\nbatch i+1: instead of using ti+1 as the threshold to eliminate\\nsamples, we use the following t\\u2032i+1:\\nt\\u2032i+1 = \\u03b1t\\n\\u2032\\ni + (1\\u2212 \\u03b1)ti+1, (9)\\nwhere \\u03b1 \\u2208 [0, 1) is a weight parameter which controls the\\nimportance of past threshold values, t\\u20321 = t1. The intuition\\nusing exponential smoothing is that we want the threshold\\nused in each epoch to be consistent. Samples with errors\\nless than t\\u2032i+1 in batch i+1 will be eliminated. If \\u03b1 is close\\nto 0, the smoothing effect on threshold is not obvious; if\\n\\u03b1 is close to 1, the threshold t\\u2032i+1 will deviate a lot from\\nti+1. In practical, we find \\u03b1 between 0.5 and 0.6 is a good\\nsetting in terms of smoothing threshold. We will show this\\nin experiment part.\\nIV. SHRINKING WITH RECALL\\nAs the training data in sDL becomes less and less, the\\nweight parameter w trained is based on the subset of training\\ndata. It is not optimized for the entire training dataset.\\nWe now introduce shrinking Deep Learning with recall\\n(Algorithm 3) to deal with this situation. In order to utilize\\nall the training data, when the number of active training\\nsamples nepoch \\u003C t, we start to use all training samples, as\\nshown in Algorithm 3, A = A0. Algorithm 3 ensures that\\nthe model trained is optimized for the entire training data.\\nShrinking with recall of Algorithm 3 will produce competi-\\ntive classification performance with standard Deep Learning\\nof Algorithm 1. In experiment, we will also investigate the\\nimpact the threshold t on the classification results (see Figure\\n7).\\n0 500 1000\\n10\\u221220\\n10\\u221210\\n100\\nSample ID\\nSu\\nm\\n s\\nqu\\nar\\ned\\n e\\nrro\\nr\\n \\n \\nDL\\nsDL\\nFigure 2: Sum squared errors (Eq.(4)) using sDNN (shrink-\\ning DNN) is smaller than errors using standard DNN in the\\nsame epoch on 1000 samples from MNIST data.\\n(a) MNIST (10 classes, size 28 \\u00d7 28, randomly select 50\\nimages).\\n(b) CIFAR-10 (10 classes in total, size 32\\u00d732, each row is\\na class, randomly select 10 images from each class).\\nFigure 3: Sample images.\\nV. EXPERIMENTS\\nIn experiment, we test our algorithms on data sets of\\ndifferent domains using 5 different random initialization.\\nThe data sets we used are listed in Table I. MNIST is\\na standard toy data set of handwritten digits; CIFAR-10\\nTable II: MNIST classification error improvement (IMP) and training time Speedup.\\nMethod DNN sDNNr IMP\\/Speedup DBN sDBNr IMP\\/Speedup CNN sCNNr IMP\\/Speedup\\nTesting error 0.0387 0.0324 16.3% 0.0192 0.0182 5.21% 0.0072 0.0073 \\u22121.39%\\nTraining time (s) 1653 805 2.05 1627 700 2.32 3042 1431 2.13\\n0 50 100\\n0.04\\n0.045\\n0.05\\n0.055\\n0.06\\nIteration\\nCl\\nas\\nsif\\nica\\ntio\\nn \\nte\\nst\\nin\\ng \\ner\\nro\\nr\\n \\n \\n784\\u2212100\\u221210\\n784\\u2212100\\u2212100\\u221210\\n784\\u2212200\\u221210\\n784\\u22121000\\u221210\\n(a) Testing error.\\n0 50 100\\n0\\n0.005\\n0.01\\n0.015\\n0.02\\nIteration\\nCl\\nas\\nsif\\nica\\ntio\\nn \\ntra\\nin\\nin\\ng \\ner\\nro\\nr\\n \\n \\n784\\u2212100\\u221210\\n784\\u2212100\\u2212100\\u221210\\n784\\u2212200\\u221210\\n784\\u22121000\\u221210\\n(b) Training error.\\nFigure 4: MNIST DNN testing and training error on different\\nnetwork (100 iterations\\/epochs).\\nTable I: Overview of data sets used in this paper.\\nDataset Dimensionality Training Set Testing Set\\nMNIST 784 (28 \\u00d7 28 grayscale) 60K 10K\\nCIFAR-10 3072 (32 \\u00d7 32 color) 50K 10K\\nHiggs Boson 7 50K 20K\\nAlternative Splicing 3446 2500 945\\ncontains tiny natural images; Higgs Boson is a dataset\\nfrom high energy physics. Alternative Splicing is RNA\\nfeatures used for predicting alternative gene splicing. We\\nuse DNN and DBN implementation from [23] and CNN\\nimplementation from [24]. All experiments were conducted\\non a laptop with Intel Core i5-3210M CPU 2.50GHz, 4GB\\nRAM, Windows 7 64-bit OS.\\nA. Results on MNIST\\nMNIST is a standard toy data set of handwritten digits\\ncontaining 10 classes. It contains 60K training samples and\\n10K testing samples. The image size is 784 (grayscale 28\\u00d7\\n28). Figure 3a shows some examples of MNIST dataset.\\n1) Deep Neural Network: In experiment, we first test on\\nsome network architecture and find a better one for our\\nfurther investigations. Figure 4a and Figure 4b show the\\n0 50 100\\n0.03\\n0.04\\n0.05\\n0.06\\n0.07\\n0.08\\nIteration\\nCl\\nas\\nsif\\nica\\ntio\\nn \\nte\\nst\\nin\\ng \\ner\\nro\\nr\\n \\n \\nDNN\\nsDNN\\nsDNNr\\n(a) Testing error.\\n0 50 100\\n0\\n0.02\\n0.04\\n0.06\\n0.08\\n0.1\\nIteration\\nCl\\nas\\nsif\\nica\\ntio\\nn \\ntra\\nin\\nin\\ng \\ner\\nro\\nr\\n \\n \\nDNN\\nsDNN\\nsDNNr\\n(b) Training error.\\nFigure 5: MNIST testing and training error (100 itera-\\ntions\\/epochs).\\ntesting and training classification error for different network\\nsettings. Results show that \\u201c784-1000-10\\u201d is a better setting\\nwith lower testing error and converges faster in training.\\nWe will use network \\u201c784-1000-10\\u201d for DNN and DBN on\\nMNIST. Learning rate is set to be 1; activation function is\\ntangent function and output unit is sigmoid function.\\nFigure 5 shows the testing error and training error of\\nusing standard DNN, sDNN (Shrinking DNN) and sDNNr\\n(shrinking DNN with recall). Results show that sDNNr\\nimproves the accuracy of standard DNN. While for training\\nerror, both DNN and sDNNr give almost 0 training error.\\nFigure 6 shows training time and number of active sam-\\nples in each iteration (epoch). In our experiments, for sDNN\\nand sDNNr, we set eliminate rate s = 20%. sDNNr has\\na recall process to use the the entire training samples, as\\nshown in Figure 6. When the number of active samples is\\nless than t = 20%\\u00d7 60K of total training samples, we stop\\neliminating samples. The speedup using sDNNr compared\\nto DNN is\\nSpeedup =\\ntDNN\\ntsDNNr\\n= 2.05. (10)\\n0 20 40 60 80 100\\n0\\n5\\n10\\n15\\n20\\n25\\nIteration\\nR\\nun\\nni\\nng\\n ti\\nm\\ne \\n(se\\nco\\nnd\\ns)\\n \\n \\nDNN\\nsDNN\\nsDNNr\\n(a) Training time.\\n0 20 40 60 80 100\\n0\\n2\\n4\\n6\\n8\\nx 104\\nIteration\\nN\\num\\nbe\\nr o\\nf a\\nct\\nive\\n s\\nam\\npl\\nes\\n \\n \\nDNN\\nsDNN\\nsDNNr\\n(b) Number of active samples.\\nFigure 6: MNIST training time and number of active samples\\n(100 iterations\\/epochs).\\n0 20 40 60 80 100\\n0.035\\n0.04\\n0.045\\n0.05\\n0.055\\nIteration\\nCl\\nas\\nsif\\nica\\ntio\\nn \\nte\\nst\\nin\\ng \\ner\\nro\\nr\\n \\n \\nDNN\\nsDNNr t=0.1\\nsDNNr t=0.2\\nsDNNr t=0.3\\nsDNNr t=0.4\\nsDNNr t=0.5\\nFigure 7: MNIST classification testing error using different\\nrecall threshold t (100 iterations\\/epochs).\\nRecall is a technique when the number of training samples\\nis decreased to a threshold t, we start to use all training\\nsamples. There is a trade-off between speedup and classi-\\nfication error: setting a lower t could reduce computation\\ntime more, but could increase classification error. Figure 7\\nshows the effect of using different recall threshold t sDNNr\\non MNIST data. When we bring all training samples back\\nat t = 20%\\u00d7 60K , we get the best testing error. It is worth\\nnoting that the classification error of sDNNr is improved\\ncompared to standard DNN, which could imply that there is\\nless overfitting for this data set.\\nFigure 8 shows an example of exponential smoothing on\\nthe elimination threshold (Eq.(9)) during one epoch. The\\nthreshold using \\u03b1 = 0.5 smooths the curve a lot.\\n10 20 30 40 50 60\\n1\\n2\\n3\\n4\\n5\\n6\\nx 10\\u22127\\nBatch\\nTh\\nre\\nsh\\nol\\nd\\n \\n \\n\\u03b1 = 0\\n\\u03b1 = 0.5\\nFigure 8: Exponential smoothing (see Eq.(9)) effect on one\\nepoch of MNIST (60 batches with 1000 samples\\/batch ).\\n0 50 100\\n0.02\\n0.04\\n0.06\\n0.08\\nIteration\\nCl\\nas\\nsif\\nica\\ntio\\nn \\nte\\nst\\nin\\ng \\ner\\nro\\nr\\n \\n \\nDNN\\nDBN\\nsDBNr\\n(a) Testing error\\n0 20 40 60 80 100\\n0\\n5\\n10\\n15\\n20\\n25\\nIteration\\nR\\nun\\nni\\nng\\n ti\\nm\\ne \\n(se\\nco\\nnd\\ns)\\n \\n \\nDBN\\nsDBNr\\n(b) Training Time\\nFigure 9: MNIST DBN result(100 iterations\\/epochs): (a)\\ncompares classification testing error; (b) compares training\\ntime.\\n2) Deep Belief Network: Figure 9 shows the classification\\ntesting error and training time of using Deep Belief Network\\n(DBN) and shrinking DBN with recall (sDBNr) on MNIST.\\nNetwork setting is same as it is in DNN experiment. sDBNr\\nfurther reduces the classification error of DBN to 0.0182 by\\nusing sDBNr.\\n3) Convolution Neural Networks (CNN): The network\\narchitecture used in MNIST is 4 convolutional layers with\\neach of the first 2 convolutional layers followed by a max-\\npooling layer, then 1 layer followed by a ReLU layer, 1\\nlayer followed by a Softmax layer. The first 2 convolutional\\nlayers have 5 \\u00d7 5 receptive field applied with a stride of 1\\npixel. The 3rd convolutional layer has 4\\u00d7 4 receptive field\\n0 10 20 30 40 50\\n0.005\\n0.01\\n0.015\\n0.02\\n0.025\\nIteration\\nCl\\nas\\nsif\\nica\\ntio\\nn \\nte\\nst\\nin\\ng \\ner\\nro\\nr\\n \\n \\nCNN\\nsCNN\\nsCNNr\\n(a) Testing error\\n0 10 20 30 40 50\\n0\\n20\\n40\\n60\\n80\\n100\\nIteration\\nR\\nun\\nni\\nng\\n ti\\nm\\ne \\n(se\\nco\\nnd\\ns)\\n \\n CNN\\nsCNN\\nsCNNr\\n(b) Training Time\\nFigure 10: MNIST CNN result(50 iterations\\/epochs): (a)\\ncompares classification testing error; (b) compares training\\ntime.\\nTable III: CIFAR-10 classification error improvement (IMP)\\nand training time Speedup.\\nMethod CNN sCNNr IMP\\/Speedup\\nTesting error (top 1) 0.2070 0.2066 0.19%\\nTraining time (s) 5571 3565 1.56\\nand the 4th layer has 1\\u00d7 1 receptive field with a stride of 1\\npixel. The max pooling layers pool 2\\u00d7 2 regions at strides\\nof 2 pixels. Figure 10 shows the classification testing error\\nand training time of CNN on MNIST data.\\nTable II summarizes the classification error improvement\\n(IMP) and training time speedup of DNN, DBN and CNN\\non MNIST data, where improvement is IMP = (errDL \\u2212\\nerrsDLr)\\/errDL.\\nB. Results on CIFAR-10\\nCIFAR-10 [25] data contains 60,000 32\\u00d7 32 color image\\nin 10 classes, with 6,000 images per class. There are\\n50,000 training and 10,000 testing images. CIFAR-10 is an\\nobject dataset, which includes airplane, car, bird, cat and\\nso on and classes are completely mutually exclusive. In our\\nexperiment, we use CNN network to evaluate the perfor-\\nmance in terms of classification error. Network architecture\\nuses 5 convolutional layers: for the first three layers, each\\nconvolutional layer is followed by a max pooling layer;\\n4th convolutional layer is followed by a ReLU layer; the\\nTable IV: Higgs Boson classification error improvement\\n(IMP) and training time Speedup.\\nDNN Network Method DNN sDNNr IMP\\/Speedup\\n7-20-20-2 Testing error 0.4759 0.4512 5.19%\\nTraining time (s) 21 13 1.62\\n7-50-2 Testing error 0.3485 0.3386 2.84%\\nTraining time (s) 52 18 2.89\\nTable V: Alternative Splicing error improvement (IMP) and\\ntraining time Speedup.\\nDNN Network Method DNN sDNNr IMP\\/Speedup\\n1389 \\u2212 100 \\u2212 3 Testing error 0.2681 0.2960 10.4%\\nTraining time (s) 32 20 1.60\\n5th layer is followed by a softmax loss output layer. Table\\nIII shows the classification error and training time. Top-1\\nclassification testing error in Table III means that the predict\\nlabel is determined by considering the class with maximum\\nprobability only.\\nC. Results on Higgs Boson\\nHiggs Boson is a subset of data from [26] with 50, 000\\ntraining and 20, 000 testing. Each sample is a signal process\\nwhich either produces Higgs bosons particle or not. We use 7\\nhigh-level features derived by physicists to help discriminate\\nparticles between the two classes. Both activation function\\nand output function were sigmoid function. The DNN batch-\\nsize is 100 and recall threshold t = 20%\\u00d7 50, 000. We test\\non different network settings and choose the best. Table IV\\nshows the experiment results using different network.\\nD. Results on Alternative Splicing\\nAlternative Splicing [27] is a set of RNA sequences used\\nin bioinfomatics. It contains 3446 cassette-type mouse exons\\nwith 1389 features per exon. We randomly select 2500 exons\\nfor training and use the rest for testing. For each exon, the\\ndataset contains three real-valued positive prediction targets\\nyi = [q\\ninc qexc qnc], corresponding to probabilities that\\nthe exon is more likely to be included in the given tissue,\\nmore likely to be excluded, or more likely to exhibit no\\nchange relative to other tissues. To demonstrate the effective\\nof proposed shrinking Deep Learning with recall approach,\\nwe use a simple DNN network of different number of layers\\nand neurons with optimal tangent activation function and\\nsigmoid output function. We use the following average sum\\nsquared error criteria to evaluate the model performance\\nerror =\\n\\u2211n\\ni=1 \\u2016yi \\u2212 y\\n(0)\\ni \\u2016\\n2\\/n, where yi is the predict\\nvector label and y(0)i is the ground-truth label vector, n is\\nnumber of samples. The DNN batchsize is 100 and recall\\nthreshold t = 20% \\u00d7 2500. We test on different network\\nsettings and choose the best. Table V shows the experiment\\nresult.\\nVI. CONCLUSION\\nIn conclusion, we proposed a shrinking Deep Learning\\nwith recall (sDLr) approach and the main contribution of\\nsDLr is that it can reduce the running time significantly. Ex-\\ntensive experiments on 4 datasets show that shrinking Deep\\nLearning with recall can reduce training time significantly\\nwhile still gives competitive classification performance.\\nREFERENCES\\n[1] G. E. Hinton and R. R. Salakhutdinov, \\u201cReducing the dimen-\\nsionality of data with neural networks,\\u201d Science, vol. 313, no.\\n5786, pp. 504\\u2013507, 2006.\\n[2] Y. Bengio, \\u201cLearning deep architectures for ai,\\u201d Foundations\\nand trends R\\u00a9 in Machine Learning, vol. 2, no. 1, pp. 1\\u2013127,\\n2009.\\n[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \\u201cImagenet\\nclassification with deep convolutional neural networks,\\u201d in\\nAdvances in neural information processing systems, 2012, pp.\\n1097\\u20131105.\\n[4] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.\\nManzagol, \\u201cStacked denoising autoencoders: Learning useful\\nrepresentations in a deep network with a local denoising cri-\\nterion,\\u201d The Journal of Machine Learning Research, vol. 11,\\npp. 3371\\u20133408, 2010.\\n[5] R. Salakhutdinov and G. E. Hinton, \\u201cDeep boltzmann ma-\\nchines,\\u201d in International Conference on Artificial Intelligence\\nand Statistics, 2009, pp. 448\\u2013455.\\n[6] G. E. Hinton, \\u201cTraining products of experts by minimizing\\ncontrastive divergence,\\u201d Neural computation, vol. 14, no. 8,\\npp. 1771\\u20131800, 2002.\\n[7] L. Deng, \\u201cThree classes of deep learning architectures and\\ntheir applications: a tutorial survey,\\u201d APSIPA transactions on\\nsignal and information processing, 2012.\\n[8] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \\u201cGradient-\\nbased learning applied to document recognition,\\u201d Proceedings\\nof the IEEE, vol. 86, no. 11, pp. 2278\\u20132324, 1998.\\n[9] Y. LeCun and Y. Bengio, \\u201cConvolutional networks for images,\\nspeech, and time series,\\u201d The handbook of brain theory and\\nneural networks, vol. 3361, no. 10, 1995.\\n[10] T. Joachims, \\u201cMaking large scale svm learning practical,\\u201d\\nUniversita\\u00a8t Dortmund, Tech. Rep., 1999.\\n[11] J. Narasimhan, A. Vishnu, L. Holder, and A. Hoisie, \\u201cFast\\nsupport vector machines using parallel adaptive shrinking on\\ndistributed systems,\\u201d arXiv preprint arXiv:1406.5161, 2014.\\n[12] J. Wang, J. Zhou, P. Wonka, and J. Ye, \\u201cLasso screening\\nrules via dual polytope projection,\\u201d in Advances in Neural\\nInformation Processing Systems, 2013, pp. 1070\\u20131078.\\n[13] A. Bonnefoy, V. Emiya, L. Ralaivola, and R. Gribonval,\\n\\u201cA dynamic screening principle for the lasso,\\u201d in Signal\\nProcessing Conference (EUSIPCO), 2014 Proceedings of the\\n22nd European. IEEE, 2014, pp. 6\\u201310.\\n[14] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs,\\nC. Roxburgh, and A. H. Byers, \\u201cBig data: The next frontier\\nfor innovation, competition, and productivity,\\u201d 2011.\\n[15] S. Zheng, X. Cai, C. H. Ding, F. Nie, and H. Huang, \\u201cA\\nclosed form solution to multi-view low-rank regression.\\u201d in\\nAAAI, 2015, pp. 1973\\u20131979.\\n[16] S. Zheng and C. Ding, \\u201cKernel alignment inspired linear\\ndiscriminant analysis,\\u201d in Joint European Conference on\\nMachine Learning and Knowledge Discovery in Databases.\\nSpringer Berlin Heidelberg, 2014, pp. 401\\u2013416.\\n[17] D. Williams, S. Zheng, X. Zhang, and H. Jamjoom, \\u201cTide-\\nwatch: Fingerprinting the cyclicality of big data workloads,\\u201d\\nin IEEE INFOCOM 2014-IEEE Conference on Computer\\nCommunications. IEEE, 2014, pp. 2031\\u20132039.\\n[18] X. Zhang, Z.-Y. Shae, S. Zheng, and H. Jamjoom, \\u201cVirtual\\nmachine migration in an over-committed cloud,\\u201d in 2012\\nIEEE Network Operations and Management Symposium.\\nIEEE, 2012, pp. 196\\u2013203.\\n[19] S. Zheng, Z.-Y. Shae, X. Zhang, H. Jamjoom, and L. Fong,\\n\\u201cAnalysis and modeling of social influence in high perfor-\\nmance computing workloads,\\u201d in European Conference on\\nParallel Processing. Springer Berlin Heidelberg, 2011, pp.\\n193\\u2013204.\\n[20] J. A. Suykens and J. Vandewalle, \\u201cLeast squares support\\nvector machine classifiers,\\u201d Neural processing letters, vol. 9,\\nno. 3, pp. 293\\u2013300, 1999.\\n[21] R. Tibshirani, \\u201cRegression shrinkage and selection via the\\nlasso,\\u201d Journal of the Royal Statistical Society. Series B\\n(Methodological), pp. 267\\u2013288, 1996.\\n[22] E. S. Gardner, \\u201cExponential smoothing: The state of the art,\\u201d\\nJournal of forecasting, vol. 4, no. 1, pp. 1\\u201328, 1985.\\n[23] R. B. Palm, \\u201cPrediction as a candidate for learning deep hi-\\nerarchical models of data,\\u201d Technical University of Denmark,\\n2012.\\n[24] A. Vedaldi and K. Lenc, \\u201cMatconvnet-convolutional neural\\nnetworks for matlab,\\u201d arXiv preprint arXiv:1412.4564, 2014.\\n[25] A. Krizhevsky and G. Hinton, \\u201cLearning multiple layers of\\nfeatures from tiny images,\\u201d 2009.\\n[26] P. Baldi, P. Sadowski, and D. Whiteson, \\u201cSearching for exotic\\nparticles in high-energy physics with deep learning,\\u201d Nature\\ncommunications, vol. 5, 2014.\\n[27] H. Y. Xiong, Y. Barash, and B. J. Frey, \\u201cBayesian prediction\\nof tissue-regulated splicing using rna sequence and cellular\\ncontext,\\u201d Bioinformatics, vol. 27, no. 18, pp. 2554\\u20132562,\\n2011.\\n\",\"id\":24796324,\"identifiers\":[{\"identifier\":\"42718521\",\"type\":\"CORE_ID\"},{\"identifier\":\"187036708\",\"type\":\"CORE_ID\"},{\"identifier\":\"oai:arxiv.org:1605.01369\",\"type\":\"OAI_ID\"},{\"identifier\":\"10.1109\\/icpads.2016.0129\",\"type\":\"DOI\"},{\"identifier\":\"1605.01369\",\"type\":\"ARXIV_ID\"}],\"title\":\"Accelerating Deep Learning with Shrinkage and Recall\",\"language\":{\"code\":\"en\",\"name\":\"English\"},\"magId\":null,\"oaiIds\":[\"oai:arxiv.org:1605.01369\"],\"publishedDate\":\"2016-09-19T00:00:00\",\"publisher\":\"\",\"pubmedId\":null,\"references\":[],\"sourceFulltextUrls\":[\"http:\\/\\/arxiv.org\\/abs\\/1605.01369\"],\"updatedDate\":\"2021-08-04T20:19:55\",\"yearPublished\":2016,\"journals\":[],\"links\":[{\"type\":\"download\",\"url\":\"http:\\/\\/arxiv.org\\/abs\\/1605.01369\"},{\"type\":\"display\",\"url\":\"https:\\/\\/core.ac.uk\\/works\\/24796324\"}]},{\"acceptedDate\":\"2017-07-10T00:00:00\",\"arxivId\":\"1612.03770\",\"authors\":[{\"name\":\"Aimone, James B.\"},{\"name\":\"Carlson, Kristofor D.\"},{\"name\":\"Cox, Jonathan A.\"},{\"name\":\"Draelos, Timothy J.\"},{\"name\":\"James, Conrad D.\"},{\"name\":\"Lamb, Christopher C.\"},{\"name\":\"Miner, Nadine E.\"},{\"name\":\"Severa, William M.\"},{\"name\":\"Vineyard, Craig M.\"}],\"citationCount\":0,\"contributors\":[\"Timothy J.\"],\"outputs\":[\"https:\\/\\/api.core.ac.uk\\/v3\\/outputs\\/278708033\"],\"createdDate\":\"2016-12-23T06:37:36\",\"dataProviders\":[{\"id\":144,\"name\":\"\",\"url\":\"https:\\/\\/api.core.ac.uk\\/v3\\/data-providers\\/144\",\"logo\":\"https:\\/\\/api.core.ac.uk\\/data-providers\\/144\\/logo\"},{\"id\":4786,\"name\":\"\",\"url\":\"https:\\/\\/api.core.ac.uk\\/v3\\/data-providers\\/4786\",\"logo\":\"https:\\/\\/api.core.ac.uk\\/data-providers\\/4786\\/logo\"}],\"depositedDate\":\"2017-05-01T00:00:00\",\"abstract\":\"Neural machine learning methods, such as deep neural networks (DNN), have\\nachieved remarkable success in a number of complex data processing tasks. These\\nmethods have arguably had their strongest impact on tasks such as image and\\naudio processing - data processing domains in which humans have long held clear\\nadvantages over conventional algorithms. In contrast to biological neural\\nsystems, which are capable of learning continuously, deep artificial networks\\nhave a limited ability for incorporating new information in an already trained\\nnetwork. As a result, methods for continuous learning are potentially highly\\nimpactful in enabling the application of deep networks to dynamic data sets.\\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\\nexplore the potential for adding new neurons to deep layers of artificial\\nneural networks in order to facilitate their acquisition of novel information\\nwhile preserving previously trained data representations. Our results on the\\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\\nlower and upper case letters and digits, demonstrate that neurogenesis is well\\nsuited for addressing the stability-plasticity dilemma that has long challenged\\nadaptive machine learning algorithms.Comment: 8 pages, 8 figures, Accepted to 2017 International Joint Conference\\n  on Neural Networks (IJCNN 2017\",\"documentType\":\"research\",\"doi\":\"10.1109\\/ijcnn.2017.7965898\",\"downloadUrl\":\"http:\\/\\/arxiv.org\\/abs\\/1612.03770\",\"fieldOfStudy\":null,\"fullText\":\"Neurogenesis Deep Learning\\nExtending deep networks to accommodate new classes\\nTimothy J. Draelos\\u2217, Nadine E. Miner\\u2217, Christopher C. Lamb\\u2217, Jonathan A. Cox\\u2217\\u2020,\\nCraig M. Vineyard\\u2217, Kristofor D. Carlson\\u2217, William M. Severa\\u2217, Conrad D. James\\u2217, and James B. Aimone\\u2217\\n\\u2217Sandia National Laboratories Albuquerque, NM, USA\\n{tjdrael, nrminer, cclamb, cmviney, kdcarls, wmsever, cdjame, jbaimon}@sandia.gov\\n\\u2020 Present Address: Qualcomm Corporation, San Diego, CA, USA joncox@alum.mit.edu\\nAbstract\\u2014Neural machine learning methods, such as deep\\nneural networks (DNN), have achieved remarkable success in\\na number of complex data processing tasks. These methods have\\narguably had their strongest impact on tasks such as image and\\naudio processing \\u2013 data processing domains in which humans\\nhave long held clear advantages over conventional algorithms. In\\ncontrast to biological neural systems, which are capable of learn-\\ning continuously, deep artificial networks have a limited ability\\nfor incorporating new information in an already trained network.\\nAs a result, methods for continuous learning are potentially\\nhighly impactful in enabling the application of deep networks\\nto dynamic data sets. Here, inspired by the process of adult\\nneurogenesis in the hippocampus, we explore the potential for\\nadding new neurons to deep layers of artificial neural networks\\nin order to facilitate their acquisition of novel information while\\npreserving previously trained data representations. Our results\\non the MNIST handwritten digit dataset and the NIST SD 19\\ndataset, which includes lower and upper case letters and digits,\\ndemonstrate that neurogenesis is well suited for addressing the\\nstability-plasticity dilemma that has long challenged adaptive\\nmachine learning algorithms.\\nKeywords\\u2014deep learning, autoencoder, class conditional sam-\\npling, replay, hippocampus, deep neural networks\\nI. INTRODUCTION\\nMachine learning methods are powerful techniques for\\nstatistically extracting useful information from \\u201cbig data\\u201d\\nthroughout modern society. In particular, deep learning (DL)\\nand other deep neural network (DNN) methods have proven\\nsuccessful in part due to their ability to utilize large volumes of\\nunlabeled data to progressively form sophisticated hierarchical\\nabstractions of information [1], [2]. While DL\\u2019s training and\\nprocessing mechanisms are quite distinct from biological neural\\nlearning and behavior, the algorithmic structure is somewhat\\nanalogous to the visual processing stream in mammals in which\\nprogressively deeper layers of the cortex appear to form more\\nabstracted representations of raw sensory information acquired\\nby the retina [3].\\nDNNs are typically trained once, either with a large amount\\nof labelled data or with a large amount of unlabeled data\\nfollowed by a smaller amount of labeled data used to \\u201cfine-\\ntune\\u201d the network for some particular function, such as\\nhandwritten digit classification. This training paradigm is often\\nvery expensive, requiring several days on large computing\\nclusters [4], so ideally a fully trained network will continue to\\nprove useful for a long duration even if the application domain\\nchanges. DNNs have found some successes in transfer learning,\\ndue to their general-purpose feature detectors at shallow layers\\nof a network [5], [6], but our focus is on situations where that is\\nnot the case. DNNs\\u2019 features are known to get more specialized\\nat deeper layers of a network and therefore presumably less\\nrobust to new classes of data. In this work, we focus on inputs\\nthat a trained network finds difficult to represent. In this regard,\\nwe are addressing the problem of continuous learning (CL).\\nIn reality, DNNs may not be robust to concept drift, where\\nthe data being processed changes gradually over time (e.g., a\\nmovie viewer\\u2019s preferred genres as they age), nor transfer\\nlearning, where a trained model is repurposed to operate in a\\ndifferent domain. Unlike the developing visual cortex, which\\nis exposed to varying inputs over many years, the data used to\\ntrain DNNs is typically limited in scope, thereby diminishing\\nthe applicability of networks to encode information statistically\\ndistinct from the training set. The impact of such training data\\nlimitations is a relatively minor concern in cases where the\\napplication domain does not change (or changes very slowly).\\nHowever, in domains where the sampled data is unpredictable\\nor changes quickly, such as what is seen by a cell phone camera,\\nthe value of a static deep network may be quite limited.\\nOne mechanism the brain has maintained in selective regions\\nsuch as the hippocampus is the permissive birth of new\\nneurons throughout one\\u2019s lifetime, a process known as adult\\nneurogenesis [7]. While the specific function of neurogenesis\\nin memory is still debated, it clearly provides the hippocampus\\nwith a unique form of plasticity that is not present in other\\nregions less exposed to concept drift. The process of biological\\nneurogenesis is complex, but two key observations are that new\\nneurons are preferentially recruited in response to behavioral\\nnovelty and that new neurons gradually learn to encode\\ninformation (e.g., they are not born with pre-programmed\\nrepresentations, rather they learn to integrate over inputs during\\ntheir development) [8].\\nWe consider the benefits of neurogenesis on DL by exploring\\nwhether \\u201cnew\\u201d artificial neurons can facilitate the learning\\nof novel information in deep networks while preserving\\npreviously trained information. To accomplish this, we consider\\na specific illustrative example with the MNIST handwritten\\ndigit dataset [9] and the larger NIST SD 19 dataset [10] that\\nincludes handwritten digits as well as upper and lower case\\nletters. An autoencoder (AE) is initially trained with a subset\\nof a dataset\\u2019s classes and continuous adaptation occurs by\\nar\\nX\\niv\\n:1\\n61\\n2.\\n03\\n77\\n0v\\n2 \\n [c\\ns.N\\nE]\\n  2\\n8 M\\nar \\n20\\n17\\nlearning each remaining class. Our results demonstrate that\\nneurogenesis with hippocampus-inspired \\u201cintrinsic replay\\u201d (IR)\\nenables the learning of new classes with minimal impairment of\\noriginal representations, which is a challenge for conventional\\napproaches that continue to train an existing network on novel\\ndata without structural changes.\\nA. Related Work\\nIn the field of machine learning, transfer learning addresses\\nthe problem of utilizing an existing trained system on a\\nnew dataset containing objects of a different kind. Over the\\npast few years, researchers have examined different ways of\\ntransferring classification capability from established networks\\nto new tasks. Recent approaches have taken a horizontal\\napproach, by transferring layers, rather than a more finely\\ngrained vertically oriented approach of dynamically creating\\nor eliminating individual nodes in a layer. Neurogenesis has\\nbeen proposed to enable the acquisition of novel information\\nwhile minimizing the potential disruption of previously stored\\ninformation [8]. Indeed, neurogenesis and similar processes\\nhave been shown to have this benefit in a number of studies\\nusing shallow neural networks [11]\\u2013[18], although these studies\\nhave typically focused on more conventional transfer learning,\\nas opposed to the continuous adaptation to learning considered\\nhere.\\nAn adaptive DNN architecture by Calandra, et al, shows how\\nDL can be applied to data unseen by a trained network [19].\\nTheir approach hinges on incrementally re-training deep\\nbelief networks (DBNs) whenever concept drift emerges in a\\nmonitored stream of data and operates within constant memory\\nbounds. They utilize the generative capability of DBNs to\\nprovide training samples of previously learned classes. Class\\nconditional sampling from trained networks has biological\\ninspiration [20]\\u2013[23] as well as historical and artificial neural\\nnetwork implementations [24]\\u2013[27].\\nYosinski evaluated transfer capability via high-level layer\\nreuse in specific DNNs [6]. Transferring learning in this\\nway increased recipient network performance, though the\\ncloser the target task was to the base task, the better the\\ntransfer. Transferring more specific layers could actually cause\\nperformance degradation however. Likewise, Kandaswamy, et\\nal., used layer transfer as a means to transfer capability in Con-\\nvolutional Neural Networks and Stacked Denoising AEs [28].\\nTransferring capability in this way resulted in a reduction in\\noverall computation time and lower classification errors. These\\npapers use fixed-sized DNNs, except for additional output\\nnodes for new classes, and demonstrate that features in early\\nlayers are more general than features in later layers and thus,\\nmore transferable to new classes.\\nII. THE NEUROGENESIS DEEP LEARNING ALGORITHM\\nNeurogenesis in the brain provides a motivation for creating\\nDNNs that adapt to changing environments. Here, we introduce\\nthe concept of neurogenesis deep learning (NDL), a process\\nof incorporating new nodes in any level of an existing DNN\\n(Figure 1) to enable the network to adapt as the environment\\nchanges. We consider the specific case of adding new nodes\\nto pre-train a stacked deep AE, although the approach should\\nextend to other types of DNNs as well. An AE is a type\\nof neural network designed to encode data such that they\\ncan be decoded to produce reconstructions with minimal\\nerror. The goal of many DNN algorithms is to learn filters\\nor feature detectors (i.e., weights) where the complexity or\\nspecialization of the features increases at deeper network layers.\\nAlthough successive layers of these feature detectors could\\nrequire an exponential expansion of nodes to guarantee that all\\ninformation is preserved as it progresses into more sophisticated\\nrepresentations (\\u201clossless encoding\\u201d), in practice, deep AEs\\ntypically use a much more manageable number of features\\nby using the training process to select those features that best\\ndescribe the training data. However, there is no guarantee\\nthat the representations of deeper layers will be sufficient to\\nlosslessly encode novel information that is not representative of\\nthe original training set. It is in this latter case that we believe\\nNDL to be most useful, as we have previously suggested that\\nbiological neurogenesis addresses a similar coding challenge\\nin the brain [8].\\nThe first step of the NDL algorithm occurs when a set of\\nnew data points fail to be appropriately reconstructed by the\\ntrained network. A reconstruction error (RE) is computed at\\neach level of a stacked AE (pair of encode\\/decode layers) to\\ndetermine when a level\\u2019s representational capacity is considered\\ninsufficient for a given application. An AE parameterized with\\nweights, W , biases, b, and activation function, s, is described\\nfrom input, x, to output as N encode layers followed by N\\ndecode layers.\\nEncoder: f\\u03b8N \\u25e6f\\u03b8N\\u22121 \\u00b7\\u00b7\\u00b7f\\u03b82\\u25e6f\\u03b81 (x) where y=f\\u03b8(x)=s(Wx+b) (1)\\nDecoder: g\\u03b8\\u2032\\nN\\n\\u25e6g\\u03b8\\u2032\\nN\\u22121\\n\\u00b7\\u00b7\\u00b7g\\u03b8\\u20322\\u25e6g\\u03b8\\u20321 (y) where g\\u03b8\\u2032 (y)=s(W\\n\\u2032y+b\\u2032) (2)\\nGlobal RE is computed at level L of an AE by encoding an\\ninput through L encode layers, then propagating through the\\ncorresponding L decode layers to the output.\\nREGlobal,L(x)=(x\\u2212g\\u03b8\\u2032\\nN\\n\\u25e6\\u00b7\\u00b7\\u00b7g\\u03b8\\u2032\\nN\\u2212L\\n\\u25e6f\\u03b8L\\u25e6\\u00b7\\u00b7\\u00b7f\\u03b81 (x))2 (3)\\nWhen a data sample\\u2019s RE is too high, the assumption is that\\nthe AE level under examination does not contain a rich enough\\nset of nodes (or features as determined by each node\\u2019s weights)\\nto accurately reconstruct the sample. Therefore, it stands to\\nreason that a sufficiently high RE warrants the addition of a\\nnew feature detector (node).\\nThe second step of the NDL algorithm is adding and training\\nnew nodes, which occurs when a critical number of input\\nsamples (outliers) fail to achieve adequate representation at\\nsome level of the network. A new node is also added if\\nthe previous level added one or more nodes. This process\\ndoes not require labels, relying entirely on the quality of a\\nsample\\u2019s representation computed from its reconstruction. If\\nthe RE is too high (greater than a user-specified threshold\\ndetermined from the statistics of reconstructing previously seen\\ndata classes), then nodes are added at that level up to a user-\\nspecified maximum number of new nodes. The new nodes\\nFig. 1. Illustration of NDL processing MNIST digits (orange\\/red circles\\nindicate accuracte\\/inaccurate feature representations of the input; green indicate\\nnew nodes added via neurogenesis). (A) AE can faithfully reconstruct originally\\ntrained digit (\\u20187\\u2019), but (B) fails at reconstructing novel digit (\\u20184\\u2019). (C) New\\nnodes added to all levels enables AE to reconstruct \\u20184\\u2019. Level 1\\u20134 arrows\\nshow how inputs can be reconstructed at various depths.\\nare trained using all nodes in the level for reconstruction on\\nall outliers. In other words, during training of the new nodes,\\nthe reconstructions, errors, gradients, and weight updates are\\ncalculated as a function of an AE that uses the entire set of\\nnodes in the current level within a single hidden layer AE (SHL-\\nAE). In order to not disturb the existing feature detectors, only\\nthe encoder weights connected to the new nodes are updated\\nin the level under consideration. Decoder weights connected\\nto existing feature detectors (nodes) are allowed to change\\nslightly at the learning rate divided by 100.\\nThe final step of the NDL algorithm is intended to stabilize\\nthe network\\u2019s previous representations in the presence of newly\\nadded nodes. It involves training all nodes in a level with new\\ndata and replayed samples from previously seen classes on\\nwhich the network has been trained. Samples from old classes,\\nwhere original data no longer exists, are created using the\\nencoding and reconstruction capability of the current network\\nin a process we call \\u201cintrinsic replay\\u201d (IR) (Figure 2).\\nThis IR process is analogous to observed dynamics within\\nthe brain\\u2019s hippocampus during memory consolidation [20]. It\\nappears that neural regions such as the hippocampus \\u201creplay\\u201d\\nneuronal sequences originally experienced during learned\\nbehaviors or explorations in an effort to strengthen and stabilize\\nnewly acquired information alongside previously encoded\\ninformation. Our method involves storing class-conditional\\nstatistics (mean and Cholesky factorization of the covariance)\\nof the top layer of the encoding network, E.\\n\\u00b5E = Mean(E), ChE = Chol(Cov(E)) (4)\\nThe Cholesky decomposition requires n3\\/6 operations [29],\\nwhere n is the dimension of E, and is performed once for\\neach class on a trained network. High-level representations\\nare retrieved through sampling from a Normal distribution\\ndescribed by these statistics and, leveraging the decoding\\nnetwork, new data points from previously trained classes are\\nreconstructed.\\nIR Images = Decode(\\u00b5E +N(0, 1) \\u2217 ChE) (5)\\nFig. 2. Illustration of the intrinsic replay process used in NDL. Original\\ndata presented to a trained network results in high-level representations in\\nthe \\u201ctop-most\\u201d layer of the encoder. The average entries and the Cholesky\\ndecomposition of the covariance matrix of this hidden layer are stored for\\neach class (e.g., \\u20181\\u2019s, \\u20187\\u2019s, and \\u20180\\u2019s). When \\u201creplayed\\u201d values are desired for\\na given class, samples are drawn randomly from a normal distribution defined\\nby the class\\u2019s stored statistics. Then, using the AE\\u2019s reconstruction pathway,\\nnew digits of the stored class are approximated.\\nTraining samples from previously seen data classes, where\\noriginal data no longer exists, are generated using (5), which\\ninvolves a single feed-forward pass through the Decoder (2).\\nIII. EXPERIMENTS\\nWe evaluated NDL on two datasets, the MNIST [9] and NIST\\nSD 19 [10] datasets. For the NIST dataset, we downsampled\\nthe original 128x128 pixel images to be 28x28 (the MNIST\\nimage size). However, we did not otherwise normalize the\\ncharacters within classes, so the variation in scale and location\\nwithin the 28x28 frame is much greater than the MNIST data.\\nFor the MNIST dataset, a deep AE was pre-trained in a\\nstacked layered manner on a subset of the dataset classes,\\nthen training with and without NDL and with and without\\nIR was conducted on new unseen data classes. The AE was\\ninitially trained with two digits (1, 7) that are not statistically\\nrepresentative of the other digits (as shown in the results).\\nThen, learning was incrementally performed with the remaining\\ndigits. We used an 8-layer AE inspired by Hinton\\u2019s network on\\nMNIST [30], but reduced to 784-200-100-75-20-75-100-200-\\n784 since only a subset of digits (1, 7) were used for initial\\ntraining. For each experiment, all training samples in a class\\nwere presented at once.\\nFor the NIST SD 19 dataset, the AE was trained on the\\ndigit classes alone (0\\u20139), and then learning was performed\\nincrementally on all letters (upper and lower case; A-Z, a-z).\\nIn order to evaluate the impact of NDL on the NIST dataset\\nwithout the potentially complicating factor of IR, training data\\nwas used for replaying old classes. The initial AE used for\\nthe NIST SD 19 dataset is also inspired by Hinton\\u2019s MNIST\\nnetwork, where the only difference is the number of highest-\\nlevel features. We used 50 instead of 30 high-level features\\nProcedure 1 Neurogenesis Deep Learning (NDL)\\nInput: 2N -layer AE trained on data classes D1\\u2013DU\\u22121, new class of data\\nDU , vector of per-level RE thresholds Th, vector of per-level maximum\\nnodes allowed MaxNodes, maximum number of samples allowed to have\\nREGlobal,L \\u003E ThL, MaxOutliers, Learning Rate LR\\nOutput: Autoencoder AE capable of representing classes D1\\u2013DU\\n\\/\\/ Create stabilization training data\\nAEStableTrain \\u2190 {DU |IntrinsicReplay(D1\\u2013DU\\u22121)}\\n\\/\\/ Perform Neurogenesis\\nfor LevelL\\u2190 1 to N do\\nNewNodes\\u2190 0\\nOutliers\\u2190 {d 3 DU |REGlobal,L(d) \\u003E ThL}\\nNOut \\u2190 |Outliers|\\n\\/\\/ Add new nodes to AEL and train\\nwhile NOut \\u003E MaxOutliers and NewNodes \\u003C MaxNodesL do\\nAEL \\u2190WL, bL;W \\u2032N+1\\u2212L, b\\u2032N+1\\u2212L from AE\\nPlasticity: NodesNew = # of new nodes to add\\nAdd NodesNew to AEL and Train on Outliers\\nUse LR to update encoder weights connected to new nodes only\\nUse LR\\/100 to update decoder weights\\nStability: Train AEL on AEStableTrain using LR\\/100 to update\\nall weights\\nWL, bL;W\\n\\u2032\\nN+1\\u2212L, b\\n\\u2032\\nN+1\\u2212L \\u2190 AEL\\nOutliers\\u2190 {d 3 DU |REGlobal,L(d) \\u003E ThL}\\nNOut \\u2190 |Outliers|\\nNewNodes\\u2190 NewNodes+NodesNew\\n\\/\\/ Add random weights from new nodes in level L to existing nodes in\\nlevel L+ 1 and train AEL+1\\nif NewNodes \\u003E 0 and L \\u003C N then\\nPlasticity: Train AEL+1 on DU\\nStability: Train AEL+1 on AEStableTrain\\nWL+1, bL+1;W\\n\\u2032\\nN\\u2212L, b\\n\\u2032\\nN\\u2212L \\u2190 AEL+1\\nsince there is much more variation in scale and location in\\nthe NIST digits. The trained NIST SD 19 \\u2018Digits\\u2019 network is\\n784-1000-500-250-50-250-500-1000-784.\\nIV. RESULTS ON MNIST\\nA. Trained networks have limited ability to represent novel\\ninformation\\nTo illustrate the process of NDL on MNIST data, we first\\ntrained a deep AE (784-1000-500-250-30-250-500-1000-784)\\nto encode a subset of MNIST classes. Then, nodes were added\\nvia neurogenesis to the trained AE network as needed to encode\\neach remaining digit. The initial DNN size for our illustrative\\nexample was determined as follows. In Calandra\\u2019s work, a\\n784-600-500-400-10 DBN classifier was trained initially on\\ndigits 4, 6, and 8 and then presented with new digits for\\ntraining together with samples of 4, 6, and 8 generated from\\nthe DBN [19]. We examined two subsets of digits for initial\\ntraining of our AE (4, 6, and 8, as in Calandra, et al. [19], or\\n1 and 7). Figure 3A illustrates that digits 4, 6, and 8 appear\\nto contain a more complete set of digit features as seen by\\nthe quality of the reconstructions compared to training only\\non 1 and 7 (Figure 3B), although both limited training sets\\nyield impaired reconstructions of novel (untrained) digits. We\\nchose to focus initial training on digits 1 and 7, as these digits\\nrepresent what may be the smallest set of features in any\\npair of digits. Then, continuous learning was simulated by\\nFig. 3. Networks initially trained on (A) \\u20184,\\u2019 \\u20186,\\u2019 and \\u20188\\u2019s and (B) \\u20181,\\u2019 and\\n\\u20187\\u2019s and not yet trained on any of the other MNIST digits reconstruct those\\nnovel digits using features biased by their original training data.\\nprogressively expanding the number of encountered classes\\nthrough adding samples from the remaining digits in sequence\\none class at a time. The Calandra network was shown to have\\novercapacity for just 3 digits by virtue of its subsequent ability\\nto learn all 10 digits. We suspect the same overcapacity for\\nHinton\\u2019s network and therefore start with a network roughly\\n1\\/5 the size, under the assumption that neurogenesis will\\ngrow a network sufficient to learn the remaining digits as\\nthey are individually presented for training. Thus the size of\\nour initial DNN prior to neurogenesis was: 784-200-100-75-20-\\n75-100-200-784. Accordingly, we trained a 1,7-network using\\nall training samples of 1\\u2019s and 7\\u2019s with a stacked denoising\\nAE. After training the 1,7-AE, it is ready to address drifting\\ninputs through NDL. New classes of digits are presented\\nin the following order: 0, 2, 3, 4, 5, 6, 8, and 9. Notably,\\nthis procedure is not strictly concept drift (where classes are\\nchanging over time) or transfer learning (where a trained\\nnetwork is retrained to apply to a different domain), but rather\\nwas designed to examine the capability of the network to\\nlearn novel inputs while maintaining the stability of previous\\ninformation (i.e., address the stability-plasticity dilemma).\\nNDL begins by presenting all samples of a new class to\\nLevel 1 of the AE and identifying \\u2018outlier\\u2019 samples having\\nREs above a user-specified threshold. Then, one or more new\\nnodes are added to Level 1 and the entire level is pre-trained in\\na SHL-AE. Initially, only the weights connected to the newly\\nadded nodes are allowed to be updated at the full learning\\nrate. Encoder weights connected to old nodes are not allowed\\nto change at all (to preserve the feature detectors trained on\\nprevious classes) and decoder weights from old nodes are\\nallowed to change at the learning rate divided by 100. This\\nstep relates to the notion of plasticity in biological neurogenesis.\\nAfter briefly training the new nodes, a stabilization step takes\\nplace, where the entire level is trained in a SHL-AE using\\ntraining samples from all classes seen by the network (samples\\nfrom old classes are generated via intrinsic replay). After again\\ncalculating the RE on samples from the new class, additional\\nnodes are added until either 1) the RE for enough samples falls\\nbelow the threshold or 2) a user-specified maximum number of\\nnew nodes are reached for the current level. Once neurogenesis\\nis complete for a level, weights connecting to the next level\\nare trained using samples from all classes. This process repeats\\nfor each succeeding level of the AE using outputs from the\\nprevious encoding layer. After NDL, the new AE should be\\ncapable of reconstructing images from the new class (e.g., 0)\\nin addition to the current previous classes (e.g., 1 and 7).\\nB. Neurogenesis allow encoding of novel information\\nResults of NDL experiments on MNIST data showed that\\nan established network trained on just digits 1 and 7 can\\nbe enlarged through neurogenesis to represent new digits as\\nguided by RE at each level of a stacked AE. We compared\\na network created with NDL and IR (\\u2018NDL+IR\\u2019) to three\\ncontrol networks: Control 1 (\\u2018CL\\u2019) \\u2013 an AE the same size\\nas the enlarged NDL without IR network trained first on the\\nsubset digits 1 and 7, and then retrained without intrinsic\\nreplay on all samples from one new single digit at a time\\n(Figure 4A); Control 2 (\\u2018NDL\\u2019) \\u2013 continuous learning on the\\noriginal 1,7 network using NDL, but not using intrinsic replay\\n(Figure 4B), and Control 3 (\\u2018CL+IR\\u2019) \\u2013 an AE the same size\\nas the enlarged NDL+IR network trained first on the subset\\ndigits 1 and 7, and then retrained with all samples from one\\nnew single digit at a time, while using intrinsic replay to\\ngenerate samples of previously trained classes throughout the\\nexperiment (Figure 4C). Figure 4D shows that the network built\\nupon NDL+IR slightly outperforms learning on a fixed network\\n(Figure 4C). Notably, NDL+IR outperforms straight learning\\nnot only on reconstruction across all digits, but in both the\\nability to represent the new data as well as preserving the ability\\nto represent previously trained digits (Figure 4E). This latter\\npoint is important, because while getting a trained network to\\nlearn new information is not particularly challenging, getting\\nit to preserve old information can be quite difficult.\\nNote that the final DNN size is unknown prior to neurogen-\\nesis. The network size is increased based on the RE when the\\nnetwork is exposed to new information, so there is possible\\nvalue in using this method to determine an effective DNN\\nsize. The original size of the 1,7-AE is 784-200-100-75-20-\\n75-100-200-784. Figure 4F shows how the DNN grows as\\nnew classes are presented during neurogenesis, gaining more\\nrepresentational capacity as new classes are learned.\\nThe \\u2018CL+IR\\u2019 control network initially had the identical size\\nof the neurogenesis network \\u2018NDL+IR\\u2019, was initially trained\\non digits 1 and 7, and then learned to represent the remaining\\nMNIST digits, one at a time in the same order as presented\\nduring neurogenesis, but the network size was fixed. Figure 5A\\nshows reconstructed images after each new class was learned on\\nthe \\u2018CL+IR\\u2019 AE and Figure 5B shows the comparable images\\nfor the \\u2018NDL+IR\\u2019 network as it was trained to accommodate\\nall MNIST digits. One can see that before being trained on new\\ndigits (to the right of the blocked trained class shown in each\\nFig. 4. Global reconstructions of trained MNIST digits after exposure to all\\n10 digits. The legend in Plot D applies to Plots A, B, and C; the dotted line\\nshows REs of the original AE trained just on 1 and 7. (A) CL without IR\\nprovides only marginal improvement in reconstruction ability after learning\\nall 10 digits; (B) NDL without IR likewise fails to improve reconstruction,\\nthough NDL training makes reconstruction through partial networks more\\nuseful; (C) CL with IR improves overall reconstruction of previously trained\\ndigits; (D) NDL with IR further improves on CL with IR in (C) along with\\nimproved partial network reconstructions; (E) Full network reconstructions\\nof all networks after progressive training through all digits; (F) Neurogenesis\\ncontribution to network size in NDL+IR networks.\\nFig. 5. Reconstructions of all digits by pre-trained \\u20181\\/7\\u2019 networks after\\nlearning on progressive new classes. (A) Networks using conventional learning\\nwith IR are able to acquire new digits and show some ability to maintain\\nrepresentations of recently trained digits (e.g., \\u20196\\u2019s after \\u20188\\u2019 is learned). (B)\\nNetworks using NDL with IR are able to acquire new digits and show superior\\nreconstructions of previously encountered digits, even for those digits trained\\nfar earlier (e.g., \\u20190\\u2019s throughout the experiment).\\nrow), both networks mis-reconstructed digits from the unseen\\nclasses into digits that appear to belong to a previously trained\\nclass as expected. Notably in the \\u2018CL+IR\\u2019 reconstructions\\n(Figure 5A), digits from previously seen classes were often\\nmis-reconstructed to more recently seen classes. In contrast,\\nthe \\u2018NDL+IR\\u2019 networks (Figure 5B) were more stable in their\\nrepresentations of previously encoded data, with only minimal\\nFig. 6. Comparison of initial and final (left and right of each line segment, respectively) full AE (Level 4) REs on each class after learning of all classes.\\nFig. 7. Neurogenesis contribution to networks trained on digits only, where new classes are presented for NDL alphabetically, upper case first.\\ndisruption to past classes as new information was acquired.\\nThis suggests that adding neurons as a network is exposed to\\nnovel information is advantageous in maintaining the stability\\nof a DNN\\u2019s previous representations.\\nV. RESULTS ON NIST SD 19\\nApplying NDL to the NIST SD 19 dataset presents chal-\\nlenges for evaluating neurogenesis performance because of\\nthe number of classes. Figure 6 shows the effect of learning\\non each class, comparing the initial RE of each class on the\\nnetwork trained on digits before any learning of letters and the\\nfinal RE after all classes have been learned. A line segment\\nwith a downward (negative) slope indicates that the final RE\\nis less than the initial RE.\\nThe clear observation is that learning new classes with\\nNDL with intrinsic replay (NDL+IR) results in smaller RE\\nthan learning without neurogenesis (CL+IR) for all classes. In\\naddition, the final REs for NDL+IR are all lower than the initial\\nREs, even for classes (digits) used to train the original AE.\\nThis implies that the ultimate AE built via neurogenesis has a\\nricher set of feature detectors, resulting in better representation\\nof all classes. Another observation is that, in general, the initial\\nREs of the CL+IR network are lower than the initial REs of\\nthe NDL+IR network. The reason is that the original NDL+IR\\nnetwork was smaller than the fixed CL+IR network.\\nWhile Figure 6 shows the improvement in class representa-\\ntion at the beginning and end of NDL+IR, Figures 7 and 8 show\\nthe progression in time of growing the final network. More new\\nneurons are added earlier in the neurogenesis process than later.\\nAs novel classes are presented, new feature are learned and\\nrepresentation capability improves for all classes. Eventually,\\nthe need for additional neurons diminishes. Figure 7 reveals\\nthat the AE is particularly lacking feature detectors necessary\\nfor good representation of class \\u2018M\\u2019 in all levels. In Figure 6,\\nit is clear that class \\u2018W\\u2019 is also lacking feature detectors, but\\nby the time it is presented for learning, its need has been met\\nby neurogenesis on previous classes.\\nFig. 8. Neurogenesis contribution to networks trained on digits only. 20 experiments were conducted where newly presented classes (upper and lower case\\nletters) were randomly ordered. The plot shows the average number of new neurons added progessively for each new class with standard deviation as error bars.\\nVI. CHARACTERIZING THE VALUE OF ADAPTING DNNS\\nThe value of a model to continuously adapt to changing\\ndata is challenging to quantify. Here, we notionally quantify\\nthe value of a machine learning algorithm at a given time as\\nU = B \\u2212 CM\\u03c4 \\u2212 CP , where the utility, U , of an algorithm is\\nconsidered as a tradeoff between the benefit, B, that the com-\\nputation provides the user, the costs of the algorithm generation\\nor the model itself, CM , and the associated run-time costs,\\nCP , of that computation. CP typically consists of the time and\\nphysical energy and space required for the computation to be\\nperformed. For machine learning applications, we must consider\\nthe lifetime, \\u03c4 , of an algorithm for which it is appropriate\\nto amortize a model\\u2019s build costs. In algorithm design, it is\\ndesirable to minimize both of the cost terms; however, the\\ndominant cost will differ depending on the extent to which\\nthe real-world data changes. Consider a DNN with N neurons\\nand on the order of N2 synapses. In this example, the cost of\\nbuilding the model, CM , will scale as O(N4) due to performing\\nO(N2) operations over N2 training samples during training\\nof a well-regularized, appropriately fit model. As a result, CM\\nwill dominate the algorithm\\u2019s cost unless the lifetime of the\\nmodel, \\u03c4 , can offset the polynomial difference between CM\\nand CP . This description illustrates the need to extend the\\nmodel\\u2019s lifetime (e.g., via neurogenesis), and to do so in an\\ninexpensive manner that minimizes the data required to adapt\\nthe model for future use.\\nVII. CONCLUSIONS AND FUTURE WORK\\nWe presented a new adaptive algorithm using the concept\\nof neurogenesis to extend the learning of an established DNN\\nwhen presented with new data classes. The algorithm adds new\\nneurons at each level of an AE when new data samples are\\nnot accurately represented by the network as determined by a\\nhigh RE. The focus of the paper is on a proof of concept of\\ncontinuous learning for DNNs to adapt to changing application\\ndomains. Several elements of our NDL algorithm that we\\nhave not sought to optimize deserve further consideration. For\\ninstance, the optimal number of IR samples is unknown and\\nwill affect the computational cost associated with their use.\\nOther elements that need to be considered are 1) better ways\\nof establishing and using RE thresholds and 2) developing a\\nmethod to determine the number of outliers to allow during\\nneurogenesis. While we considered a network of growing size\\nvia neurogenesis, adaptation may be obtainable use of a larger\\nnetwork with a fixed size and restricting the learning rate on a\\nsubset of neurons until needed at a later time. We evaluated\\nthe NDL algorithm on two datasets having gray-scale objects\\non blank backgrounds and look forward to application on\\nadditional datasets, including natural, color imagery.\\nUltimately, we anticipate that there are several significant\\nadvantages of a neurogenesis-like method for adapting existing\\nnetworks to incorporate new data, particularly given suitable IR\\ncapabilities. The first relates to the costs of DL in application\\ndomains. The ability to adapt to new information can extend\\na model\\u2019s useful lifetime in real-world situations, possibly by\\nsubstantial amounts. Extending a model\\u2019s lifetime increases the\\nduration over which one can amortize the costs of developing\\nthe model, and in the case of DL, the build cost often\\nvastly outpaces the runtime operational costs of the trained\\nfeed-forward network. As a result, continuous adaptation can\\npotentially make DL cost effective for domains with significant\\nconcept drift. Admittedly, the method we describe here has an\\nadded processing cost due to the neurogenesis process and the\\nrequired intrinsic replay; however, this cost will most likely\\namount to a constant factor increase on the processing costs\\nand still be significantly lower than those costs associated with\\nrepeatedly retraining with the original training data.\\nThe second advantage concerns the continuous learning\\nnature of the NDL algorithm. The ability to train a large\\nnetwork without maintaining a growing repository of data\\ncan be valuable, particularly in cases where the bulk storage\\nof data is not permitted due to costs or other restrictions.\\nWhile much of the DL community has focused on cases where\\nthere is extensive unlabeled training data, our technique can\\nprovide solutions where training data at any time is limited\\nand new data is expected to arrive continuously. Furthermore,\\nwe have considered a very stark change in the data landscape,\\nwith the network exposed exclusively to novel classes. In real-\\nworld applications, novel information may be encountered more\\ngradually. This slower drift would likely require neurogenesis\\nless often, but it would be equally useful when needed.\\nFinally, it has not escaped us that the algorithm we present\\nis emulating adult neurogenesis within a cortical-like circuit,\\nwhereas in adult mammals, substantial neurogenesis does not\\nappear in sensory cortices [7]. In this way, our NDL networks\\nare more similar to juvenile or developmental visual systems,\\nwhere the network has only been exposed to a limited extent\\nof the information it will eventually encounter. Presumably, if\\none takes a DNN with many more nodes per layer and trains\\nit with a much larger and broader set of data, the requirement\\nfor neurogenesis will diminish. In this situation, we predict\\nthat the levels of neurogenesis will eventually diminish to\\nzero early in the network because the DNN will have the\\nability to represent a broad set of low level features that prove\\nsufficient for even the most novel data encountered, whereas\\nneurogenesis may always remain useful at the deepest network\\nlayers that are more comparable to the medial temporal lobe and\\nhippocampus areas of cortex. Indeed, this work illustrates that\\nthe incorporation of neural developmental and adult plasticity\\nmechanisms, such as staggering network development by layer\\n(e.g., \\u201clayergenesis\\u201d), into conventional DNNs will likely\\ncontinue to offer considerable benefits.\\nAcknowledgments: This work was supported by Sandia\\nNational Laboratories\\u2019 Laboratory Directed Research and De-\\nvelopment (LDRD) Program under the Hardware Acceleration\\nof Adaptive Neural Algorithms (HAANA) Grand Challenge.\\nSandia is a multi-mission laboratory managed and operated by\\nSandia Corporation, a wholly owned subsidiary of Lockheed\\nMartin Corporation, for the U.S. Department of Energy\\u2019s\\nNational Nuclear Security Administration under Contract DE-\\nAC04-94AL85000.\\nREFERENCES\\n[1] Y. LeCun, Y. Bengio, and G. Hinton, \\u201cDeep learning,\\u201d Nature, vol. 521,\\nno. 7553, pp. 436\\u2013444, 2015.\\n[2] J. Schmidhuber, \\u201cDeep learning in neural networks: An overview,\\u201d Neural\\nnetworks, vol. 61, pp. 85\\u2013117, 2015.\\n[3] D. C. Van Essen, C. H. Anderson, and D. J. Felleman, \\u201cInformation\\nprocessing in the primate visual system: an integrated systems perspective,\\u201d\\nScience, vol. 255, no. 5043, p. 419, 1992.\\n[4] Q. V. Le, \\u201cBuilding high-level features using large scale unsupervised\\nlearning,\\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013\\nIEEE International Conference on. IEEE, 2013, pp. 8595\\u20138598.\\n[5] D. C. Cires\\u00b8an, U. Meier, and J. Schmidhuber, \\u201cTransfer learning for latin\\nand chinese characters with deep neural networks,\\u201d in Neural Networks\\n(IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp.\\n1\\u20136.\\n[6] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, \\u201cHow transferable are\\nfeatures in deep neural networks?\\u201d in Advances in neural information\\nprocessing systems, 2014, pp. 3320\\u20133328.\\n[7] J. B. Aimone, Y. Li, S. W. Lee, G. D. Clemenson, W. Deng, and F. H.\\nGage, \\u201cRegulation and function of adult neurogenesis: from genes to\\ncognition,\\u201d Physiological reviews, vol. 94, no. 4, pp. 991\\u20131026, 2014.\\n[8] J. B. Aimone, W. Deng, and F. H. Gage, \\u201cResolving new memories:\\na critical look at the dentate gyrus, adult neurogenesis, and pattern\\nseparation,\\u201d Neuron, vol. 70, no. 4, pp. 589\\u2013596, 2011.\\n[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \\u201cGradient-based learning\\napplied to document recognition,\\u201d Proceedings of the IEEE, vol. 86,\\nno. 11, pp. 2278\\u20132324, 1998.\\n[10] P. J. Grother, \\u201cNist special database 19,\\u201d Handprinted forms and\\ncharacters database, National Institute of Standards and Technology,\\n1995.\\n[11] P. A. Appleby, G. Kempermann, and L. Wiskott, \\u201cThe role of additive\\nneurogenesis and synaptic plasticity in a hippocampal memory model\\nwith grid-cell like input,\\u201d PLoS Comput Biol, vol. 7, no. 1, p. e1001063,\\n2011.\\n[12] P. A. Appleby and L. Wiskott, \\u201cAdditive neurogenesis as a strategy\\nfor avoiding interference in a sparsely-coding dentate gyrus,\\u201d Network:\\nComputation in Neural Systems, vol. 20, no. 3, pp. 137\\u2013161, 2009.\\n[13] G. A. Carpenter and S. Grossberg, \\u201cThe art of adaptive pattern recognition\\nby a self-organizing neural network,\\u201d Computer, vol. 21, no. 3, pp. 77\\u201388,\\n1988.\\n[14] R. A. Chambers and S. K. Conroy, \\u201cNetwork modeling of adult\\nneurogenesis: shifting rates of neuronal turnover optimally gears network\\nlearning according to novelty gradient,\\u201d Journal of cognitive neuroscience,\\nvol. 19, no. 1, pp. 1\\u201312, 2007.\\n[15] R. A. Chambers, M. N. Potenza, R. E. Hoffman, and W. Miranker, \\u201cSim-\\nulated apoptosis\\/neurogenesis regulates learning and memory capabilities\\nof adaptive neural networks,\\u201d Neuropsychopharmacology, vol. 29, no. 4,\\np. 747, 2004.\\n[16] C. Crick and W. Miranker, \\u201cApoptosis, neurogenesis, and information\\ncontent in hebbian networks,\\u201d Biological cybernetics, vol. 94, no. 1, pp.\\n9\\u201319, 2006.\\n[17] L. Wiskott, M. J. Rasch, and G. Kempermann, \\u201cA functional hypothesis\\nfor adult hippocampal neurogenesis: avoidance of catastrophic interfer-\\nence in the dentate gyrus,\\u201d Hippocampus, vol. 16, no. 3, pp. 329\\u2013343,\\n2006.\\n[18] J. B. Aimone and F. H. Gage, \\u201cModeling new neuron function: a history of\\nusing computational neuroscience to study adult neurogenesis,\\u201d European\\nJournal of Neuroscience, vol. 33, no. 6, pp. 1160\\u20131169, 2011.\\n[19] R. Calandra, T. Raiko, M. P. Deisenroth, and F. M. Pouzols, \\u201cLearning\\ndeep belief networks from non-stationary streams,\\u201d in International\\nConference on Artificial Neural Networks. Springer, 2012, pp. 379\\u2013386.\\n[20] M. F. Carr, S. P. Jadhav, and L. M. Frank, \\u201cHippocampal replay in the\\nawake state: a potential substrate for memory consolidation and retrieval,\\u201d\\nNature neuroscience, vol. 14, no. 2, pp. 147\\u2013153, 2011.\\n[21] K. Louie and M. A. Wilson, \\u201cTemporally structured replay of awake\\nhippocampal ensemble activity during rapid eye movement sleep,\\u201d Neuron,\\nvol. 29, no. 1, pp. 145\\u2013156, 2001.\\n[22] R. Stickgold, \\u201cSleep-dependent memory consolidation,\\u201d Nature, vol. 437,\\nno. 7063, pp. 1272\\u20131278, 2005.\\n[23] D. J. Felleman and D. C. Van Essen, \\u201cDistributed hierarchical processing\\nin the primate cerebral cortex,\\u201d Cerebral cortex, vol. 1, no. 1, pp. 1\\u201347,\\n1991.\\n[24] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal, \\u201cThe \\u0022Wake-Sleep\\u0022\\nalgorithm for unsupervised neural networks,\\u201d Science, vol. 268, no. 5214,\\np. 1158, 1995.\\n[25] R. Salakhutdinov, \\u201cLearning deep generative models,\\u201d Ph.D. dissertation,\\nUniversity of Toronto, 2009.\\n[26] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra,\\n\\u201cDRAW: A recurrent neural network for image generation,\\u201d arXiv preprint\\narXiv:1502.04623, 2015.\\n[27] J. Rudy and G. Taylor, \\u201cGenerative class-conditional autoencoders,\\u201d arXiv\\npreprint arXiv:1412.7009, 2014.\\n[28] C. Kandaswamy, L. M. Silva, L. A. Alexandre, J. M. Santos, and J. M.\\nde S\\u00e1, \\u201cImproving deep neural network performance by reusing features\\ntrained with transductive transference,\\u201d in International Conference on\\nArtificial Neural Networks. Springer, 2014, pp. 265\\u2013272.\\n[29] A. Krishnamoorthy and D. Menon, \\u201cMatrix inversion using cholesky\\ndecomposition,\\u201d in Signal Processing: Algorithms, Architectures, Ar-\\nrangements, and Applications (SPA), 2013. IEEE, 2013, pp. 70\\u201372.\\n[30] G. E. Hinton and R. R. Salakhutdinov, \\u201cReducing the dimensionality of\\ndata with neural networks,\\u201d science, vol. 313, no. 5786, pp. 504\\u2013507,\\n2006.\\n\",\"id\":37687589,\"identifiers\":[{\"identifier\":\"73954810\",\"type\":\"CORE_ID\"},{\"identifier\":\"10.1109\\/ijcnn.2017.7965898\",\"type\":\"DOI\"},{\"identifier\":\"oai:arxiv.org:1612.03770\",\"type\":\"OAI_ID\"},{\"identifier\":\"1612.03770\",\"type\":\"ARXIV_ID\"},{\"identifier\":\"278708033\",\"type\":\"CORE_ID\"}],\"title\":\"Neurogenesis Deep Learning\",\"language\":{\"code\":\"en\",\"name\":\"English\"},\"magId\":null,\"oaiIds\":[\"oai:arxiv.org:1612.03770\"],\"publishedDate\":\"2017-03-28T01:00:00\",\"publisher\":\"\\u0027Institute of Electrical and Electronics Engineers (IEEE)\\u0027\",\"pubmedId\":null,\"references\":[],\"sourceFulltextUrls\":[\"http:\\/\\/arxiv.org\\/abs\\/1612.03770\"],\"updatedDate\":\"2021-08-05T15:57:13\",\"yearPublished\":2017,\"journals\":[],\"links\":[{\"type\":\"download\",\"url\":\"http:\\/\\/arxiv.org\\/abs\\/1612.03770\"},{\"type\":\"display\",\"url\":\"https:\\/\\/core.ac.uk\\/works\\/37687589\"}]}],\"searchId\":\"89b4455da1412d56762aea7a673d8095\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "CORE_API_KEY = \"ZLQojgG1uJDYRdprWS8UhEzsIPM03cNi\"  # 🔁 Replace with your actual key\n",
    "\n",
    "query = \"deep learning\"\n",
    "\n",
    "url = \"https://api.core.ac.uk/v3/search/works\"\n",
    "\n",
    "params = {\n",
    "    \"q\": query,             # ✅ Required search query\n",
    "    \"page\": 1,\n",
    "    \"pageSize\": 5,\n",
    "    \"metadata\": True,\n",
    "    \"fulltext\": True,\n",
    "    \"apiKey\": CORE_API_KEY,\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "print(\"Status code:\", response.status_code)\n",
    "print(\"Response text:\", response.text[:])  # Show only first 500 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a012b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_paper.py\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "CORE_API_KEY = \"ZLQojgG1uJDYRdprWS8UhEzsIPM03cNi\"  # 🔁 Replace this with your actual CORE API key\n",
    "\n",
    "def fetch_core_papers(query, max_results=10):\n",
    "    url = \"https://api.core.ac.uk/v3/search/works\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"page\": 1,\n",
    "        \"pageSize\": max_results,\n",
    "        \"metadata\": True,\n",
    "        \"fulltext\": True,\n",
    "        \"apiKey\": CORE_API_KEY,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, params=params)\n",
    "        res.raise_for_status()\n",
    "        data = res.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "    records = []\n",
    "    for paper in data.get(\"data\", []):\n",
    "        records.append({\n",
    "            \"title\": paper.get(\"title\", \"\"),\n",
    "            \"authors\": [author.get(\"name\", \"\") for author in paper.get(\"authors\", [])] if paper.get(\"authors\") else [],\n",
    "            \"full_text\": paper.get(\"fullText\", \"\") or paper.get(\"description\", \"\"),\n",
    "            \"year\": paper.get(\"publishedDate\", \"\")[:4] if paper.get(\"publishedDate\") else \"\",\n",
    "            \"doi\": paper.get(\"doi\", \"\"),\n",
    "            \"url\": paper.get(\"downloadUrl\") or paper.get(\"identifier\", \"\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def save_to_csv(df, filename):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save.\n",
    "        filename (str): Output CSV file path.\n",
    "    \"\"\"\n",
    "    df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df806e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(fetch_core_papers('Natural Langage Processing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5cb76d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "mistral is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/mistral/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1115\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1643\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1639\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[0;32m   1640\u001b[0m ):\n\u001b[0;32m   1641\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m   1642\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1645\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1531\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1531\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1448\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[1;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m     )\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-682df345-68214af677ed56da317b093b;a5d5123f-abc2-401a-821c-cf320c7927fb)\n\nRepository Not Found for url: https://huggingface.co/mistral/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderModel, BertTokenizer, GPT2Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load hybrid model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderDecoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_encoder_decoder_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistral\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50256\u001b[39m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50256\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:514\u001b[0m, in \u001b[0;36mEncoderDecoderModel.from_encoder_decoder_pretrained\u001b[1;34m(cls, encoder_pretrained_model_name_or_path, decoder_pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `decoder_model` is not defined as an argument, a `decoder_pretrained_model_name_or_path` has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m     )\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs_decoder:\n\u001b[1;32m--> 514\u001b[0m     decoder_config, kwargs_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_pretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_config\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m decoder_config\u001b[38;5;241m.\u001b[39madd_cross_attention \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    520\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_pretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as a decoder model. Cross attention\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    521\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m layers are added to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_pretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and randomly initialized if\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_pretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms architecture allows for cross attention layers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    523\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1114\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1111\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1112\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1114\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1115\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1116\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:590\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    588\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 590\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:649\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    645\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 649\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\aravi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:456\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    457\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: mistral is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel, BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "# Load hybrid model\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"mistral\")\n",
    "model.config.decoder_start_token_id = model.config.pad_token_id = 50256\n",
    "model.config.eos_token_id = 50256\n",
    "\n",
    "# Load tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7386ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Tokenization function\n",
    "def encode(example):\n",
    "    input_enc = bert_tokenizer(example[\"input\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    target_enc = gpt2_tokenizer(example[\"target\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    labels = target_enc.input_ids.clone()\n",
    "    labels[labels == gpt2_tokenizer.pad_token_id] = -100  # ignore pad tokens in loss\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_enc.input_ids[0],\n",
    "        \"attention_mask\": input_enc.attention_mask[0],\n",
    "        \"decoder_input_ids\": target_enc.input_ids[0],\n",
    "        \"labels\": labels[0]\n",
    "    }\n",
    "\n",
    "class BERTGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [encode(d) for d in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = BERTGPTDataset(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ccc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 — Avg Loss: 3.5536\n",
      "✅ Epoch 2 — Avg Loss: 0.1720\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"✅ Epoch {epoch+1} — Avg Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generated Summary:\n",
      " This is a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_input = build_input(df.iloc[0])\n",
    "\n",
    "encoded = bert_tokenizer(test_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=encoded.input_ids,\n",
    "    attention_mask=encoded.attention_mask,\n",
    "    max_length=150,\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "summary = gpt2_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"\\n🧠 Generated Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdeaac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "mistral-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_raise_for_status\u001b[39m(response: Response, endpoint_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Internal version of `response.raise_for_status()` that will refine a\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    This helper is meant to be the unique method to raise_for_status when making a call\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    to the Hugging Face Hub.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m        import requests\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m        from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m        response = get_session().post(...)\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m        try:\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m            hf_raise_for_status(response)\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        except HfHubHTTPError as e:\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m            print(str(e)) # formatted message\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m            e.request_id, e.server_message # details returned by server\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m            # Complete the error message with additional information once it's raised\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m            e.append_to_message(\"\\n`create_commit` expects the repository to exist.\")\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m            raise\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m        response (`Response`):\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m            Response from the server.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m        endpoint_name (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m            Name of the endpoint that has been called. If provided, the error message\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m            will be more complete.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    <Tip warning={true}>\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Raises when the request has failed:\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m        - [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m            doesn't exist, because `repo_type` is not set correctly, or because the repo\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m            is `private` and you do not have access.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m        - [`~utils.GatedRepoError`]\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m            If the repository exists but is gated and the user is not on the authorized\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m            list.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        - [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m            If the repository exists but the revision couldn't be find.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m        - [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m            If the repository exists but the entry (e.g. the requested file) couldn't be\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m            find.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m        - [`~utils.BadRequestError`]\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m            If request failed with a HTTP 400 BadRequest error.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m        - [`~utils.HfHubHTTPError`]\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m            If request failed for a reason not listed above.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \n\u001b[1;32m--> 406\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistral-7b/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(cache_dir)\n\u001b[1;32m--> 398\u001b[0m existing_files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    399\u001b[0m file_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_hub_download\u001b[39m(\n\u001b[0;32m    811\u001b[0m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m     local_dir_use_symlinks: Union[\u001b[38;5;28mbool\u001b[39m, Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    832\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download a given file if it's not already present in the local cache.\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \n\u001b[0;32m    835\u001b[0m \u001b[38;5;124;03m    The new cache file layout looks like this:\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;124;03m    - The cache directory contains one subfolder per repo_id (namespaced by repo type)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;124;03m    - inside each repo folder:\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m        - refs is a list of the latest known revision => commit_hash pairs\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m        - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m          whether they're LFS files or not)\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;124;03m        - snapshots contains one subfolder per commit, each \"commit\" contains the subset of the files\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;124;03m          that have been resolved at that particular commit. Each filename is a symlink to the blob\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03m          at that particular commit.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03m    [  96]  .\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03m    └── [ 160]  models--julien-c--EsperBERTo-small\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03m        ├── [ 160]  blobs\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03m        │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;124;03m        │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;124;03m        │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;124;03m        ├── [  96]  refs\u001b[39;00m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;124;03m        │   └── [  40]  main\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;124;03m        └── [ 128]  snapshots\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;124;03m            ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03m            │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;124;03m            │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;124;03m            └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m                ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m                └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m \n\u001b[0;32m    863\u001b[0m \u001b[38;5;124;03m    If `local_dir` is provided, the file structure from the repo will be replicated in this location. When using this\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m    option, the `cache_dir` will not be used and a `.cache/huggingface/` folder will be created at the root of `local_dir`\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m    to store some metadata related to the downloaded files. While this mechanism is not as robust as the main\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m    cache-system, it's optimized for regularly pulling the latest version of a repository.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;124;03m        repo_id (`str`):\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m            A user or an organization name and a repo name separated by a `/`.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m        filename (`str`):\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m            The name of the file in the repo.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m        subfolder (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;124;03m            An optional value corresponding to a folder inside the model repo.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;124;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m            Set to `\"dataset\"` or `\"space\"` if downloading from a dataset or space,\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m            `None` or `\"model\"` if downloading from a model. Default is `None`.\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m            An optional Git revision id which can be a branch name, a tag, or a\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m            commit hash.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03m        library_name (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m            The name of the library to which the object corresponds.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;124;03m        library_version (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03m            The version of the library.\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m        cache_dir (`str`, `Path`, *optional*):\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m            Path to the folder where cached files are stored.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m        local_dir (`str` or `Path`, *optional*):\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m            If provided, the downloaded file will be placed under this directory.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m        user_agent (`dict`, `str`, *optional*):\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m            The user-agent info in the form of a dictionary or a string.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m            Whether the file should be downloaded even if it already exists in\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m            the local cache.\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m        proxies (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;124;03m            Dictionary mapping protocol to the URL of the proxy passed to\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;124;03m            `requests.request`.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;124;03m        etag_timeout (`float`, *optional*, defaults to `10`):\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03m            When fetching ETag, how many seconds to wait for the server to send\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m            data before giving up which is passed to `requests.request`.\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03m        token (`str`, `bool`, *optional*):\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;124;03m            A token to be used for the download.\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m                - If `True`, the token is read from the HuggingFace config\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m                  folder.\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m                - If a string, it's used as the authentication token.\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;124;03m        local_files_only (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m            If `True`, avoid downloading the file and return the path to the\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;124;03m            local cached file if it exists.\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m        headers (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m            Additional headers to be sent with the request.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m        `str`: Local path of file or if networking is off, last version of file cached on disk.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m        [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it doesn't exist,\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m            or because it is set to `private` and you do not have access.\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m        [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m            If the revision to download from cannot be found.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m        [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;124;03m            If the file to download cannot be found.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;124;03m        [`~utils.LocalEntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;124;03m            If network is disabled or unavailable and file is not found in cache.\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;124;03m        [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;124;03m            If `token=True` but the token cannot be found.\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;124;03m        [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m            If ETag cannot be determined.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m        [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m            If some parameter value is invalid.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_ETAG_TIMEOUT \u001b[38;5;241m!=\u001b[39m constants\u001b[38;5;241m.\u001b[39mDEFAULT_ETAG_TIMEOUT:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;66;03m# Respect environment variable above user value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HfFileMetadata(\n\u001b[0;32m   1461\u001b[0m         commit_hash\u001b[38;5;241m=\u001b[39mr\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(constants\u001b[38;5;241m.\u001b[39mHUGGINGFACE_HEADER_X_REPO_COMMIT),\n\u001b[0;32m   1462\u001b[0m         \u001b[38;5;66;03m# We favor a custom header indicating the etag of the linked resource, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mparse_xet_file_data_from_response(r),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m     )\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_metadata_or_catch_error\u001b[39m(\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1478\u001b[0m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1479\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1480\u001b[0m     repo_type: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1481\u001b[0m     revision: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1482\u001b[0m     endpoint: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   1483\u001b[0m     proxies: Optional[Dict],\n\u001b[1;32m-> 1484\u001b[0m     etag_timeout: Optional[\u001b[38;5;28mfloat\u001b[39m],\n\u001b[0;32m   1485\u001b[0m     headers: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],  \u001b[38;5;66;03m# mutated inplace!\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m     token: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m   1487\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   1488\u001b[0m     relative_filename: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# only used to store `.no_exists` in cache\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m     storage_folder: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# only used to store `.no_exists` in cache\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;66;03m# Either an exception is caught and returned\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m     Tuple[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m],\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;66;03m# Or the metadata is returned as\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# `(url_to_download, etag, commit_hash, expected_size, xet_file_data, None)`\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m     Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, Optional[XetFileData], \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m   1496\u001b[0m ]:\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get metadata for a file on the Hub, safely handling network issues.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \n\u001b[0;32m   1499\u001b[0m \u001b[38;5;124;03m    Returns either the etag, commit_hash and expected size of the file, or the error\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;124;03m          domain of the location (typically an S3 bucket).\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(refs_dir):\n\u001b[1;32m-> 1376\u001b[0m     revision_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(refs_dir, revision)\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(revision_file):\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     paths\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;241m.\u001b[39munlink(missing_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# delete outdated file first\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1289\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mincomplete_path(etag),\n\u001b[0;32m   1290\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mfile_path,\n\u001b[0;32m   1291\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1292\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1293\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1294\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1295\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m-> 1296\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1297\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1298\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[0;32m   1301\u001b[0m write_download_metadata(local_dir\u001b[38;5;241m=\u001b[39mlocal_dir, filename\u001b[38;5;241m=\u001b[39mfilename, commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash, etag\u001b[38;5;241m=\u001b[39metag)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_wrapper\u001b[39m(\n\u001b[0;32m    265\u001b[0m     method: HTTP_METHOD_T, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, follow_relative_redirects: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around requests methods to follow relative redirects if `follow_relative_redirects=True` even when\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m    `allow_redirection=False`.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m    A backoff mechanism retries the HTTP call on 429, 503 and 504 errors.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        method (`str`):\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m            HTTP method, such as 'GET' or 'HEAD'.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        url (`str`):\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m            The URL of the resource to fetch.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;124;03m        follow_relative_redirects (`bool`, *optional*, defaults to `False`)\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m            If True, relative redirection (redirection to the same site) will be resolved even when `allow_redirection`\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m            kwarg is set to False. Useful when we want to follow a redirection to a renamed repository without\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m            following redirection to a CDN.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m        **params (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m            Params to pass to `requests.request`.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    296\u001b[0m parsed_target = urlparse(response.headers[\"Location\"])\n\u001b[0;32m    297\u001b[0m if parsed_target.netloc == \"\":\n\u001b[0;32m    298\u001b[0m     # This means it is a relative 'location' headers, as allowed by RFC 7231.\n\u001b[0;32m    299\u001b[0m     # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')\n\u001b[0;32m    300\u001b[0m     # We want to follow this relative redirect !\n\u001b[1;32m--> 301\u001b[0m     #\n\u001b[0;32m    302\u001b[0m     # Highly inspired by `resolve_redirects` from requests library.\n\u001b[0;32m    303\u001b[0m     # See https://github.com/psf/requests/blob/main/requests/sessions.py#L159\n\u001b[0;32m    304\u001b[0m     next_url = urlparse(url)._replace(path=parsed_target.path).geturl()\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepoNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    439\u001b[0m     response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m error_message \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid credentials in Authorization header\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[0;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepository Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 454\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease make sure you specified the correct `repo_id` and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `repo_type`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to access a private or gated repo,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated. For more details, see\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6824c8c0-595703496a05777051014bf1;20c7c551-95de-454e-af4c-65576dcbc3a6)\n\nRepository Not Found for url: https://huggingface.co/mistral-7b/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m clusters_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustered_papers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load Mistral model and tokenizer using Auto classes\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m mistral_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmistral-7b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m mistral_model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral-7b\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Function to generate summary with Mistral\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:779\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    777\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    778\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m--> 779\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    780\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[0;32m    781\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    782\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    783\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    784\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    785\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    786\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    787\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    788\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    789\u001b[0m     _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    790\u001b[0m     _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    791\u001b[0m     _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    792\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    793\u001b[0m )\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:612\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     TOKENIZER_MAPPING_NAMES: OrderedDict[\u001b[38;5;28mstr\u001b[39m, Tuple[Optional[\u001b[38;5;28mstr\u001b[39m], Optional[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     TOKENIZER_MAPPING_NAMES \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[0;32m     62\u001b[0m         [\n\u001b[0;32m     63\u001b[0m             (\n\u001b[0;32m     64\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     65\u001b[0m                 (\n\u001b[0;32m     66\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     67\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m                 ),\n\u001b[0;32m     69\u001b[0m             ),\n\u001b[0;32m     70\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malign\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     71\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maria\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     72\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maya_vision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohereTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     73\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbark\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     74\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbart\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBartTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBartTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     75\u001b[0m             (\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarthez\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m                 (\n\u001b[0;32m     78\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarthezTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     79\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarthezTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m                 ),\n\u001b[0;32m     81\u001b[0m             ),\n\u001b[0;32m     82\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbartpho\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBartphoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     83\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     84\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertGenerationTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     85\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-japanese\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertJapaneseTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     86\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertweet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertweetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     87\u001b[0m             (\n\u001b[0;32m     88\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbig_bird\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m                 (\n\u001b[0;32m     90\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigBirdTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigBirdTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m                 ),\n\u001b[0;32m     93\u001b[0m             ),\n\u001b[0;32m     94\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigbird_pegasus\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     95\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiogpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBioGptTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     96\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblenderbot\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlenderbotTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlenderbotTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     97\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblenderbot-small\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlenderbotSmallTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     98\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     99\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblip-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    100\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbloom\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBloomTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    101\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbridgetower\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    102\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbros\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    103\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbyt5\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mByT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    104\u001b[0m             (\n\u001b[0;32m    105\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m                 (\n\u001b[0;32m    107\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCamembertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    108\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCamembertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m                 ),\n\u001b[0;32m    110\u001b[0m             ),\n\u001b[0;32m    111\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcanine\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCanineTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    112\u001b[0m             (\n\u001b[0;32m    113\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchameleon\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    114\u001b[0m                 (\n\u001b[0;32m    115\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    116\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    117\u001b[0m                 ),\n\u001b[0;32m    118\u001b[0m             ),\n\u001b[0;32m    119\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchinese_clip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    120\u001b[0m             (\n\u001b[0;32m    121\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    122\u001b[0m                 (\n\u001b[0;32m    123\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m                 ),\n\u001b[0;32m    126\u001b[0m             ),\n\u001b[0;32m    127\u001b[0m             (\n\u001b[0;32m    128\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m                 (\n\u001b[0;32m    130\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    132\u001b[0m                 ),\n\u001b[0;32m    133\u001b[0m             ),\n\u001b[0;32m    134\u001b[0m             (\n\u001b[0;32m    135\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclipseg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    136\u001b[0m                 (\n\u001b[0;32m    137\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    138\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    139\u001b[0m                 ),\n\u001b[0;32m    140\u001b[0m             ),\n\u001b[0;32m    141\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclvp\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClvpTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    142\u001b[0m             (\n\u001b[0;32m    143\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_llama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    144\u001b[0m                 (\n\u001b[0;32m    145\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m                 ),\n\u001b[0;32m    148\u001b[0m             ),\n\u001b[0;32m    149\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodegen\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeGenTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeGenTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    150\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohere\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohereTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    151\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohere2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohereTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    152\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolpali\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    153\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    154\u001b[0m             (\n\u001b[0;32m    155\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    156\u001b[0m                 (\n\u001b[0;32m    157\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCpmTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCpmTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    159\u001b[0m                 ),\n\u001b[0;32m    160\u001b[0m             ),\n\u001b[0;32m    161\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpmant\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCpmAntTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    162\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctrl\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCTRLTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    163\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata2vec-audio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    164\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata2vec-text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    165\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbrx\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    166\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeberta\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    167\u001b[0m             (\n\u001b[0;32m    168\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeberta-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    169\u001b[0m                 (\n\u001b[0;32m    170\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebertaV2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    171\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebertaV2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    172\u001b[0m                 ),\n\u001b[0;32m    173\u001b[0m             ),\n\u001b[0;32m    174\u001b[0m             (\n\u001b[0;32m    175\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek_v3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    176\u001b[0m                 (\n\u001b[0;32m    177\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    178\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    179\u001b[0m                 ),\n\u001b[0;32m    180\u001b[0m             ),\n\u001b[0;32m    181\u001b[0m             (\n\u001b[0;32m    182\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffllama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    183\u001b[0m                 (\n\u001b[0;32m    184\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    185\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    186\u001b[0m                 ),\n\u001b[0;32m    187\u001b[0m             ),\n\u001b[0;32m    188\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistilBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistilBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    189\u001b[0m             (\n\u001b[0;32m    190\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    191\u001b[0m                 (\n\u001b[0;32m    192\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDPRQuestionEncoderTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    193\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDPRQuestionEncoderTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    194\u001b[0m                 ),\n\u001b[0;32m    195\u001b[0m             ),\n\u001b[0;32m    196\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melectra\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElectraTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElectraTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    197\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memu3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    198\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mernie\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    199\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mernie_m\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErnieMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    200\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEsmTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    201\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalcon\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    202\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalcon_mamba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    203\u001b[0m             (\n\u001b[0;32m    204\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastspeech2_conformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    205\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastSpeech2ConformerTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_g2p_en_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    206\u001b[0m             ),\n\u001b[0;32m    207\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflaubert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlaubertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    208\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFNetTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    209\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfsmt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFSMTTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    210\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunnel\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunnelTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunnelTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    211\u001b[0m             (\n\u001b[0;32m    212\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    213\u001b[0m                 (\n\u001b[0;32m    214\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m                 ),\n\u001b[0;32m    217\u001b[0m             ),\n\u001b[0;32m    218\u001b[0m             (\n\u001b[0;32m    219\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m                 (\n\u001b[0;32m    221\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    222\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    223\u001b[0m                 ),\n\u001b[0;32m    224\u001b[0m             ),\n\u001b[0;32m    225\u001b[0m             (\n\u001b[0;32m    226\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m                 (\n\u001b[0;32m    228\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    229\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m                 ),\n\u001b[0;32m    231\u001b[0m             ),\n\u001b[0;32m    232\u001b[0m             (\n\u001b[0;32m    233\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma3_text\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    234\u001b[0m                 (\n\u001b[0;32m    235\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    236\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    237\u001b[0m                 ),\n\u001b[0;32m    238\u001b[0m             ),\n\u001b[0;32m    239\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    240\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    241\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglm4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    242\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-sw3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTSw3Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    243\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    244\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_bigcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    245\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_neo\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    246\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_neox\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    247\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_neox_japanese\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXJapaneseTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    248\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptj\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    249\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptsan-japanese\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTSanJapaneseTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    250\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrounding-dino\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    251\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupvit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    252\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelium\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    253\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mherbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHerbertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHerbertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    254\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhubert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    255\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    256\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midefics\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    257\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midefics2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    258\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midefics3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    259\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructblip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    260\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructblipvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    261\u001b[0m             (\n\u001b[0;32m    262\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjamba\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    263\u001b[0m                 (\n\u001b[0;32m    264\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    265\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m                 ),\n\u001b[0;32m    267\u001b[0m             ),\n\u001b[0;32m    268\u001b[0m             (\n\u001b[0;32m    269\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjetmoe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    270\u001b[0m                 (\n\u001b[0;32m    271\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    272\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m                 ),\n\u001b[0;32m    274\u001b[0m             ),\n\u001b[0;32m    275\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjukebox\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJukeboxTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    276\u001b[0m             (\n\u001b[0;32m    277\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkosmos-2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    278\u001b[0m                 (\n\u001b[0;32m    279\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    280\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    281\u001b[0m                 ),\n\u001b[0;32m    282\u001b[0m             ),\n\u001b[0;32m    283\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayoutlm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    284\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayoutlmv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    285\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayoutlmv3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv3Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv3TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    286\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayoutxlm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutXLMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutXLMTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    287\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mled\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEDTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEDTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    288\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlilt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv3Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv3TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    289\u001b[0m             (\n\u001b[0;32m    290\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    291\u001b[0m                 (\n\u001b[0;32m    292\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    293\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    294\u001b[0m                 ),\n\u001b[0;32m    295\u001b[0m             ),\n\u001b[0;32m    296\u001b[0m             (\n\u001b[0;32m    297\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    298\u001b[0m                 (\n\u001b[0;32m    299\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    300\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    301\u001b[0m                 ),\n\u001b[0;32m    302\u001b[0m             ),\n\u001b[0;32m    303\u001b[0m             (\n\u001b[0;32m    304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama4_text\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    305\u001b[0m                 (\n\u001b[0;32m    306\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m                 ),\n\u001b[0;32m    309\u001b[0m             ),\n\u001b[0;32m    310\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    311\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava_next\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    312\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava_next_video\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    313\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava_onevision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    314\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongformerTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongformerTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    315\u001b[0m             (\n\u001b[0;32m    316\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongt5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    317\u001b[0m                 (\n\u001b[0;32m    318\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    319\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m                 ),\n\u001b[0;32m    321\u001b[0m             ),\n\u001b[0;32m    322\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mluke\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLukeTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    323\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxmert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLxmertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLxmertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    324\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm2m_100\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM2M100Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    325\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    326\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    327\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarian\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarianTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    328\u001b[0m             (\n\u001b[0;32m    329\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmbart\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    330\u001b[0m                 (\n\u001b[0;32m    331\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMBartTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    332\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMBartTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    333\u001b[0m                 ),\n\u001b[0;32m    334\u001b[0m             ),\n\u001b[0;32m    335\u001b[0m             (\n\u001b[0;32m    336\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmbart50\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    337\u001b[0m                 (\n\u001b[0;32m    338\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMBart50Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    339\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMBart50TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    340\u001b[0m                 ),\n\u001b[0;32m    341\u001b[0m             ),\n\u001b[0;32m    342\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmega\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    343\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmegatron-bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    344\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmgp-str\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMgpstrTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    345\u001b[0m             (\n\u001b[0;32m    346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    347\u001b[0m                 (\n\u001b[0;32m    348\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    350\u001b[0m                 ),\n\u001b[0;32m    351\u001b[0m             ),\n\u001b[0;32m    352\u001b[0m             (\n\u001b[0;32m    353\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixtral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    354\u001b[0m                 (\n\u001b[0;32m    355\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    356\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    357\u001b[0m                 ),\n\u001b[0;32m    358\u001b[0m             ),\n\u001b[0;32m    359\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmllama\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    360\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmluke\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLukeTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    361\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmobilebert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMobileBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMobileBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    362\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodernbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    363\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoonshine\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    364\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoshi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    365\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMPNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMPNetTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    366\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    367\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmra\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    368\u001b[0m             (\n\u001b[0;32m    369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmt5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    370\u001b[0m                 (\n\u001b[0;32m    371\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    372\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    373\u001b[0m                 ),\n\u001b[0;32m    374\u001b[0m             ),\n\u001b[0;32m    375\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusicgen\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    376\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusicgen_melody\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    377\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmvp\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMvpTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMvpTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    378\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmyt5\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMyT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    379\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnemotron\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    380\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnezha\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    381\u001b[0m             (\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnllb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    383\u001b[0m                 (\n\u001b[0;32m    384\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNllbTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    385\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNllbTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    386\u001b[0m                 ),\n\u001b[0;32m    387\u001b[0m             ),\n\u001b[0;32m    388\u001b[0m             (\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnllb-moe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    390\u001b[0m                 (\n\u001b[0;32m    391\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNllbTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    392\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNllbTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    393\u001b[0m                 ),\n\u001b[0;32m    394\u001b[0m             ),\n\u001b[0;32m    395\u001b[0m             (\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnystromformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m                 (\n\u001b[0;32m    398\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    399\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m                 ),\n\u001b[0;32m    401\u001b[0m             ),\n\u001b[0;32m    402\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molmo\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    403\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molmo2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    404\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molmoe\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    405\u001b[0m             (\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momdet-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    407\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    408\u001b[0m             ),\n\u001b[0;32m    409\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    410\u001b[0m             (\n\u001b[0;32m    411\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-gpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    412\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIGPTTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIGPTTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    413\u001b[0m             ),\n\u001b[0;32m    414\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    415\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowlv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    416\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowlvit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    417\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaligemma\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    418\u001b[0m             (\n\u001b[0;32m    419\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    420\u001b[0m                 (\n\u001b[0;32m    421\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    423\u001b[0m                 ),\n\u001b[0;32m    424\u001b[0m             ),\n\u001b[0;32m    425\u001b[0m             (\n\u001b[0;32m    426\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus_x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    427\u001b[0m                 (\n\u001b[0;32m    428\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    429\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    430\u001b[0m                 ),\n\u001b[0;32m    431\u001b[0m             ),\n\u001b[0;32m    432\u001b[0m             (\n\u001b[0;32m    433\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperceiver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    434\u001b[0m                 (\n\u001b[0;32m    435\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerceiverTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    436\u001b[0m                     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    437\u001b[0m                 ),\n\u001b[0;32m    438\u001b[0m             ),\n\u001b[0;32m    439\u001b[0m             (\n\u001b[0;32m    440\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersimmon\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    441\u001b[0m                 (\n\u001b[0;32m    442\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    443\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    444\u001b[0m                 ),\n\u001b[0;32m    445\u001b[0m             ),\n\u001b[0;32m    446\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeGenTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeGenTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    447\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    448\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphimoe\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    449\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphobert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhobertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    450\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpix2struct\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    451\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixtral\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    452\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplbart\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLBartTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    453\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProphetNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    454\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqdqbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    455\u001b[0m             (\n\u001b[0;32m    456\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    457\u001b[0m                 (\n\u001b[0;32m    458\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    459\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    460\u001b[0m                 ),\n\u001b[0;32m    461\u001b[0m             ),\n\u001b[0;32m    462\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_5_vl\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    463\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    464\u001b[0m             (\n\u001b[0;32m    465\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_moe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    466\u001b[0m                 (\n\u001b[0;32m    467\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    468\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    469\u001b[0m                 ),\n\u001b[0;32m    470\u001b[0m             ),\n\u001b[0;32m    471\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_vl\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    472\u001b[0m             (\n\u001b[0;32m    473\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    474\u001b[0m                 (\n\u001b[0;32m    475\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    476\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    477\u001b[0m                 ),\n\u001b[0;32m    478\u001b[0m             ),\n\u001b[0;32m    479\u001b[0m             (\n\u001b[0;32m    480\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen3_moe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    481\u001b[0m                 (\n\u001b[0;32m    482\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    483\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    484\u001b[0m                 ),\n\u001b[0;32m    485\u001b[0m             ),\n\u001b[0;32m    486\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRagTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    487\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrealm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRealmTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRealmTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    488\u001b[0m             (\n\u001b[0;32m    489\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecurrent_gemma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    490\u001b[0m                 (\n\u001b[0;32m    491\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m                 ),\n\u001b[0;32m    494\u001b[0m             ),\n\u001b[0;32m    495\u001b[0m             (\n\u001b[0;32m    496\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    497\u001b[0m                 (\n\u001b[0;32m    498\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReformerTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    499\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReformerTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    500\u001b[0m                 ),\n\u001b[0;32m    501\u001b[0m             ),\n\u001b[0;32m    502\u001b[0m             (\n\u001b[0;32m    503\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrembert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    504\u001b[0m                 (\n\u001b[0;32m    505\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    506\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    507\u001b[0m                 ),\n\u001b[0;32m    508\u001b[0m             ),\n\u001b[0;32m    509\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretribert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetriBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetriBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    510\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    511\u001b[0m             (\n\u001b[0;32m    512\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-prelayernorm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    513\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    514\u001b[0m             ),\n\u001b[0;32m    515\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoCBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    516\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoFormerTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoFormerTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    517\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrwkv\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    518\u001b[0m             (\n\u001b[0;32m    519\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseamless_m4t\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    520\u001b[0m                 (\n\u001b[0;32m    521\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeamlessM4TTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    522\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeamlessM4TTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    523\u001b[0m                 ),\n\u001b[0;32m    524\u001b[0m             ),\n\u001b[0;32m    525\u001b[0m             (\n\u001b[0;32m    526\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseamless_m4t_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    527\u001b[0m                 (\n\u001b[0;32m    528\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeamlessM4TTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    529\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeamlessM4TTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    530\u001b[0m                 ),\n\u001b[0;32m    531\u001b[0m             ),\n\u001b[0;32m    532\u001b[0m             (\n\u001b[0;32m    533\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshieldgemma2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    534\u001b[0m                 (\n\u001b[0;32m    535\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    536\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    537\u001b[0m                 ),\n\u001b[0;32m    538\u001b[0m             ),\n\u001b[0;32m    539\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiglip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSiglipTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    540\u001b[0m             (\n\u001b[0;32m    541\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiglip2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    542\u001b[0m                 (\n\u001b[0;32m    543\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    544\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    545\u001b[0m                 ),\n\u001b[0;32m    546\u001b[0m             ),\n\u001b[0;32m    547\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_to_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeech2TextTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    548\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_to_text_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeech2Text2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    549\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeecht5\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeechT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    550\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplinter\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplinterTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplinterTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m    551\u001b[0m             (\n\u001b[0;32m    552\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqueezebert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    553\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSqueezeBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSqueezeBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    554\u001b[0m             ),\n\u001b[0;32m    555\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstablelm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    556\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarcoder2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    557\u001b[0m             (\n\u001b[0;32m    558\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswitch_transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    559\u001b[0m                 (\n\u001b[0;32m    560\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    561\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    562\u001b[0m                 ),\n\u001b[0;32m    563\u001b[0m             ),\n\u001b[0;32m    564\u001b[0m             (\n\u001b[0;32m    565\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    566\u001b[0m                 (\n\u001b[0;32m    567\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    568\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m                 ),\n\u001b[0;32m    570\u001b[0m             ),\n\u001b[0;32m    571\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtapas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTapasTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    572\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtapex\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTapexTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    573\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfo-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransfoXLTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    574\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtvp\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    575\u001b[0m             (\n\u001b[0;32m    576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mudop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    577\u001b[0m                 (\n\u001b[0;32m    578\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUdopTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUdopTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    580\u001b[0m                 ),\n\u001b[0;32m    581\u001b[0m             ),\n\u001b[0;32m    582\u001b[0m             (\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mumt5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    584\u001b[0m                 (\n\u001b[0;32m    585\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    586\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    587\u001b[0m                 ),\n\u001b[0;32m    588\u001b[0m             ),\n\u001b[0;32m    589\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_llava\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    590\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvilt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    591\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvipllava\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    592\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisual_bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    593\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvits\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVitsTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    594\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    595\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2-bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    596\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2-conformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    597\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2_phoneme\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2PhonemeCTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    598\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhisper\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisperTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisperTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    599\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    600\u001b[0m             (\n\u001b[0;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxglm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    602\u001b[0m                 (\n\u001b[0;32m    603\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGLMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    604\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGLMTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    605\u001b[0m                 ),\n\u001b[0;32m    606\u001b[0m             ),\n\u001b[0;32m    607\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    608\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-prophetnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMProphetNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    609\u001b[0m             (\n\u001b[0;32m    610\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    611\u001b[0m                 (\n\u001b[1;32m--> 612\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    613\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    614\u001b[0m                 ),\n\u001b[0;32m    615\u001b[0m             ),\n\u001b[0;32m    616\u001b[0m             (\n\u001b[0;32m    617\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    618\u001b[0m                 (\n\u001b[0;32m    619\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    620\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    621\u001b[0m                 ),\n\u001b[0;32m    622\u001b[0m             ),\n\u001b[0;32m    623\u001b[0m             (\n\u001b[0;32m    624\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    625\u001b[0m                 (\n\u001b[0;32m    626\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLNetTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    628\u001b[0m                 ),\n\u001b[0;32m    629\u001b[0m             ),\n\u001b[0;32m    630\u001b[0m             (\n\u001b[0;32m    631\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxmod\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    632\u001b[0m                 (\n\u001b[0;32m    633\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    634\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    635\u001b[0m                 ),\n\u001b[0;32m    636\u001b[0m             ),\n\u001b[0;32m    637\u001b[0m             (\n\u001b[0;32m    638\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoso\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    639\u001b[0m                 (\n\u001b[0;32m    640\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    641\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    642\u001b[0m                 ),\n\u001b[0;32m    643\u001b[0m             ),\n\u001b[0;32m    644\u001b[0m             (\n\u001b[0;32m    645\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzamba\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    646\u001b[0m                 (\n\u001b[0;32m    647\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m                 ),\n\u001b[0;32m    650\u001b[0m             ),\n\u001b[0;32m    651\u001b[0m             (\n\u001b[0;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzamba2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    653\u001b[0m                 (\n\u001b[0;32m    654\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    655\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    656\u001b[0m                 ),\n\u001b[0;32m    657\u001b[0m             ),\n\u001b[0;32m    658\u001b[0m         ]\n\u001b[0;32m    659\u001b[0m     )\n\u001b[0;32m    661\u001b[0m TOKENIZER_MAPPING \u001b[38;5;241m=\u001b[39m _LazyAutoMapping(CONFIG_MAPPING_NAMES, TOKENIZER_MAPPING_NAMES)\n\u001b[0;32m    663\u001b[0m CONFIG_TO_TYPE \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING_NAMES\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# download the files if needed\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m    424\u001b[0m         hf_hub_download(\n\u001b[0;32m    425\u001b[0m             path_or_repo_id,\n\u001b[0;32m    426\u001b[0m             filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    437\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: mistral-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "# Load the clusters CSV file (make sure to provide the correct file path)\n",
    "clusters_df = pd.read_csv('clustered_papers.csv')\n",
    "\n",
    "# Load Mistral model and tokenizer using Auto classes\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained('mistral-7b')\n",
    "mistral_model = AutoModelForSeq2SeqLM.from_pretrained('mistral-7b')\n",
    "\n",
    "# Function to generate summary with Mistral\n",
    "def generate_summary_with_mistral(text):\n",
    "    mistral_inputs = mistral_tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_output = mistral_model.generate(mistral_inputs['input_ids'], max_length=300, num_return_sequences=1)\n",
    "    summary = mistral_tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Loop through each cluster and generate summaries with bibliometric insights\n",
    "for cluster_id in clusters_df['cluster_id'].unique():\n",
    "    # Filter articles for the current cluster\n",
    "    cluster_articles = clusters_df[clusters_df['cluster_id'] == cluster_id]\n",
    "    \n",
    "    # Combine all abstracts from the current cluster for summarization\n",
    "    combined_abstracts = \" \".join(cluster_articles['abstract'].tolist())\n",
    "    \n",
    "    # Generate a summary for the combined abstracts\n",
    "    generated_summary = generate_summary_with_mistral(combined_abstracts)\n",
    "    \n",
    "    # Combine bibliometric metrics\n",
    "    avg_citation_count = cluster_articles['citation_count'].mean()\n",
    "    avg_h_index = cluster_articles['h_index'].mean()\n",
    "    \n",
    "    # Format output to make it more human-friendly\n",
    "    print(f\"Research Trends for Cluster {cluster_id}:\")\n",
    "    print(f\"------------------------------------------------\")\n",
    "    print(f\"**Summary of Research**: {generated_summary}\")\n",
    "    print(f\"\\n**Bibliometric Insights**:\")\n",
    "    print(f\"- Average Citation Count: {avg_citation_count:.2f}\")\n",
    "    print(f\"- Average H-index: {avg_h_index:.2f}\")\n",
    "    print(f\"- Total Number of Papers in Cluster: {len(cluster_articles)}\")\n",
    "    print(f\"- Most Cited Paper: {cluster_articles.loc[cluster_articles['citation_count'].idxmax()]['title']}\")\n",
    "    print(f\"- Top Author: {cluster_articles.groupby('author')['citation_count'].sum().idxmax()}\")\n",
    "    print(f\"------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01444c41",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-6824cb2c-71c6583710d6c0a9237dd1a4;0157bd78-bb45-49bb-b097-0c78997e0478)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_raise_for_status\u001b[39m(response: Response, endpoint_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Internal version of `response.raise_for_status()` that will refine a\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    This helper is meant to be the unique method to raise_for_status when making a call\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    to the Hugging Face Hub.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m        import requests\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m        from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m        response = get_session().post(...)\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m        try:\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m            hf_raise_for_status(response)\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        except HfHubHTTPError as e:\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m            print(str(e)) # formatted message\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m            e.request_id, e.server_message # details returned by server\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m            # Complete the error message with additional information once it's raised\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m            e.append_to_message(\"\\n`create_commit` expects the repository to exist.\")\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m            raise\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m        response (`Response`):\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m            Response from the server.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m        endpoint_name (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m            Name of the endpoint that has been called. If provided, the error message\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m            will be more complete.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    <Tip warning={true}>\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Raises when the request has failed:\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m        - [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m            doesn't exist, because `repo_type` is not set correctly, or because the repo\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m            is `private` and you do not have access.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m        - [`~utils.GatedRepoError`]\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m            If the repository exists but is gated and the user is not on the authorized\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m            list.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        - [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m            If the repository exists but the revision couldn't be find.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m        - [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m            If the repository exists but the entry (e.g. the requested file) couldn't be\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m            find.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m        - [`~utils.BadRequestError`]\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m            If request failed with a HTTP 400 BadRequest error.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m        - [`~utils.HfHubHTTPError`]\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m            If request failed for a reason not listed above.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \n\u001b[1;32m--> 406\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(cache_dir)\n\u001b[1;32m--> 398\u001b[0m existing_files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    399\u001b[0m file_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_hub_download\u001b[39m(\n\u001b[0;32m    811\u001b[0m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m     local_dir_use_symlinks: Union[\u001b[38;5;28mbool\u001b[39m, Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    832\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download a given file if it's not already present in the local cache.\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \n\u001b[0;32m    835\u001b[0m \u001b[38;5;124;03m    The new cache file layout looks like this:\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;124;03m    - The cache directory contains one subfolder per repo_id (namespaced by repo type)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;124;03m    - inside each repo folder:\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m        - refs is a list of the latest known revision => commit_hash pairs\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m        - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m          whether they're LFS files or not)\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;124;03m        - snapshots contains one subfolder per commit, each \"commit\" contains the subset of the files\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;124;03m          that have been resolved at that particular commit. Each filename is a symlink to the blob\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03m          at that particular commit.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03m    [  96]  .\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03m    └── [ 160]  models--julien-c--EsperBERTo-small\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03m        ├── [ 160]  blobs\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03m        │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;124;03m        │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;124;03m        │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;124;03m        ├── [  96]  refs\u001b[39;00m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;124;03m        │   └── [  40]  main\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;124;03m        └── [ 128]  snapshots\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;124;03m            ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03m            │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;124;03m            │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;124;03m            └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m                ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m                └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m \n\u001b[0;32m    863\u001b[0m \u001b[38;5;124;03m    If `local_dir` is provided, the file structure from the repo will be replicated in this location. When using this\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m    option, the `cache_dir` will not be used and a `.cache/huggingface/` folder will be created at the root of `local_dir`\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m    to store some metadata related to the downloaded files. While this mechanism is not as robust as the main\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m    cache-system, it's optimized for regularly pulling the latest version of a repository.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;124;03m        repo_id (`str`):\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m            A user or an organization name and a repo name separated by a `/`.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m        filename (`str`):\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m            The name of the file in the repo.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m        subfolder (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;124;03m            An optional value corresponding to a folder inside the model repo.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;124;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m            Set to `\"dataset\"` or `\"space\"` if downloading from a dataset or space,\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m            `None` or `\"model\"` if downloading from a model. Default is `None`.\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m            An optional Git revision id which can be a branch name, a tag, or a\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m            commit hash.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03m        library_name (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m            The name of the library to which the object corresponds.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;124;03m        library_version (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03m            The version of the library.\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m        cache_dir (`str`, `Path`, *optional*):\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m            Path to the folder where cached files are stored.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m        local_dir (`str` or `Path`, *optional*):\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m            If provided, the downloaded file will be placed under this directory.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m        user_agent (`dict`, `str`, *optional*):\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m            The user-agent info in the form of a dictionary or a string.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m            Whether the file should be downloaded even if it already exists in\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m            the local cache.\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m        proxies (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;124;03m            Dictionary mapping protocol to the URL of the proxy passed to\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;124;03m            `requests.request`.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;124;03m        etag_timeout (`float`, *optional*, defaults to `10`):\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03m            When fetching ETag, how many seconds to wait for the server to send\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m            data before giving up which is passed to `requests.request`.\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03m        token (`str`, `bool`, *optional*):\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;124;03m            A token to be used for the download.\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m                - If `True`, the token is read from the HuggingFace config\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m                  folder.\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m                - If a string, it's used as the authentication token.\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;124;03m        local_files_only (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m            If `True`, avoid downloading the file and return the path to the\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;124;03m            local cached file if it exists.\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m        headers (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m            Additional headers to be sent with the request.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m        `str`: Local path of file or if networking is off, last version of file cached on disk.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m        [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it doesn't exist,\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m            or because it is set to `private` and you do not have access.\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m        [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m            If the revision to download from cannot be found.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m        [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;124;03m            If the file to download cannot be found.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;124;03m        [`~utils.LocalEntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;124;03m            If network is disabled or unavailable and file is not found in cache.\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;124;03m        [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;124;03m            If `token=True` but the token cannot be found.\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;124;03m        [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m            If ETag cannot be determined.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m        [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m            If some parameter value is invalid.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_ETAG_TIMEOUT \u001b[38;5;241m!=\u001b[39m constants\u001b[38;5;241m.\u001b[39mDEFAULT_ETAG_TIMEOUT:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;66;03m# Respect environment variable above user value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HfFileMetadata(\n\u001b[0;32m   1461\u001b[0m         commit_hash\u001b[38;5;241m=\u001b[39mr\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(constants\u001b[38;5;241m.\u001b[39mHUGGINGFACE_HEADER_X_REPO_COMMIT),\n\u001b[0;32m   1462\u001b[0m         \u001b[38;5;66;03m# We favor a custom header indicating the etag of the linked resource, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mparse_xet_file_data_from_response(r),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m     )\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_metadata_or_catch_error\u001b[39m(\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1478\u001b[0m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1479\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1480\u001b[0m     repo_type: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1481\u001b[0m     revision: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1482\u001b[0m     endpoint: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   1483\u001b[0m     proxies: Optional[Dict],\n\u001b[1;32m-> 1484\u001b[0m     etag_timeout: Optional[\u001b[38;5;28mfloat\u001b[39m],\n\u001b[0;32m   1485\u001b[0m     headers: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],  \u001b[38;5;66;03m# mutated inplace!\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m     token: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m   1487\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   1488\u001b[0m     relative_filename: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# only used to store `.no_exists` in cache\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m     storage_folder: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# only used to store `.no_exists` in cache\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;66;03m# Either an exception is caught and returned\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m     Tuple[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m],\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;66;03m# Or the metadata is returned as\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# `(url_to_download, etag, commit_hash, expected_size, xet_file_data, None)`\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m     Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, Optional[XetFileData], \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m   1496\u001b[0m ]:\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get metadata for a file on the Hub, safely handling network issues.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \n\u001b[0;32m   1499\u001b[0m \u001b[38;5;124;03m    Returns either the etag, commit_hash and expected size of the file, or the error\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;124;03m          domain of the location (typically an S3 bucket).\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(refs_dir):\n\u001b[1;32m-> 1376\u001b[0m     revision_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(refs_dir, revision)\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(revision_file):\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     paths\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;241m.\u001b[39munlink(missing_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# delete outdated file first\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1289\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mincomplete_path(etag),\n\u001b[0;32m   1290\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mfile_path,\n\u001b[0;32m   1291\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1292\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1293\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1294\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1295\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m-> 1296\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1297\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1298\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[0;32m   1301\u001b[0m write_download_metadata(local_dir\u001b[38;5;241m=\u001b[39mlocal_dir, filename\u001b[38;5;241m=\u001b[39mfilename, commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash, etag\u001b[38;5;241m=\u001b[39metag)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_wrapper\u001b[39m(\n\u001b[0;32m    265\u001b[0m     method: HTTP_METHOD_T, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, follow_relative_redirects: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around requests methods to follow relative redirects if `follow_relative_redirects=True` even when\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m    `allow_redirection=False`.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m    A backoff mechanism retries the HTTP call on 429, 503 and 504 errors.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        method (`str`):\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m            HTTP method, such as 'GET' or 'HEAD'.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        url (`str`):\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m            The URL of the resource to fetch.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;124;03m        follow_relative_redirects (`bool`, *optional*, defaults to `False`)\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m            If True, relative redirection (redirection to the same site) will be resolved even when `allow_redirection`\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m            kwarg is set to False. Useful when we want to follow a redirection to a renamed repository without\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m            following redirection to a CDN.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m        **params (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m            Params to pass to `requests.request`.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    296\u001b[0m parsed_target = urlparse(response.headers[\"Location\"])\n\u001b[0;32m    297\u001b[0m if parsed_target.netloc == \"\":\n\u001b[0;32m    298\u001b[0m     # This means it is a relative 'location' headers, as allowed by RFC 7231.\n\u001b[0;32m    299\u001b[0m     # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')\n\u001b[0;32m    300\u001b[0m     # We want to follow this relative redirect !\n\u001b[1;32m--> 301\u001b[0m     #\n\u001b[0;32m    302\u001b[0m     # Highly inspired by `resolve_redirects` from requests library.\n\u001b[0;32m    303\u001b[0m     # See https://github.com/psf/requests/blob/main/requests/sessions.py#L159\n\u001b[0;32m    304\u001b[0m     next_url = urlparse(url)._replace(path=parsed_target.path).geturl()\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:423\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 423\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m     )\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-6824cb2c-71c6583710d6c0a9237dd1a4;0157bd78-bb45-49bb-b097-0c78997e0478)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load Mistral model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m mistral_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmistral_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      9\u001b[0m     mistral_id,\n\u001b[0;32m     10\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m     11\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load your clustered data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:794\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    778\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    779\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    780\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    792\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    793\u001b[0m )\n\u001b[1;32m--> 794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1138\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1135\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1137\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1139\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1140\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers from source with the command \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1142\u001b[0m         )\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:631\u001b[0m, in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    628\u001b[0m     )\n\u001b[0;32m    630\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_auto_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: from_auto_class}\n\u001b[1;32m--> 631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     user_agent[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m from_pipeline\n\u001b[0;32m    634\u001b[0m pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:686\u001b[0m, in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m         \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[0;32m    684\u001b[0m         config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n\u001b[1;32m--> 686\u001b[0m     config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m commit_hash\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like the config file at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid JSON file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:416\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inside \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_counter \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_filenames):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m existing_files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(existing_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    419\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n",
      "\u001b[1;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-6824cb2c-71c6583710d6c0a9237dd1a4;0157bd78-bb45-49bb-b097-0c78997e0478)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load Mistral model\n",
    "mistral_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load your clustered data\n",
    "df = pd.read_csv(\"clustered_papers.csv\")  # replace with your actual file\n",
    "\n",
    "# Assume you have a function or mapping that returns keywords for each cluster\n",
    "cluster_keywords = {\n",
    "    0: [\"crowdsourced data\", \"urban mobility\", \"transportation modeling\"],\n",
    "    1: [\"machine learning\", \"citation analysis\", \"bibliometric networks\"],\n",
    "    # Add all your cluster -> keyword mappings\n",
    "}\n",
    "\n",
    "# Generate and print summary for each cluster\n",
    "for cluster_id in sorted(df['cluster_id'].unique()):\n",
    "    cluster_df = df[df['cluster_id'] == cluster_id]\n",
    "    abstracts = cluster_df['abstract'].dropna().tolist()\n",
    "    if not abstracts:\n",
    "        continue\n",
    "\n",
    "    combined_text = \" \".join(abstracts)[:2000]  # Truncate if too long\n",
    "    keywords = cluster_keywords.get(cluster_id, [\"science\", \"research\"])\n",
    "\n",
    "    # Build instruct prompt\n",
    "    prompt = f\"<s>[INST] Generate a bibliometric summary using the topics: {', '.join(keywords)}. Text:\\n{combined_text}\\n[/INST]\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n📘 Cluster {cluster_id} Summary\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(summary)\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding: [-3.03823858e-01 -2.94588953e-01 -3.76790494e-01  5.01689222e-03\n",
      " -3.54408994e-02  1.06439918e-01 -3.23471457e-01  2.17532516e-01\n",
      "  2.92097963e-03 -7.44312882e-01 -4.35850620e-01 -3.50625992e-01\n",
      "  2.00050063e-02 -2.46841222e-01 -1.59674272e-01  5.49915060e-02\n",
      " -2.40113243e-01  6.04893386e-01  4.65398401e-01  5.87469265e-02\n",
      " -3.19653422e-01 -7.89481580e-01  1.41855806e-01  2.59826928e-01\n",
      " -1.24172002e-01 -3.95769417e-01 -7.99091160e-02  4.51406538e-01\n",
      " -7.91762024e-02  1.85417414e-01 -4.29062903e-01  3.09045583e-01\n",
      " -5.20451665e-01 -8.73078465e-01  7.41924167e-01 -7.56629258e-02\n",
      " -5.66660017e-02 -4.74565268e-01 -5.19070625e-01  4.38661039e-01\n",
      " -3.53880256e-01  3.36756796e-01  4.73318577e-01 -3.30739498e-01\n",
      " -2.30438918e-01  6.87498897e-02 -2.97834635e+00 -2.10847571e-01\n",
      " -8.75047624e-01 -1.28071606e-01  3.81216854e-01  4.08959866e-01\n",
      "  3.36281300e-01  1.29074126e-01  6.62514269e-01  7.79323637e-01\n",
      "  2.08158702e-01 -5.16489595e-02  3.43803972e-01  1.69889957e-01\n",
      "  5.51769316e-01  4.24994111e-01 -2.14304030e-01 -5.35979211e-01\n",
      "  1.90755606e-01 -6.43610209e-02  5.47206998e-02  4.96894479e-01\n",
      " -1.86606094e-01  3.67489755e-02 -5.50784171e-01  1.43669158e-01\n",
      "  5.18800020e-01 -8.66626352e-02 -2.19657600e-01 -3.38900946e-02\n",
      "  1.65021107e-01  4.66869235e-01 -8.07627559e-01 -1.64124861e-01\n",
      " -4.41753775e-01  8.41248453e-01  1.09165795e-01  1.17643267e-01\n",
      "  5.80080807e-01  7.92051256e-01 -9.70901430e-01 -4.78672832e-01\n",
      " -1.40501127e-01  4.45860445e-01 -2.10405946e-01 -3.04221839e-01\n",
      "  3.54654014e-01  1.11477005e+00  7.84012079e-01  3.58697236e-01\n",
      " -1.30415529e-01 -2.64885068e-01 -2.08544940e-01  6.15005851e-01\n",
      "  5.91294467e-01 -2.97800004e-01  2.65872836e-01 -3.22568297e-01\n",
      "  4.06827807e-01 -1.51383504e-02 -5.54092228e-01 -4.92251009e-01\n",
      " -1.23656601e-01 -2.69746614e+00  5.47028840e-01 -1.67653672e-02\n",
      " -2.85866261e-01 -1.77994788e-01  1.48758769e-01  4.89803404e-01\n",
      "  7.20278740e-01  2.15709254e-01 -4.52702269e-02 -6.47674873e-02\n",
      "  1.18794814e-01  1.44697383e-01 -1.32495329e-01  5.85410953e-01\n",
      " -3.43137234e-01  2.75409102e-01  2.25036308e-01  1.50559187e-01\n",
      "  3.22888345e-01  2.57087618e-01  3.78596783e-01  6.61356747e-01\n",
      " -1.34976059e-01 -5.85843503e-01 -3.71585906e-01  2.45749384e-01\n",
      "  4.72799122e-01 -6.50312379e-02  1.03827089e-01 -2.42710449e-02\n",
      " -1.61127567e-01 -7.94591725e-01 -2.10137796e+00 -5.65133512e-01\n",
      "  9.99554455e-01  1.97853357e-01  2.83387154e-01 -1.78595394e-01\n",
      "  1.94912940e-01  1.02922007e-01  2.71759927e-01  6.88619688e-02\n",
      " -7.25524187e-01 -3.99493039e-01 -4.28509004e-02  1.11385517e-01\n",
      " -8.41510236e-01  5.08089811e-02  6.40503943e-01  3.04388493e-01\n",
      " -6.12770170e-02 -1.59682423e-01 -4.55193728e-01 -9.55761075e-02\n",
      " -4.41343069e-01  1.41912133e-01  5.37566960e-01  4.37040254e-02\n",
      "  2.81829208e-01  1.31740540e-01 -2.05661908e-01  2.07400724e-01\n",
      "  6.62233114e-01  2.85458058e-01  4.90685165e-01  1.84566736e-01\n",
      "  4.49979722e-01  1.80129901e-01  3.74054313e-01  4.95090872e-01\n",
      " -2.44576976e-01  6.07260168e-01  9.72533897e-02 -9.37253982e-02\n",
      "  4.00745600e-01 -4.28710580e-01 -1.26358718e-01 -4.71248806e-01\n",
      " -5.64967275e-01  5.60319573e-02 -7.20048249e-02 -1.23202875e-02\n",
      " -2.10176483e-02  9.74207371e-02  1.10258490e-01 -4.30400342e-01\n",
      "  9.30555701e-01 -1.01523809e-01  2.85602123e-01 -4.15301621e-02\n",
      "  7.48717785e-02 -1.72861680e-01 -1.09371006e-01  4.48875040e-01\n",
      " -4.10333753e-01  3.37432790e+00 -2.87095606e-01 -4.76357937e-01\n",
      " -4.42594528e-01 -1.17385298e-01 -4.25485581e-01 -2.73876131e-01\n",
      "  3.14201355e-01 -6.57704413e-01  2.26520717e-01 -4.71419692e-01\n",
      "  3.71921003e-01  5.24181187e-01 -4.40325737e-01 -2.06464320e-01\n",
      "  6.30145431e-01  6.35774493e-01 -6.52629197e-01  5.44244170e-01\n",
      " -2.82690018e-01  2.39978552e-01 -5.97796142e-02  8.54915619e-01\n",
      "  6.00129604e-01 -1.09372818e+00 -9.90878940e-02 -4.58210222e-02\n",
      " -3.11456978e-01  2.77797788e-01 -7.13510215e-01 -4.21125144e-01\n",
      " -8.74434114e-01 -3.36560160e-01  2.15873480e-01 -2.42640406e-01\n",
      " -4.69421923e-01  1.97370306e-01  1.51250750e-01  1.42509937e-01\n",
      "  3.78442913e-01  1.08452030e-02  5.81535041e-01  4.40722227e-01\n",
      "  1.27532721e-01 -8.95353556e-02  7.39166498e-01 -3.70861232e-01\n",
      "  1.89236686e-01 -4.68166918e-01 -2.60707974e-01 -5.73436856e-01\n",
      "  6.51094913e-02  2.68359780e-01 -1.82223946e-01  9.69101936e-02\n",
      "  1.71345383e-01 -1.45429730e-01  3.33489805e-01 -4.26005721e-01\n",
      " -7.76545465e-01 -7.37197459e-01  1.44036040e-01 -4.78814095e-01\n",
      "  1.76023588e-01 -1.49703681e-01  3.54002655e-01 -5.69597147e-02\n",
      "  1.60185277e-01 -3.06181550e+00 -1.53075606e-01 -5.72886348e-01\n",
      "  3.72718900e-01  2.65904754e-01 -7.43334472e-01 -3.34909648e-01\n",
      "  5.35075307e-01  4.95426089e-01 -6.81253314e-01  5.32151222e-01\n",
      "  1.91439055e-02  7.03206241e-01  1.86690971e-01 -3.41856211e-01\n",
      "  1.39765084e-01  1.60684347e-01 -1.73364252e-01  1.68120354e-01\n",
      " -1.63153559e-01  1.38858601e-01  7.40036368e-02  2.27854222e-01\n",
      " -4.17960808e-03  3.71486396e-01 -4.31283355e-01 -3.82834226e-01\n",
      " -3.82469833e-01 -9.37596560e-01  2.14142278e-01  3.81438613e-01\n",
      " -1.36773497e-01  1.67250991e-01  9.40938741e-02 -1.58943430e-01\n",
      " -2.27256823e+00  8.74977410e-02  6.83586299e-02 -2.62213707e-01\n",
      " -2.07681179e-01  1.79981917e-01  6.50544941e-01 -4.82416958e-01\n",
      " -8.67757499e-01 -3.85069132e-01 -8.24159384e-02  1.86037987e-01\n",
      "  2.50081360e-01 -2.50337124e-02  4.05665159e-01  5.58602035e-01\n",
      "  3.51490289e-01 -1.08947076e-01  4.96256709e-01  4.05098870e-03\n",
      "  1.17049038e-01 -4.46413666e-01 -5.37404597e-01  3.13501775e-01\n",
      " -5.66248670e-02  5.21882594e-01 -9.82223213e-01  7.10331425e-02\n",
      " -3.75795454e-01 -4.09534037e-01  4.66200769e-01 -6.72969222e-01\n",
      " -2.77400702e-01  2.38609567e-01 -7.29408264e-02  1.57251060e-01\n",
      "  1.44245982e-01  4.86383438e-01  8.53721559e-01  9.28595066e-02\n",
      "  1.44029930e-01  1.26498985e+00 -2.79999554e-01  1.84412450e-01\n",
      "  8.95227253e-01  9.07380879e-02  9.75967586e-01  2.24406958e-01\n",
      " -1.51531339e-01  3.39259207e-01  2.18420342e-01  2.55489945e-01\n",
      "  1.38760459e+00 -1.12043172e-01 -4.88151424e-02 -3.77121210e-01\n",
      "  4.90213275e-01  2.32067689e-01 -2.33816460e-01  3.89621079e-01\n",
      "  1.06561100e+00 -5.45746148e-01  2.14555860e-01 -2.25018382e-01\n",
      "  9.91273671e-03 -1.01733625e+00 -2.12945834e-01 -5.80432117e-01\n",
      "  1.14616603e-01  3.52275312e-01  1.74265936e-01  1.00929439e+00\n",
      " -2.64209270e-01 -1.08852112e+00 -2.11574450e-01 -3.33137751e-01\n",
      " -1.17299333e-03 -1.76978469e-01  6.71120286e-02 -4.46681261e-01\n",
      " -4.02579993e-01 -1.10299431e-01 -7.47526884e-01  1.28722847e-01\n",
      " -6.41264796e-01 -2.27028072e-01  1.07649677e-01 -1.36430532e-01\n",
      " -1.42240429e+00 -4.63469446e-01 -1.86222434e-01  3.05613786e-01\n",
      "  2.84615278e-01  2.09360570e-01  1.66598447e-02 -1.18049547e-01\n",
      "  4.79218185e-01 -5.94101846e-01  2.67336249e-01  7.77873099e-02\n",
      " -4.12358820e-01 -5.60837567e-01 -7.07649708e-01  3.53718102e-01\n",
      "  1.73673928e-01  3.19273062e-02 -2.15477735e-01 -1.93510488e-01\n",
      "  2.93597519e-01  5.13518751e-02 -1.38250962e-01  1.22219570e-01\n",
      "  1.66278824e-01 -1.91558555e-01  5.62437892e-01  3.82898808e-01\n",
      " -2.28021555e-02  9.63744640e-01  8.39688420e-01  5.86799324e-01\n",
      "  5.17765820e-01  7.15042472e-01  5.18150628e-01 -4.56401818e-02\n",
      "  4.41424847e-01  3.31310928e-02 -1.52724907e-01 -3.37495178e-01\n",
      " -5.01755297e-01 -5.08557558e-01  3.33946794e-01 -7.20271528e-01\n",
      " -3.24351043e-01 -3.90101671e-01 -3.14785615e-02 -8.30262661e-01\n",
      " -2.79336691e-01  5.52389503e-01 -2.19960034e-01  6.25406802e-01\n",
      "  5.38597107e-01 -4.56903249e-01 -3.21185365e-02  1.59916401e-01\n",
      "  2.54068673e-01  7.68879831e-01 -1.01516485e-01  6.22536987e-03\n",
      " -2.71685809e-01  8.47205520e-01 -2.91768074e-01 -2.32147962e-01\n",
      " -7.91487277e-01 -2.97860414e-01  3.73831689e-01  9.03848037e-02\n",
      " -3.77684027e-01 -2.99598217e-01 -1.10735960e-01 -2.96604633e-01\n",
      "  2.86643356e-01  2.93127000e-01 -2.13755465e+00  1.72231644e-01\n",
      "  5.80931962e-01  6.72860026e-01  3.25232685e-01 -3.36615294e-02\n",
      " -5.17170787e-01  6.92778409e-01  4.24069129e-02 -1.25111952e-01\n",
      " -1.41847819e-01 -9.97623861e-01 -2.48993754e-01  1.05955768e-02\n",
      " -7.20817149e-02  8.99536908e-02 -2.56312937e-01 -3.30127180e-01\n",
      "  2.58050542e-02 -1.56628281e-01 -2.72992730e-01  3.80017787e-01\n",
      "  1.90809652e-01 -6.39048517e-02  5.49922138e-03 -6.79573178e-01\n",
      "  3.54787916e-01  4.71579760e-01  1.73889071e-01  1.40344739e-01\n",
      " -1.40151963e-01 -3.82846445e-01 -6.93244576e-01 -9.62501615e-02\n",
      "  5.48109055e-01  1.79084480e-01  2.74240017e-01 -3.88765752e-01\n",
      "  6.95122838e-01 -2.81543583e-02 -4.93431091e-01  7.08428562e-01\n",
      " -9.22967196e-02  1.95841670e-01  2.23768845e-01 -1.41412497e-01\n",
      " -3.38614881e-01 -1.63368180e-01 -4.15521324e-01 -1.52449340e-01\n",
      " -3.67466539e-01  3.45857702e-02  3.77637185e-02 -6.30373731e-02\n",
      "  1.53604642e-01 -4.50040996e-02 -1.92984223e-01  2.23364532e-01\n",
      " -5.29767215e-01  2.00826257e-01  3.97735357e-01 -3.19702953e-01\n",
      " -4.84797508e-02  1.15604013e-01 -3.43546420e-01 -1.02862966e+00\n",
      " -3.31704050e-01 -6.10948205e-01 -7.40960777e-01 -3.00927728e-01\n",
      "  6.50627315e-01  8.65414888e-02 -4.83936876e-01  6.73789144e-01\n",
      " -5.71556926e-01  3.92382368e-02  5.23419201e-01 -1.18905552e-01\n",
      "  5.41147470e-01 -2.56521910e-01 -1.36582881e-01 -4.95750368e-01\n",
      " -3.91434759e-01  6.52786553e-01 -2.83701926e-01 -1.31263316e-01\n",
      " -3.97048533e-01 -3.18676531e-01  9.12800059e-02 -2.30428457e-01\n",
      " -7.33456910e-01 -3.79245013e-01  2.45084420e-01  1.03215203e-01\n",
      " -2.03761622e-01 -6.88308537e-01  1.63904279e-01 -4.04506177e-01\n",
      " -8.35094154e-02 -2.79382616e-01  5.37252389e-02  5.96332550e-01\n",
      "  4.65979367e-01  3.86881888e-01  3.48968863e-01 -6.10448793e-02\n",
      "  5.73337018e-01 -9.94527340e-03  1.97490454e-01 -5.92190087e-01\n",
      " -2.42805377e-01 -3.69883567e-01 -2.13024855e-01  2.47948125e-01\n",
      "  6.82375789e-01 -9.12577391e-01  3.97825867e-01 -3.95976931e-01\n",
      "  1.60022473e+00  4.84747827e-01  4.28680152e-01 -3.50376487e-01\n",
      "  7.78658450e-01  9.69223157e-02 -4.49009359e-01  8.62805545e-01\n",
      " -6.82071626e-01 -5.01335375e-02 -2.53381848e-01  3.27517778e-01\n",
      " -4.40697312e-01  5.13285518e-01  4.98526692e-01  9.87671494e-01\n",
      " -3.59471083e-01 -5.72911382e-01 -3.43150198e-01  1.68191940e-01\n",
      " -7.14259505e-01  1.12951815e+00  5.40782988e-01  8.75323117e-02\n",
      "  1.77787513e-01  6.41454935e-01 -3.85245025e-01  4.81645465e-01\n",
      "  5.02464771e-01  3.48949730e-02 -4.80260670e-01  2.29665935e-01\n",
      "  6.58189356e-02  1.00322294e+00 -3.03836644e-01  1.12223059e-01\n",
      " -2.97916025e-01 -3.99048865e-01  6.91226363e-01  1.76347539e-01\n",
      " -1.89142719e-01 -4.04415131e-01  5.17377853e-01 -1.55545384e-01\n",
      "  1.04448453e-01  5.28798223e-01 -3.42488855e-01 -6.55166626e-01\n",
      "  4.68079656e-01  6.13815010e-01 -4.29856740e-02  3.29212487e-01\n",
      " -3.51444870e-01  5.70579708e-01 -9.71956924e-02  1.82344645e-01\n",
      " -3.37710157e-02  1.21567518e-01 -1.69176549e-01  8.69155228e-02\n",
      "  1.72598064e-01  5.76301455e-01  2.73743808e-01 -6.75247982e-03\n",
      " -1.78825364e-01  1.50688201e-01 -3.86660546e-01  4.37192023e-01\n",
      "  1.97378904e-01 -3.23726207e-01  3.10098886e-01  1.97389320e-01\n",
      "  4.56643790e-01  3.94443721e-01  4.93506610e-01  1.33734584e-01\n",
      "  5.39645672e-01  5.66998184e-01 -1.09497458e-01 -2.22327590e+00\n",
      "  1.71253443e-01  5.31610847e-01  4.14107919e-01 -5.05743265e-01\n",
      " -2.27950923e-02  3.09769601e-01  2.55310774e-01 -2.49603629e-01\n",
      "  5.20329969e-03 -9.26829427e-02  8.10874224e-01  9.66617048e-01\n",
      " -1.13633126e-01  1.70411661e-01  7.40716994e-01  2.14106500e-01\n",
      " -1.02572048e+00 -2.04366624e-01 -3.48678857e-01  5.46867132e-01\n",
      "  1.73461482e-01 -3.42002243e-01 -4.06353138e-02 -6.69542253e-01\n",
      "  6.34213462e-02  1.39105931e-01 -8.11827064e-01  5.00545144e-01\n",
      "  1.14766550e+00 -2.50892967e-01  2.79290289e-01  9.77494121e-02\n",
      "  8.25779021e-01  1.56420261e-01 -4.77133840e-01  1.74098849e-01\n",
      "  1.50189206e-01 -3.00052673e-01  1.51577458e-01 -4.98352557e-01\n",
      "  4.45586532e-01 -7.53745437e-02  7.63495862e-02 -2.85221398e-01\n",
      "  7.40827769e-02  4.42952424e-01 -3.00074041e-01  6.32266283e-01\n",
      " -3.60712737e-01 -2.04998404e-01 -1.46959364e-01  5.83032481e-02\n",
      " -1.38016552e-01  6.28215730e-01 -8.39272738e-02  7.44105995e-01\n",
      " -1.23712048e-03  5.05161762e-01 -3.92003208e-01 -1.15079045e-01\n",
      "  5.90554357e-01 -3.02465022e-01  4.90493178e-01  1.37736261e-01\n",
      " -3.94927770e-01 -5.11058807e-01 -3.48522961e-01  2.24246264e-01\n",
      "  2.00705603e-01 -9.79637355e-03 -2.42050290e-01  3.36554170e-01\n",
      " -2.56607383e-02  4.89471048e-01  2.60042906e-01  2.92030573e-02\n",
      " -3.23106974e-01  6.08169496e-01  1.44946828e-01  1.18433356e-01\n",
      " -3.92132401e-01 -6.56875789e-01  1.19904369e-01  3.13865155e-01\n",
      " -6.76669455e+00 -1.54439926e-01 -6.14286065e-01 -3.30135822e-01\n",
      " -8.49660933e-02 -4.70077127e-01  7.47107863e-02  1.54274181e-02\n",
      " -1.97616071e-01 -2.90094823e-01 -1.81808963e-01 -2.76574850e-01\n",
      " -4.75893825e-01 -5.28998673e-01 -8.01036730e-02  1.27828076e-01]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example input text (replace with your actual article or text)\n",
    "input_text = \"Research trends in urban mobility using crowd-sourced data.\"\n",
    "\n",
    "# Tokenize input text and convert it to input IDs\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Get the model's output (embeddings)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # Embeddings of the input text\n",
    "\n",
    "# Optional: If you want a single vector representation (e.g., using the [CLS] token)\n",
    "sentence_embedding = embeddings[:, 0, :].squeeze().numpy()  # [CLS] token representation\n",
    "print(\"Sentence Embedding:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b1cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Grok API: 401\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Grok API URL (replace with your actual endpoint)\n",
    "grok_api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "# Prepare the payload with BERT embeddings\n",
    "data = {\n",
    "    \"text_embedding\": sentence_embedding.tolist(),  # Convert numpy array to list for JSON compatibility\n",
    "    \"additional_info\": \"Research trends in urban mobility\"  # Optional additional info\n",
    "}\n",
    "\n",
    "# Send the POST request to Grok API\n",
    "response = requests.post(grok_api_url, json=data)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Grok API Response:\", result)\n",
    "else:\n",
    "    print(\"Error with Grok API:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abdcbda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.78.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.78.1-py3-none-any.whl (680 kB)\n",
      "   ---------------------------------------- 0.0/680.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 680.9/680.9 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl (207 kB)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   ---------------------------------------- 3/3 [openai]\n",
      "\n",
      "Successfully installed distro-1.9.0 jiter-0.9.0 openai-1.78.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9efc4881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: keybert in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: openai in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.78.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keybert) (13.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keybert) (1.5.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keybert) (3.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas torch transformers keybert openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fc1a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['id', 'title', 'doi', 'publication_year', 'authors', 'abstract', 'open_access', 'host_venue', 'clean_abstract', 'keywords', 'entities', 'gmm_cluster', 'gmm_probs']\n",
      "\n",
      "🔹 Cluster 0 🔹\n",
      "⚠️ Error processing cluster: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ERROR\n",
      "\n",
      "🔹 Cluster 1 🔹\n",
      "⚠️ Error processing cluster: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ERROR\n",
      "\n",
      "🔹 Cluster 2 🔹\n",
      "⚠️ Error processing cluster: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ERROR\n",
      "\n",
      "🔹 Cluster 3 🔹\n",
      "⚠️ Error processing cluster: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ERROR\n",
      "\n",
      "🔹 Cluster 4 🔹\n",
      "⚠️ Error processing cluster: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ERROR\n",
      "\n",
      "✅ All summaries saved to 'cluster_summaries.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from keybert import KeyBERT\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# === Config ===\n",
    "CSV_FILE = \"clustered_papers.csv\"  # ← Update this to your clustered file\n",
    "TEXT_COLUMN = \"clean_abstract\"  # Changed to \"clean_abstract\" based on your columns\n",
    "CLUSTER_COLUMN = \"gmm_cluster\"  # Updated to \"gmm_cluster\" since that's the correct column\n",
    "GROQ_API_KEY = \"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\"  # ← Replace this\n",
    "\n",
    "# === Groq Setup ===\n",
    "openai.api_key = GROQ_API_KEY\n",
    "openai.api_base = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# === Load Models ===\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "kw_model = KeyBERT(model='bert-base-uncased')\n",
    "\n",
    "# === Step 1: Load and Group Clusters ===\n",
    "def load_clusters(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Column names:\", df.columns.tolist())  # Show column names explicitly\n",
    "    df.columns = df.columns.str.strip()  # Clean any extra spaces in column names\n",
    "    if CLUSTER_COLUMN not in df.columns:\n",
    "        raise KeyError(f\"Column '{CLUSTER_COLUMN}' not found in the CSV file.\")\n",
    "    return df.groupby(CLUSTER_COLUMN)[TEXT_COLUMN].apply(list).to_dict()\n",
    "\n",
    "# === Step 2: Extract Keywords from BERT Input ===\n",
    "def embedding_to_keywords(texts, num_keywords=10):\n",
    "    combined_text = \" \".join(texts)\n",
    "# Adjust the number of keywords\n",
    "    keywords = kw_model.extract_keywords(combined_text, top_n=20)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "# === Step 3: Build Prompt for Groq ===\n",
    "def build_prompt(keywords):\n",
    "    return (\n",
    "        \"You are a research assistant. Generate a detailed research summary \"\n",
    "        \"focusing on the research methodology and key findings based on the following concepts:\\n\\n\"\n",
    "        + \", \".join(keywords) + \"\\n\\nSummary:\"\n",
    "    )\n",
    "\n",
    "\n",
    "# === Step 4: Groq API Call ===\n",
    "def query_groq(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# === Full Pipeline for One Cluster ===\n",
    "def process_cluster(texts):\n",
    "    try:\n",
    "        keywords = embedding_to_keywords(texts)\n",
    "        prompt = build_prompt(keywords)\n",
    "        return query_groq(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing cluster: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "\n",
    "# === Run for All Clusters ===\n",
    "def run_pipeline():\n",
    "    try:\n",
    "        clusters = load_clusters(CSV_FILE)\n",
    "        all_summaries = {}\n",
    "\n",
    "        for label, texts in clusters.items():\n",
    "            print(f\"\\n🔹 Cluster {label} 🔹\")\n",
    "            try:\n",
    "                summary = process_cluster(texts)\n",
    "                all_summaries[label] = summary\n",
    "                print(summary)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error processing cluster {label}: {e}\")\n",
    "                all_summaries[label] = \"ERROR\"\n",
    "\n",
    "        # Optional: Save output\n",
    "        with open(\"cluster_summaries.json\", \"w\") as f:\n",
    "            json.dump(all_summaries, f, indent=4)\n",
    "        print(\"\\n✅ All summaries saved to 'cluster_summaries.json'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading clusters: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d6940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.5555555555555556, recall=0.45454545454545453, fmeasure=0.5), 'rouge2': Score(precision=0.125, recall=0.1, fmeasure=0.11111111111111112), 'rougeL': Score(precision=0.4444444444444444, recall=0.36363636363636365, fmeasure=0.39999999999999997)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Example generated summary and ground truth summary\n",
    "generated_summary = \"AI models like GatorTron face challenges in plagiarism detection...\"\n",
    "ground_truth_summary = \"The GatorTron model faces plagiarism risks and challenges in content detection...\"\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = scorer.score(ground_truth_summary, generated_summary)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a1f9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scispacy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.5)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (3.7.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (1.14.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (2.32.3)\n",
      "Requirement already satisfied: conllu in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (6.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (1.26.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (1.5.2)\n",
      "Requirement already satisfied: pysbd in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (0.3.4)\n",
      "Requirement already satisfied: nmslib-metabrainz==2.1.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (2.1.3)\n",
      "Requirement already satisfied: pybind11>=2.2.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nmslib-metabrainz==2.1.3->scispacy) (2.13.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nmslib-metabrainz==2.1.3->scispacy) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2024.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.15.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (4.66.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (4.12.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->scispacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (1.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (0.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.20.3->scispacy) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->scispacy) (2.1.5)\n",
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz\n",
      "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz (15.9 MB)\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/15.9 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.8/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.8/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     -- ------------------------------------- 1.0/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 1.3/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 1.6/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     ---- ----------------------------------- 1.8/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     ----- ---------------------------------- 2.1/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 2.1/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 2.4/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 2.6/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ------- -------------------------------- 2.9/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ------- -------------------------------- 3.1/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 3.4/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 3.4/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     --------- ------------------------------ 3.7/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     --------- ------------------------------ 3.9/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     ---------- ----------------------------- 4.2/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 4.5/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 4.7/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ------------ --------------------------- 5.0/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ------------ --------------------------- 5.0/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 5.2/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 5.5/15.9 MB 1.1 MB/s eta 0:00:10\n",
      "     -------------- ------------------------- 5.8/15.9 MB 1.1 MB/s eta 0:00:10\n",
      "     --------------- ------------------------ 6.0/15.9 MB 1.1 MB/s eta 0:00:10\n",
      "     --------------- ------------------------ 6.3/15.9 MB 1.1 MB/s eta 0:00:10\n",
      "     ---------------- ----------------------- 6.6/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ---------------- ----------------------- 6.6/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ----------------- ---------------------- 6.8/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ----------------- ---------------------- 7.1/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ------------------ --------------------- 7.3/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ------------------- -------------------- 7.6/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     ------------------- -------------------- 7.9/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     ------------------- -------------------- 7.9/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     -------------------- ------------------- 8.1/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     --------------------- ------------------ 8.4/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     --------------------- ------------------ 8.7/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ---------------------- ----------------- 8.9/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------------------- ---------------- 9.2/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------------------- ---------------- 9.2/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------------------- ---------------- 9.4/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ------------------------ --------------- 9.7/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 10.0/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 10.2/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "     -------------------------- ------------- 10.5/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------------------- ------------ 10.7/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 10.7/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 11.0/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 11.3/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     ----------------------------- ---------- 11.5/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     ----------------------------- ---------- 11.8/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 12.1/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 12.3/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 12.3/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 12.6/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 12.8/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 13.1/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 13.4/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 13.6/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 13.6/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 13.9/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 14.2/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 14.4/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 14.7/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 14.9/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.2/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.2/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.5/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.7/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.9/15.9 MB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting spacy<3.3.0,>=3.2.3 (from en_core_sci_sm==0.5.0)\n",
      "  Downloading spacy-3.2.6.tar.gz (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.1/1.1 MB 8.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [664 lines of output]\n",
      "      Collecting setuptools\n",
      "        Downloading setuptools-80.7.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "      Collecting cython<3.0,>=0.25\n",
      "        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "      Collecting preshed<3.1.0,>=3.0.2\n",
      "        Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "      Collecting murmurhash<1.1.0,>=0.28.0\n",
      "        Using cached murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "      Collecting thinc<8.1.0,>=8.0.12\n",
      "        Downloading thinc-8.0.17.tar.gz (189 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting blis<0.8.0,>=0.4.0\n",
      "        Using cached blis-0.7.11-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "      Collecting pathy\n",
      "        Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
      "      Collecting numpy>=1.15.0\n",
      "        Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "      Collecting wasabi<1.1.0,>=0.8.1 (from thinc<8.1.0,>=8.0.12)\n",
      "        Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
      "      Collecting srsly<3.0.0,>=2.4.0 (from thinc<8.1.0,>=8.0.12)\n",
      "        Using cached srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "      Collecting catalogue<2.1.0,>=2.0.4 (from thinc<8.1.0,>=8.0.12)\n",
      "        Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "      Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from thinc<8.1.0,>=8.0.12)\n",
      "        Downloading pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n",
      "      Collecting typing-extensions>=3.7.4.3 (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->thinc<8.1.0,>=8.0.12)\n",
      "        Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "      Collecting smart-open<7.0.0,>=5.2.1 (from pathy)\n",
      "        Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "      Collecting typer<1.0.0,>=0.3.0 (from pathy)\n",
      "        Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "      Collecting pathlib-abc==0.1.1 (from pathy)\n",
      "        Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "      Collecting click<8.2,>=8.0.0 (from typer<1.0.0,>=0.3.0->pathy)\n",
      "        Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "      Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "      Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->pathy)\n",
      "        Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "      Collecting colorama (from click<8.2,>=8.0.0->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "      Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "      Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "      Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "      Using cached cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "      Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "      Using cached murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "      Using cached blis-0.7.11-cp312-cp312-win_amd64.whl (6.6 MB)\n",
      "      Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "      Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "      Using cached srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "      Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "      Using cached setuptools-80.7.0-py3-none-any.whl (1.2 MB)\n",
      "      Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
      "      Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
      "      Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "      Downloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "      Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "      Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "      Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "      Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "         ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "         ---------------------------------------- 1.2/1.2 MB 8.8 MB/s eta 0:00:00\n",
      "      Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "      Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "      Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "      Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "      Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "      Building wheels for collected packages: thinc\n",
      "        Building wheel for thinc (pyproject.toml): started\n",
      "        Building wheel for thinc (pyproject.toml): finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        Ã— Building wheel for thinc (pyproject.toml) did not run successfully.\n",
      "        â”‚ exit code: 1\n",
      "        â•°â”€> [571 lines of output]\n",
      "            Cythonizing sources\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "                    License :: OSI Approved :: MIT License\n",
      "      \n",
      "                    See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              self._finalize_license_expression()\n",
      "            running bdist_wheel\n",
      "            running build\n",
      "            running build_py\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\about.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\api.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\config.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\initializers.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\loss.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\model.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\mypy.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\optimizers.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\schedules.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\types.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\util.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\cupy_ops.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\ops.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_cupy_allocators.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_custom_kernels.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_param_server.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\extra\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\add.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\array_getitem.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\bidirectional.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\cauchysimilarity.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\chain.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\clipped_linear.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\clone.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\concatenate.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\dropout.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\embed.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\expand_window.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\gelu.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\hard_swish.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\hard_swish_mobilenet.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\hashembed.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\layernorm.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\linear.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\list2array.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\list2padded.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\list2ragged.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\logistic.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\lstm.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\map_list.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\maxout.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\mish.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\multisoftmax.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\mxnetwrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\noop.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\padded2list.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\parametricattention.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\pytorchwrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\ragged2list.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_first.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_last.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_max.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_mean.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_sum.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\relu.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\remap_ids.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\residual.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\resizable.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\siamese.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\sigmoid.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\sigmoid_activation.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\softmax.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\softmax_activation.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\strings2arrays.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\swish.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\tensorflowwrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\tuplify.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\uniqued.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_array.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_array2d.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_cpu.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_debug.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_flatten.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_getitem.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_list.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_nvtx_range.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_padded.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_ragged.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_reshape.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\mxnet.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\pytorch.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\pytorch_grad_scaler.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\shim.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\tensorflow.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\conftest.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\strategies.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_config.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_examples.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_indexing.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_initializers.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_loss.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_optimizers.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_schedules.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_serialize.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_types.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_util.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\util.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\extra\\tests\n",
      "            copying thinc\\extra\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\extra\\tests\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\test_mem.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\test_ops.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\backends\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\extra\n",
      "            copying thinc\\tests\\extra\\test_beam_search.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\extra\n",
      "            copying thinc\\tests\\extra\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\extra\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_basic_tagger.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_combinators.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_feed_forward.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_hash_embed.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_layers_api.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_linear.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_lstm.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_mnist.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_mxnet_wrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_pytorch_wrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_reduce.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_shim.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_softmax.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_sparse_linear.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_tensorflow_wrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_transforms.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_uniqued.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_with_debug.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_with_transforms.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\test_model.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\test_validation.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\model\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\n",
      "            copying thinc\\tests\\mypy\\test_mypy.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\n",
      "            copying thinc\\tests\\mypy\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\test_issue208.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\test_issue564.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\shims\n",
      "            copying thinc\\tests\\shims\\test_pytorch_grad_scaler.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\shims\n",
      "            copying thinc\\tests\\shims\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\shims\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\fail_no_plugin.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\fail_plugin.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\success_no_plugin.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\success_plugin.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\program.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\test_issue519.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\\issue519\n",
      "            running egg_info\n",
      "            writing thinc.egg-info\\PKG-INFO\n",
      "            writing dependency_links to thinc.egg-info\\dependency_links.txt\n",
      "            writing requirements to thinc.egg-info\\requires.txt\n",
      "            writing top-level names to thinc.egg-info\\top_level.txt\n",
      "            reading manifest file 'thinc.egg-info\\SOURCES.txt'\n",
      "            reading manifest template 'MANIFEST.in'\n",
      "            no previously-included directories found matching 'tmp'\n",
      "            warning: no previously-included files matching '*.cpp' found under directory 'thinc'\n",
      "            adding license file 'LICENSE'\n",
      "            writing manifest file 'thinc.egg-info\\SOURCES.txt'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.backends' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.backends' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.backends' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.backends' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.backends' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.extra' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.extra' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.extra' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.extra' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.extra' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.layers' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.layers' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.layers' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.layers' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.layers' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.tests.mypy.configs' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.tests.mypy.configs' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.tests.mypy.configs' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.tests.mypy.configs' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.tests.mypy.configs' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.tests.mypy.outputs' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.tests.mypy.outputs' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.tests.mypy.outputs' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.tests.mypy.outputs' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.tests.mypy.outputs' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            copying thinc\\__init__.pxd -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\py.typed -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\backends\\linalg.cpp -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.cpp -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\extra\\search.cpp -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\layers\\sparselinear.cpp -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\backends\\__init__.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_custom_kernels.cu -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_murmur3.cu -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\linalg.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\linalg.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\extra\\__init__.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\extra\\search.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\extra\\search.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\layers\\sparselinear.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\extra\\tests\\c_test_search.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\extra\\tests\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\configs\n",
      "            copying thinc\\tests\\mypy\\configs\\mypy-default.ini -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\configs\n",
      "            copying thinc\\tests\\mypy\\configs\\mypy-plugin.ini -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\configs\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\fail-no-plugin.txt -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\fail-plugin.txt -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\success-no-plugin.txt -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\success-plugin.txt -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            running build_ext\n",
      "            building 'thinc.backends.linalg' extension\n",
      "            creating build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\n",
      "            \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\numpy\\_core\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /EHsc /Tpthinc/backends/linalg.cpp /Fobuild\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.obj /Ox /EHsc\n",
      "            linalg.cpp\n",
      "            thinc/backends/linalg.cpp(2034): warning C4244: '=': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            thinc/backends/linalg.cpp(2474): warning C4244: '=': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            thinc/backends/linalg.cpp(2521): warning C4244: '=': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            thinc/backends/linalg.cpp(2688): warning C4244: '=': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            thinc/backends/linalg.cpp(3356): warning C4244: 'argument': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX86\\x64\\link.exe\" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\libs /LIBPATH:C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312 /LIBPATH:C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\PCbuild\\amd64 \"/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\lib\\x64\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.22621.0\\ucrt\\x64\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\\\lib\\10.0.22621.0\\\\um\\x64\" /EXPORT:PyInit_linalg build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.obj /OUT:build\\lib.win-amd64-cpython-312\\thinc\\backends\\linalg.cp312-win_amd64.pyd /IMPLIB:build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.cp312-win_amd64.lib\n",
      "               Creating library build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.cp312-win_amd64.lib and object build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.cp312-win_amd64.exp\n",
      "            Generating code\n",
      "            Finished generating code\n",
      "            building 'thinc.backends.numpy_ops' extension\n",
      "            \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\numpy\\_core\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /EHsc /Tpthinc/backends/numpy_ops.cpp /Fobuild\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\numpy_ops.obj /Ox /EHsc\n",
      "            numpy_ops.cpp\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\numpy\\_core\\include\\numpy\\npy_1_7_deprecated_api.h(14) : Warning Msg: Using deprecated NumPy API, disable it with #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n",
      "            thinc/backends/numpy_ops.cpp(4627): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(4720): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(4829): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5067): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5167): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5217): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5347): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5694): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5814): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5946): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6079): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6220): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6369): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6557): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6706): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6854): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6990): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7143): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7247): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7268): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7277): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7286): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7471): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7594): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7603): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7612): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7741): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7870): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8082): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8224): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8450): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8473): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8501): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8622): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8631): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(8640): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(8721): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(8876): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8899): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8927): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9049): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9058): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9076): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9157): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9312): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9425): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9550): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9662): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9671): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9680): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9689): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9834): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9946): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9955): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9964): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9973): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10118): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(10234): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(10243): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10252): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10510): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(10626): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(10635): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10644): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10902): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11017): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11026): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11035): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11044): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11246): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11375): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11384): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11393): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11667): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11791): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11987): warning C4244: 'argument': conversion from 'npy_intp' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11987): warning C4244: 'argument': conversion from 'npy_intp' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(12087): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(12292): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(12292): fatal error C1003: error count exceeds 100; stopping compilation\n",
      "            error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "        ERROR: Failed building wheel for thinc\n",
      "      Failed to build thinc\n",
      "      ERROR: Failed to build installable wheels for some pyproject.toml based projects (thinc)\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install scispacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a14173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Import all modules ===\n",
    "import pandas as pd\n",
    "import re, json, ast, torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keybert import KeyBERT\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import openai\n",
    "import gradio as gr\n",
    "from fetch_papers import fetch_openalex_papers, save_to_csv  # Replace with your actual function\n",
    "\n",
    "# === Model/Loaders ===\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model='bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "\n",
    "# === Groq API Setup ===\n",
    "openai.api_key = \"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\"  # Replace with your actual key\n",
    "openai.api_base = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# === Neo4j Setup ===\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"qwertyuiop\"))\n",
    "\n",
    "\n",
    "# === Step 1: Fetch Papers ===\n",
    "def fetch_and_save_papers(query, max_results=50):\n",
    "    df = fetch_openalex_papers(query, max_results=max_results)\n",
    "    save_to_csv(df, \"papers.csv\")\n",
    "\n",
    "\n",
    "# === Step 2: Enrichment ===\n",
    "def enrich_papers():\n",
    "    df = pd.read_csv(\"papers.csv\")\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text).strip()\n",
    "    df[\"clean_abstract\"] = df[\"abstract\"].fillna(\"\").apply(clean_text)\n",
    "    df[\"keywords\"] = df[\"keywords\"].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    df[\"entities\"] = df[\"entities\"].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    df.to_csv(\"papers_enriched.csv\", index=False)\n",
    "\n",
    "\n",
    "# === Step 3: Build Knowledge Graph ===\n",
    "def build_knowledge_graph():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\")\n",
    "    graph.delete_all()\n",
    "    for _, row in df.iterrows():\n",
    "        paper_node = Node(\"Paper\", title=row[\"title\"], year=row[\"publication_year\"], doi=row[\"doi\"])\n",
    "        graph.create(paper_node)\n",
    "        for author in eval(str(row[\"authors\"])):\n",
    "            author_node = Node(\"Author\", name=author)\n",
    "            graph.merge(author_node, \"Author\", \"name\")\n",
    "            graph.create(Relationship(author_node, \"WROTE\", paper_node))\n",
    "        for keyword in row[\"keywords\"]:\n",
    "            keyword_node = Node(\"Keyword\", name=keyword)\n",
    "            graph.merge(keyword_node, \"Keyword\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"HAS_KEYWORD\", keyword_node))\n",
    "        for entity in row[\"entities\"]:\n",
    "            entity_node = Node(\"Entity\", name=entity)\n",
    "            graph.merge(entity_node, \"Entity\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"MENTIONS\", entity_node))\n",
    "\n",
    "\n",
    "# === Step 4: Cluster Abstracts ===\n",
    "def cluster_abstracts():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\").dropna(subset=[\"clean_abstract\"])\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid abstracts to cluster.\")\n",
    "    embeddings = model.encode(df[\"clean_abstract\"].tolist(), show_progress_bar=True)\n",
    "    gmm = GaussianMixture(n_components=5, covariance_type='full', random_state=42)\n",
    "    gmm.fit(embeddings)\n",
    "    df[\"gmm_cluster\"] = gmm.predict(embeddings)\n",
    "    df[\"gmm_probs\"] = gmm.predict_proba(embeddings).tolist()\n",
    "    df.to_csv(\"clustered_papers.csv\", index=False)\n",
    "\n",
    "\n",
    "# === Step 5: Summarization with Groq ===\n",
    "def summarize_clusters():\n",
    "    df = pd.read_csv(\"clustered_papers.csv\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if \"gmm_cluster\" not in df.columns:\n",
    "        raise ValueError(\"Missing 'gmm_cluster' column.\")\n",
    "    clusters = df.groupby(\"gmm_cluster\")[\"clean_abstract\"].apply(list).to_dict()\n",
    "\n",
    "    def embedding_to_keywords(texts):\n",
    "        combined = \" \".join(texts)\n",
    "        return [kw[0] for kw in kw_model.extract_keywords(combined, top_n=20)]\n",
    "\n",
    "    def query_groq(prompt):\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    summaries = {}\n",
    "    for cluster_id, texts in clusters.items():\n",
    "        try:\n",
    "            keywords = embedding_to_keywords(texts)\n",
    "            prompt = (\n",
    "                \"You are a research assistant. Generate a detailed research summary \"\n",
    "                \"focusing on research methodology and key findings for:\\n\\n\"\n",
    "                + \", \".join(keywords) + \"\\n\\nSummary:\"\n",
    "            )\n",
    "            summaries[cluster_id] = query_groq(prompt)\n",
    "        except Exception as e:\n",
    "            summaries[cluster_id] = f\"ERROR: {str(e)}\"\n",
    "\n",
    "    with open(\"cluster_summaries.json\", \"w\") as f:\n",
    "        json.dump(summaries, f, indent=4)\n",
    "\n",
    "\n",
    "# === Unified Runner ===\n",
    "def run_pipeline(query):\n",
    "    try:\n",
    "        fetch_and_save_papers(query)\n",
    "        enrich_papers()\n",
    "        build_knowledge_graph()\n",
    "        cluster_abstracts()\n",
    "        summarize_clusters()\n",
    "        return \"✅ Pipeline executed successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Pipeline failed: {e}\"\n",
    "\n",
    "\n",
    "# === UI Wrapper ===\n",
    "def run_pipeline_ui(query):\n",
    "    status = run_pipeline(query)\n",
    "\n",
    "    try:\n",
    "        df_enriched = pd.read_csv(\"papers_enriched.csv\")\n",
    "    except:\n",
    "        df_enriched = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df_clustered = pd.read_csv(\"clustered_papers.csv\")\n",
    "    except:\n",
    "        df_clustered = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        with open(\"cluster_summaries.json\") as f:\n",
    "            summaries = json.load(f)\n",
    "        summaries_str = \"\\n\\n\".join([f\"🔹 Cluster {k}:\\n{v}\" for k, v in summaries.items()])\n",
    "    except:\n",
    "        summaries_str = \"No summary generated.\"\n",
    "\n",
    "    return (\n",
    "        status,\n",
    "        df_enriched.head(10) if not df_enriched.empty else \"No enriched data available.\",\n",
    "        df_clustered[[\"title\", \"gmm_cluster\"]].head(10) if not df_clustered.empty else \"No clustered data available.\",\n",
    "        summaries_str,\n",
    "        \"papers_enriched.csv\" if not df_enriched.empty else None,\n",
    "        \"clustered_papers.csv\" if not df_clustered.empty else None,\n",
    "        \"cluster_summaries.json\" if summaries_str else None\n",
    "    )\n",
    "\n",
    "\n",
    "# === GRADIO UI ===\n",
    "gr.Interface(\n",
    "    fn=run_pipeline_ui,\n",
    "    inputs=gr.Textbox(lines=2, label=\"Enter Research Topic\", placeholder=\"e.g., LLMs in medicine\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Pipeline Status\"),\n",
    "        gr.Dataframe(label=\"Enriched Papers Preview\"),\n",
    "        gr.Dataframe(label=\"Clustered Papers Preview\"),\n",
    "        gr.Textbox(label=\"Cluster Summaries\"),\n",
    "        gr.File(label=\"📄 Enriched CSV\"),\n",
    "        gr.File(label=\"📄 Clustered CSV\"),\n",
    "        gr.File(label=\"📄 Summary JSON\")\n",
    "    ],\n",
    "    title=\"Unified Hybrid NLP Research Assistant\",\n",
    "    description=\"End-to-end research pipeline: OpenAlex → Enrichment → Neo4j KG → Clustering → Groq Summarization\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bacaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching OpenAlex for: transformers\n",
      "Request failed: HTTPSConnectionPool(host='api.openalex.org', port=443): Max retries exceeded with url: /works?filter=title.search:transformers&per-page=25&cursor=* (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate is not yet valid (_ssl.c:1000)')))\n",
      "✅ Retrieved 0 papers.\n",
      "Saved metadata to papers.csv\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import re, json, ast, torch, os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keybert import KeyBERT\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import openai\n",
    "import gradio as gr\n",
    "from fetch_papers import fetch_openalex_papers, save_to_csv\n",
    "\n",
    "# === Model/Loaders ===\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model='bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "\n",
    "# === Groq API Setup ===\n",
    "openai.api_key = \"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\"  # Replace with your actual key\n",
    "openai.api_base = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# === Neo4j Setup ===\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"qwertyuiop\"))\n",
    "\n",
    "\n",
    "# === Step 1: Fetch Papers ===\n",
    "def fetch_and_save_papers(query, max_results=50):\n",
    "    df = fetch_openalex_papers(query, max_results=max_results)\n",
    "    save_to_csv(df, \"papers.csv\")\n",
    "\n",
    "\n",
    "# === Step 2: Enrichment ===\n",
    "def enrich_papers():\n",
    "    df = pd.read_csv(\"papers.csv\")\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text).strip()\n",
    "    df[\"clean_abstract\"] = df[\"abstract\"].fillna(\"\").apply(clean_text)\n",
    "    df[\"keywords\"] = df[\"keywords\"].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    df[\"entities\"] = df[\"entities\"].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    df.to_csv(\"papers_enriched.csv\", index=False)\n",
    "\n",
    "\n",
    "# === Step 3: Build Knowledge Graph ===\n",
    "def build_knowledge_graph():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\")\n",
    "    graph.delete_all()\n",
    "    for _, row in df.iterrows():\n",
    "        paper_node = Node(\"Paper\", title=row[\"title\"], year=row[\"publication_year\"], doi=row[\"doi\"])\n",
    "        graph.create(paper_node)\n",
    "        for author in eval(str(row[\"authors\"])):\n",
    "            author_node = Node(\"Author\", name=author)\n",
    "            graph.merge(author_node, \"Author\", \"name\")\n",
    "            graph.create(Relationship(author_node, \"WROTE\", paper_node))\n",
    "        for keyword in row[\"keywords\"]:\n",
    "            keyword_node = Node(\"Keyword\", name=keyword)\n",
    "            graph.merge(keyword_node, \"Keyword\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"HAS_KEYWORD\", keyword_node))\n",
    "        for entity in row[\"entities\"]:\n",
    "            entity_node = Node(\"Entity\", name=entity)\n",
    "            graph.merge(entity_node, \"Entity\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"MENTIONS\", entity_node))\n",
    "\n",
    "\n",
    "# === Step 4: Cluster Abstracts ===\n",
    "def cluster_abstracts():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\").dropna(subset=[\"clean_abstract\"])\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid abstracts to cluster.\")\n",
    "    embeddings = model.encode(df[\"clean_abstract\"].tolist(), show_progress_bar=True)\n",
    "    gmm = GaussianMixture(n_components=5, covariance_type='full', random_state=42)\n",
    "    gmm.fit(embeddings)\n",
    "    df[\"gmm_cluster\"] = gmm.predict(embeddings)\n",
    "    df[\"gmm_probs\"] = gmm.predict_proba(embeddings).tolist()\n",
    "    df.to_csv(\"clustered_papers.csv\", index=False)\n",
    "\n",
    "\n",
    "# === Step 5: Summarization with Groq ===\n",
    "def summarize_clusters():\n",
    "    df = pd.read_csv(\"clustered_papers.csv\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if \"gmm_cluster\" not in df.columns:\n",
    "        raise ValueError(\"Missing 'gmm_cluster' column.\")\n",
    "    clusters = df.groupby(\"gmm_cluster\")[\"clean_abstract\"].apply(list).to_dict()\n",
    "\n",
    "    def embedding_to_keywords(texts):\n",
    "        combined = \" \".join(texts)\n",
    "        return [kw[0] for kw in kw_model.extract_keywords(combined, top_n=20)]\n",
    "\n",
    "    def query_groq(prompt):\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    summaries = {}\n",
    "    for cluster_id, texts in clusters.items():\n",
    "        try:\n",
    "            keywords = embedding_to_keywords(texts)\n",
    "            prompt = (\n",
    "                \"You are a research assistant. Generate a detailed research summary \"\n",
    "                \"focusing on research methodology and key findings for:\\n\\n\"\n",
    "                + \", \".join(keywords) + \"\\n\\nSummary:\"\n",
    "            )\n",
    "            summaries[cluster_id] = query_groq(prompt)\n",
    "        except Exception as e:\n",
    "            summaries[cluster_id] = f\"ERROR: {str(e)}\"\n",
    "\n",
    "    with open(\"cluster_summaries.json\", \"w\") as f:\n",
    "        json.dump(summaries, f, indent=4)\n",
    "\n",
    "\n",
    "# === Visualize Neo4j Graph ===\n",
    "def visualize_neo4j_graph():\n",
    "    query = \"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    RETURN n.name AS source, type(r) AS relation, m.name AS target\n",
    "    \"\"\"\n",
    "    result = graph.run(query).data()\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for row in result:\n",
    "        G.add_edge(row[\"source\"], row[\"target\"], label=row[\"relation\"])\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G, k=0.5)\n",
    "    nx.draw(G, pos, with_labels=True, node_size=800, node_color=\"skyblue\", font_size=10, font_weight='bold', edge_color=\"gray\")\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"graph.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# === Unified Runner ===\n",
    "def run_pipeline(query):\n",
    "    try:\n",
    "        fetch_and_save_papers(query)\n",
    "        enrich_papers()\n",
    "        build_knowledge_graph()\n",
    "        cluster_abstracts()\n",
    "        summarize_clusters()\n",
    "        visualize_neo4j_graph()\n",
    "        return \"✅ Pipeline executed successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Pipeline failed: {e}\"\n",
    "\n",
    "\n",
    "# === UI Wrapper ===\n",
    "def run_pipeline_ui(query):\n",
    "    status = run_pipeline(query)\n",
    "\n",
    "    try:\n",
    "        df_enriched = pd.read_csv(\"papers_enriched.csv\")\n",
    "    except:\n",
    "        df_enriched = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df_clustered = pd.read_csv(\"clustered_papers.csv\")\n",
    "    except:\n",
    "        df_clustered = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        with open(\"cluster_summaries.json\") as f:\n",
    "            summaries = json.load(f)\n",
    "        summaries_str = \"\\n\\n\".join([f\"🔹 Cluster {k}:\\n{v}\" for k, v in summaries.items()])\n",
    "    except:\n",
    "        summaries_str = \"No summary generated.\"\n",
    "\n",
    "    graph_file = \"graph.png\" if os.path.exists(\"graph.png\") else None\n",
    "\n",
    "    return (\n",
    "        status,\n",
    "        df_enriched.head(10) if not df_enriched.empty else \"No enriched data available.\",\n",
    "        df_clustered[[\"title\", \"gmm_cluster\"]].head(10) if not df_clustered.empty else \"No clustered data available.\",\n",
    "        summaries_str,\n",
    "        \"papers_enriched.csv\" if not df_enriched.empty else None,\n",
    "        \"clustered_papers.csv\" if not df_clustered.empty else None,\n",
    "        \"cluster_summaries.json\" if summaries_str else None,\n",
    "        graph_file,   # for Image (display)\n",
    "        graph_file    # for File (download)\n",
    "    )\n",
    "\n",
    "\n",
    "# === GRADIO UI ===\n",
    "gr.Interface(\n",
    "    fn=run_pipeline_ui,\n",
    "    inputs=gr.Textbox(lines=2, label=\"Enter Research Topic\", placeholder=\"e.g., LLMs in medicine\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Pipeline Status\"),\n",
    "        gr.Dataframe(label=\"Enriched Papers Preview\"),\n",
    "        gr.Dataframe(label=\"Clustered Papers Preview\"),\n",
    "        gr.Textbox(label=\"Cluster Summaries\"),\n",
    "        gr.File(label=\"📄 Enriched CSV\"),\n",
    "        gr.File(label=\"📄 Clustered CSV\"),\n",
    "        gr.File(label=\"📄 Summary JSON\"),\n",
    "        gr.Image(label=\"📷 Graph Preview\"),    # Show graph inline\n",
    "        gr.File(label=\"📥 Download Graph PNG\")  # Allow file download\n",
    "    ],\n",
    "    title=\"Unified Hybrid NLP Research Assistant\",\n",
    "    description=\"End-to-end research pipeline: OpenAlex → Enrichment → Neo4j KG → Clustering → Groq Summarization → Graph Visualization\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39119cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
